\documentclass{article}

\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{multicol}

\usepackage{mathenv}

\def\nbOne{{\mathchoice {\rm 1\mskip-4mu l} {\rm 1\mskip-4mu l}
{\rm 1\mskip-4.5mu l} {\rm 1\mskip-5mu l}}}

\usepackage{vmargin}
\setmarginsrb{2.5cm}{2.5cm}{2.5cm}{2.5cm}{0cm}{0cm}{0cm}{0cm}

\usepackage[utf8]{inputenc}

\usepackage[french]{babel}
\selectlanguage{french}

\usepackage{color}
\usepackage{graphicx}
\graphicspath{{img/}} 
\usepackage{listings}
\definecolor{colKeys}{rgb}{0.75,0,0}
\definecolor{colIdentifier}{rgb}{0,0,0}
\definecolor{colComments}{rgb}{0.75,0.75,0}
\definecolor{colString}{rgb}{0,0,0.7}

\lstset{
basicstyle=\ttfamily\small, %
identifierstyle=\color{colIdentifier}, %
keywordstyle=\color{colKeys}, %
stringstyle=\color{colString}, %
commentstyle=\color{colComments}
}
\lstset{language=java}

% Commandes personnelles %

\definecolor{darkred}{rgb}{0.85,0,0}
\definecolor{darkblue}{rgb}{0,0,0.7}
\definecolor{darkgreen}{rgb}{0,0.6,0}
\definecolor{darko}{rgb}{0.93,0.43,0}
\newcommand{\dred}[1]{\textcolor{darkred}{\textbf{#1}}}
\newcommand{\dgre}[1]{\textcolor{darkgreen}{\textbf{#1}}}
\newcommand{\dblu}[1]{\textcolor{darkblue}{\textbf{#1}}}
\newcommand{\dora}[1]{\textcolor{darko}{\textbf{#1}}}
\newcommand{\gre}[1]{\textcolor{darkgreen}{#1}}
\newcommand{\blu}[1]{\textcolor{darkblue}{#1}}
\newcommand{\ora}[1]{\textcolor{darko}{#1}}
\newcommand{\red}[1]{\textcolor{darkred}{#1}}

\newcommand{\image}[1]{\includegraphics{#1}}
\newcommand{\imageR}[2]{\includegraphics[width=#2px]{#1}}
\newcommand{\imageRT}[2]{\includegraphics[height=#2px]{#1}}
\newcommand{\img}[1]{\begin{center}\includegraphics[width=400px]{#1}\end{center}}
\newcommand{\imag}[1]{\begin{center}\includegraphics{#1}\end{center}}
\newcommand{\imgR}[2]{\begin{center}\includegraphics[width=#2px]{#1}\end{center}}
\newcommand{\imgRT}[2]{\begin{center}\includegraphics[height=#2px]{#1}\end{center}}
\newcommand{\point}[2]{\item \ora{\underline{#1}} : \textit{#2}}
\newcommand{\bfp}[2]{\item \textbf{#1} : \textit{#2}}
\newcommand{\sumin}[3]{\sideset{}{_{i=#1}^{#2}}\sum{#3}}
\newcommand{\stitre}[1]{\noindent\textbf{\underline{#1}} \\}
\newcommand{\stitreD}[2]{\noindent\textbf{\underline{#1}} \textit{(#2)}\\}

\newcommand{\neu}{n\oe ud}
\newcommand{\neuSP}{n\oe ud }
\newcommand{\neus}{n\oe uds}
\newcommand{\neuSPs}{n\oe uds }

\DeclareMathAlphabet{\mathpzc}{OT1}{pzc}{m}{it}

\title{\textbf{\textcolor{darkblue}{Computer Networks - Résumé Janvier 2010.}}}
\author{\textit{Dubuc Xavier}}

\begin{document}

\maketitle

\hbox{\raisebox{0.4em}{\vrule depth 0.4pt height 0.4pt width 10cm}}

\tableofcontents

$ $ \\
\hbox{\raisebox{0.4em}{\vrule depth 0.4pt height 0.4pt width 10cm}}

\newpage

\section{Introduction}

Internet est une immense toile d'araignée reliant chacune de ses extrêmités entre elles, chacune de ces 
extrêmités est appellée un \textbf{«hôte»}. Un \textbf{hôte} peut être une télévision, une voiture, ... n'importe
quoi tant que ça exécute des applications Internet. La toile est constituée de \textbf{liens} qui peuvent être à
la fois des fibres, des ondes radios, des liaisons satellites, ... La communication par ces liens est régie par 
des \dred{protocoles}, c'est-à-dire des formattages de données qui doivent être respectés pour arriver à 
communiquer. Lors de la communication, certains paquets de données peuvent être jetés (\textbf{«drop»}, on peut
l'éviter avec l'utilisation de certains protocoles sécurisés) pour diverses raisons (protocole non respecté, 
protocole inconnu, routeur trop chargé, ...) et d'autres peuvent être prioritaires, ceux-ci seront traités avant 
les paquets utilisant le service \textit{«best effort»} («on fait ce qu'on peut»).

On distingue plusieurs «types» de connexion, 2 hôtes peuvent communiquer directement entre eux sans passer par
un serveur ou autres, on appelle cette connexion le \textbf{«peer-to-peer»}. Si la connexion utilisée passe par 
un ou plusieurs serveurs, on parle de connexion \textbf{«client/serveur»}. Il existe également des connexions
«hybrides» où le serveur sert juste d'annuaire permettant à un hôte de connaître l'adresse à utiliser pour 
joindre l'hôte désiré (c'est comme cela que fonctionne Skype notamment).

\subsection{Les protocoles}

Un protocole est une «ligne de conduite» à adapter lors de la communication, par exemple, pour la communication 
entre humains, on a une espèce de protocole qui vise à dire que l'on commence par se saluer. On a un autre qui 
vise à présenter 2 personnes qui ne se connaissent pas en disant à l'un et à l'autre le prénom de l'autre. En
informatique ça va être sensiblement pareil bien que ce sera plus strict, en effet un protocole de communication
va définir le format et l'ordre des messages envoyés et reçus par une borne (routeur ou autre) connecté à 
Internet ainsi que les actions à effecter en vue du message reçu.

\imgR{CN_001.png}{300}

\subsection{Accès à Internet}

Il y a différents modules permettant de se connecter à Internet, chacun ayant ses propres caractéristiques. En
voici quelques-uns :
\begin{itemize}
\point{Dial-up Modem}{Il transforme des informations digitales venant de l'ordinateur 
en informations analogiques pour être transférées via le réseau téléphonique (et vice versa). \\
\red{Inconvénient} : impossibilité de téléphoner en même temps qu'aller sur Internet.\\
Bande passante : \red{limitée à $56$Kbps}}.
\point{ADSL (Asymmetric Digital Subscriber Line)}{Assez semblable que pour le réseau 
téléphonique mais la transformation des données est différentes et le procédé va utiliser une gamme de fréquences
plus élevée que celles de la voix permettant ainsi de \gre{téléphoner en même temps que naviguer sur Internet}.
(le splitter sépare les informations de la voix et les informations concernant le modem (selon les fréquences)
et le DSLAM retransforme l'information transformée par le modem DSL.)\\ 
Bande passante : \gre{jusque $20$Mbps}}
\point{Télévision}{semblable que DSL mais avec la télé. (Les 2 infos sont de la vidéo et 
de l'information Internet que l'on sépare via le FDM) une chaîne est transférée par cable via une gamme de 
fréquence spécifique et on sélectionne celle-ci via un filtre contenu dans la télé qui filtre les fréquences 
désirées pour afficher la chaine voulue. L'information Internet sera envoyée via des fréquences non-utilisées par 
des chaines.}
\point{Ethernet}{Plusieurs équipements sont connectés à un switch lui même connectés à 
un routeur connecté au réseau Internet. De nos jours, tous les bouts de la toile sont typiquement connectés à un
switch Ethernet.\\
Bande passante : \gre{de $10$Mbps à $10$Gbps !} (dans le réseau Ethernet)}
\point{Wireless LAN(Local Area Network)'s}{Une borne (appelée point d'accès) est 
connectée au routeur d'accès Internet et diffuse un réseau où peuvent se connecter différentes machines. \\
Bande passante : \gre{jusque $54$Mbps} (intra LAN)}
\end{itemize}

\subsection{Média physique}

Les hôtes s'envoient des \textbf{bits} via un \textbf{lien physique} (tout ce qui est entre l'émetteur et le 
receveur). On distingue les médias guidés qui sont typiquement les cables et les médias non-guidés où le signal
se propage librement comme dans les ondes radios par exemple.

\subsubsection{Les médias guidés}

\noindent \begin{itemize}
\point{Twisted Pair}{Cable constitué de 2 fils de cuivres torsadés, ceci permettant de
résister plus facilement aux perturbations magnétiques et ainsi éviter plus facilement d'éventuelles altérations 
du signal. (Typiquement les cables de téléphone ainsi que les cables Ethernet)}
\point{Cable coaxial}{Plus ou moins identique mais les 2 fils ne sont plus torsadés, 
ils sont concentriques et séparés par des isolants (l'un des 2 «fils» est donc creux).}
\point{Fibre optique}{La fibre de verre transporte des impulsions lumineuses, chaque
impulsion représentant un bit. Elle est pratiquement immunisée contre les perturbations magnétiques et 
l'atténuation (sauf sur de très très longues distances). C'est un cable très performant car il permet un débit 
variant de $10$Gbps à plusieurs $100$aines de Gbps.}
\end{itemize}

\subsubsection{Les médias non-guidés}

Le signal est gardé dans un spectre electromagnétique spécifié et présente l'avantage de se passer de l'usage de
cables. Cependant il est confronté aux problèmes d'interférence, de réflexion et d'obstruction par les objets.
\begin{itemize}
\point{Micro-ondes terrestres}{Bande passante jusque $45$Mbps.}
\point{LAN}{Jusque $54$Mbps.}
\point{Wide-area}{Exemple, réseau 3G cellulaire $\sim1$Mbps.}
\point{Satellite}{De quelques Kbps à $45$Mbps, $270$msec de delai entre les 2 extrémités du média.}
\end{itemize}

\subsection{Transfert de données via Internet}

La question fondamentale que l'on se pose c'est : \textbf{Comment sont transférées les données d'un routeur à 
l'autre via Internet ?} Il y a en fait 2 façons de transférer les données, le \textbf{«circuit switching»} et le
\textbf{«packet switching»}. Dans le premier cas, un circuit est dédié pour chaque connexion entre 2 routeurs et
dans le second les données sont envoyées sur le net par «morceaux» (des paquets) discrets.

\subsubsection{Circuit switching}

\noindent Principales caractéristiques :
\begin{itemize}
\item Cela nécessite un réglage des circuits à utiliser pour chaque pair de routeurs,
\item la performance du circuit est garantie car il sera toujours le même,
\item les ressources sont dédiées $\rightarrow$ pas de partage.
\end{itemize}

\noindent Les ressources sont «divisées en morceaux» et ces morceaux sont alloués aux différents circuits 
($\rightarrow$ plus il y aura de circuits passant par la ressource plus il y aura de divisions). Si le morceau 
d'un circuit n'est pas utilisé par le détenteur, alors il est inutilisé car il n'y a pas de partage de 
ressources. 
\newpage
\noindent On peut diviser ces ressources de différentes façons, prenons par exemple la bande passante : 
\begin{itemize}
\point{frequency division (FDM)}{Une bande de fréquence est allouée à un certain circuit : 
\imgR{CN_002.png}{300}}
\point{time division (TDM)}{Un interval de temps est alloué à un certain circuit : 
\imgR{CN_003.png}{300}}
\end{itemize}

\noindent\underline{Exemple} : Combien de temps faut-il pour envoyer un fichier de $640.000$ bits d'un hôte A à 
un hôte B sur un réseau «circuit-switched» ? On considère que tous les liens ont une bande passante de $1,536$ 
Mbps, qu'ils utilisent TDM avec $24$ slots/sec et qu'il faut $500$ msec pour établir le circuit.

\imgR{CN_004.png}{300}

\noindent L'utilisateur reçoit 1 slot par seconde pendant $\frac{1}{24}$ de seconde, il peut donc envoyer par 
seconde $\frac{1}{24}$ de fois ce que le lien le lui permet (vu que sur une seconde il n'envoie rien pendant 
$\frac{23}{24}$ de seconde et envoie des données pendant $\frac{1}{24}$ de seconde). Donc au lieu d'avoir un 
débit de $1,536$ Mbps, on a un débit de $\frac{1,536}{24} = 0.064$Mbps $=64$Kbps pour le circuit allant de A à 
B, et donc A enverra son fichier de $640.000$ bits (=$640$Kb) à raison de $64$Kbps ; il faudra donc $\frac{640}
{64} = \red{10s}$ à A pour envoyer son fichier en entier à B.

\subsubsection{Packet switching}

\noindent Principales caractéristiques :
\begin{itemize}
\item Tous les streams de données entre hôtes sont séparés en paquets,
\item les paquets partagent les ressources Internet, nous imposant de gérer ce partage : 
\begin{itemize}
\item La demande des ressources peut être plus grande que celle disponible,
\item les paquets sont mis en attente d'un lien occupé pour éviter la congestion,
\item chaque routeur reçoit un paquet en entier (et le stocke donc) et seulement ensuite le forward. 
(\textbf{store-and-forward})
\end{itemize}
\item chaque paquet utilise toute la bande passante d'un lien,
\item les ressources sont utilisées comme on en a besoin.
\end{itemize}

\noindent \underline{Store-and-forward} 
\imgR{CN_005.png}{300}

\noindent Il faut un temps égal à $\frac{L}{R}$ pour envoyer un paquet de $L$ bits sur un lien de bande passante 
de $R$ bps. Dans notre exemple, considérons $R = 1,5$Mbps et $L = 7,5$Mbps et qu'il n'y a aucun temps de
propagation intra-routeur, alors la machine de gauche prendra $\frac{3L}{R}$ c'est à dire $\frac{22.5}{1.5} = 15$ 
secondes pour envoyer le fichier à l'autre machine.\\
En effet, le paquet doit passer par 3 liens de bande passante $R$, il 
faudra donc à chaque nouveau lien un temps de $\frac{L}{R}$ pour que le routeur place le paquet en entier sur le 
lien \textit{(Ne pas oublier que le routeur ne commence à placer le paquet que lorsqu'il l'a reçu en entier)}.

\subsubsection{Comparaison}

On peut prouver que le \textbf{packet switching} permet à plus d'utilisateurs d'utiliser le réseau, pour un lien
d'$1$Mb/s, chaque utilisateur d'un \textbf{circuit switching} reçoit $100$Kb/s quand il est actif ($10\%$ du 
temps). Celà implique donc qu'il y a $10$ utilisateurs sur ce réseau alors que pour un même lien, avec la 
politique du \textbf{packet switching} on peut placer $35$ utilisateurs et on observe que la probabilité pour que
plus de $10$ utilisateurs soient actifs en même temps est inférieure à $0,0004$ (on obtient cette valeur avec la
loi binomiale avec $N=35$ et $x=10$).

Le \textbf{packet switching} est très bon pour les grandes données car il partage les ressources et il est plus
simple car il ne faut pas de réglages des circuits à utiliser au préalable. Par contre, lorsque la congestion est
élevée, il y a des délais supplémentaires pour le transfert de paquets ainsi que des pertes ; on aura donc besoin
de protocoles spéciaux pour gérer ce problème. Ensuite il doit également pouvoir agir comme un circuit et ainsi
garantir une bande passante à un stream (comme de la vidéo ou de l'audio) ... Nous verrons ça dans le chapitre 7.

\subsection{Infrastructure d'Internet}

Internet est un réseau constitué de plusieurs réseaux, il est composé de plusieurs ISP(Internet Service Provider) 
de différents niveaux : 
\begin{itemize}
\point{ISP de niveau 1}{assure la couverture nationale/internationale, ils sont interconnectés entre eux de 
manière privée}
\point{ISP de niveau 2}{un peu plus «petit» que les niveaux 1 (souvent régionnaux), ils sont connectés à un ou 
plusieurs ISP de niveau 1 et il paie cet(ces) ISP pour être connecté au réseau international et ils peuvent être 
connectés entre eux de manière privée (comme les ISP de niveau 1)}
\point{ISP de niveau 3 et ISP local}{Ce sont les derniers ISP, ceux qui sont les plus proches des bouts de la 
toile.}
\img{CN_006.png}

\end{itemize}

\subsection{Delai et perte de paquets}

Les paquets reçus par un routeur sont stockés dans une file d'attente, si le taux d'arrivée de paquets est plus
grand que la capacité du lien à la sortie du routeur (et donc il faut plus de temps pour faire partir les paquets
que de les recevoir). Les paquets stockés dans la file subissent un délai d'attente le temps que ce soit à leur
tour d'être envoyé et après un temps la file sera pleine, dès lors les paquets reçus seront droppés.

\subsubsection{Délai}

Il y a 4 raisons possibles à un délai supplémentaire,
\begin{itemize}
\point{Travail par n\oe ud (nodal processing)}{Le travail consistant à la vérification d'erreur et à la
détermination du lien de sortie.}
\point{Mise en file (queueing)}{Attente dans la file pour être envoyé sur le lien de sortie.}
\point{Délai de transmission}{Le temps de transmission est le temps pris par un routeur pour placer le paquet 
sur le lien de sortie multiplié par le nombre de routeurs jalonnant le chemin.}
\point{Délai de propagation}{Le temps de propagation est le temps mis par le paquet pour parcourir un lien 
physique.}
\end{itemize}

\imgR{CN_007.png}{300}

\subsection{Couches de protocoles}

Pourquoi faire des couches ? La raison est simple, dans le traitement de systèmes complexes, la structure 
explicite permet l'identification ainsi que la relation entre les pièces du système complexe ; de plus, la 
modularisation facilite la maintenance et la mise à jour du système (modifier l'implémentation d'une couche est 
totalement transparent au reste du système). Voici les couches définies pour les protocoles Internet : 

\begin{center}
	\begin{tabular}{|*{3}{c|}}
	\hline
	\dred{\underline{Nom}} & \dred{\underline{Descriptions}} & \dred{\underline{Exemples}} \\
	\hline
	Application & Application utilisant Internet & FTP, SMTP, HTTP\\
	\hline
	Transport & Transfert de données entre processus & TCP, UDP\\
	\hline
	Network & Routage de datagrammes de la source à la destination & IP, routing protocols\\
	\hline
	Link & Transfert de données entre des éléments d'un réseau qui sont voisins & PPP, Ethernet\\
	\hline
	Physical & Les bits sur le fil & /\\
	\hline
	\end{tabular}
\end{center}

\img{CN_008.png}

\subsection{Sécurité}

Ce point traite des manières dont un réseau d'ordinateur peut être attaqué, comment on peut le défendre contre 
les attaques ainsi que comment construire des architectures qui sont immunisées contre les attaques. Internet n'a
pas été conçu avec la sécurité en tête et maintenant il y a des considérations dans chaque couche de protocole.

Mis à part les habituels vers, virus et cheval de Troie ; les «personnes malveillantes» peuvent attaquer des 
serveurs et une infrastructure de réseau avec un \textbf{DoS} \textit{(Denial of Service)}, \textit{Packet 
Sniffing}, \textit{IP Spoofing} et \textit{Record-and-playback}.

\subsubsection{DoS}

Il s'agit d'envoyer beaucoup de messages à une cible, la submergeant de messages venant d'hôtes compromis et 
l'empêchant de traiter d'autres paquets.

\imgRT{CN_009.png}{150}

\newpage
\subsubsection{Packet Sniffing}

Ca consiste à enregistrer tous les paquets passant par le routeur.

\imgR{CN_010.png}{300}

\subsubsection{IP Spoofing}

Envoyer des données avec une fausse adresse IP source.

\imgR{CN_011.png}{300}

\subsubsection{Record-and-playback}

«Sniffer» des informations sensibles et les utiliser plus tard (comme des mots de passe) ; du point de vue du 
système le «sniffeur» est le possesseur du mot de passe.

\imgR{CN_012.png}{300}

\hbox{\raisebox{0.4em}{\vrule depth 0.4pt height 0.4pt width 10cm}}

\section{La couche application (Application Layer)}

Commençons ce chapitre par donner quelques exemples d'applications réseau : \textit{e-mail, Web, messagerie 
instantanée, client P2P, jeux multijoueurs, streaming, ...} Pour implémenter ces applications, il y a différentes
architectures déjà citées plus haut : 

\subsection{Architectures des applications}

\subsubsection{Client-Serveur}

Le serveur est un hôte qui est tout le temps connecté au réseau avec un adresse IP permanente, quant aux clients,
ils sont connectés par moment avec une IP dynamique et communiquent avec le serveur, mais jamais entre eux.

\subsubsection{Peer-to-Peer (P2P)}

Il n'y a ici pas d'hôte toujours connecté, il s'agit de communication entre 2 utilisateurs se trouvant au bout de
la toile. Ces 2 utilisateurs peuvent être connectés par moment et ne pas l'être par d'autres et avoir une IP 
dynamique. C'est très évolué mais pas évident à gérer (si l'un des 2 change d'IP l'autre ne sait plus lui envoyer
de données).

\subsubsection{Hybride}

\textbf{Skype}, application Voice-over-IP P2P, maintient un serveur centralisé dans lequel se trouve les adresses
des personnes connectées et contactables. Un utilisateur se connecte au serveur et demande l'adresse d'une 
personne, le serveur lui donne et il contacte directement l'utilisateur (les 2 pairs discutent directement, ils 
ne passent plus par le serveur). En ce qui concerne les messageries instantanées, le principe est très semblable.

\subsection{Processus de communication}

Un processus est un programme tournant au sein d'un hôte. Lorsqu'un processus d'un hôte et un processus d'un 
autre hôte désirent communiquer, ils communiquent en échangeant des \dred{messages}. On distingue dès lors le 
processus initialisateur de la communication appelé \textbf{processus Client} de celui qui attend d'être 
contacté appelé \textbf{processus Serveur}. Ces 2 processus échangent des messages via ce que l'on appelle des
\textbf{sockets} que l'on compare à des portes. Envoyer un message correspond à pousser un message de l'autre 
coté de la porte et recevoir un message correspond à recevoir un «colis» de l'autre coté de la porte et il faut
ouvrir la porte pour pouvoir en connaître le contenu (le transport étant assuré par l'infrastructure se trouvant
de l'autre coté de la porte).

\subsubsection{Adressage de processus}

Pour recevoir des messages, les processus doivent être identifiés de manière unique. On est capable d'identifier
l'hôte de manière unique via l'adresse IP (unique, codée sur 32 bits) mais un seul hôte possède plusieurs 
processus ; on introduit donc la notion de ports. Chaque processus est lié à un port sur la machine, et pour 
communiquer avec lui, l'hôte étranger doit spécifier à la fois l'hôte qu'il souhaite contacter via son adresse IP
ainsi que le port du processus qu'il souhaite joindre. (Certains ports sont réservés, par exemple le port 80 pour
HTTP et le port 25 pour les mails)

\imgRT{CN_013.png}{200}

\subsubsection{Protocoles}

Les protocoles de la couche d'application définissent : 
\begin{itemize}
\point{le type de message échangé}{requête, réponse,..}
\point{la syntaxe des messages}{quels champs sont nécéssaires et comment ils sont séparés}
\point{la sémantique des messages}{la signification de l'information présente dans les messages}
\point{les règles pour quand et comment un processus envoie ou reçoit un message}{ignorer, répondre, ...}
\end{itemize}

\noindent Il en existe de différents types, des publiques comme HTTP, d'autres propriétaires comme Skype.

\subsection{Services de la couche transport nécessaires}

Les applications ont des besoins divers, certaines demandent un débit garantit (applications \dred{non-
élastiques}) d'autres pas (applications \dgre{élastiques}), d'autres ne tolèrent pas de pertes de données 
d'autres si, d'autres demandent de respecter un timing d'autres non et finalement, d'autres nécéssitent une 
sécurité accrue et d'autres non.

\imgR{CN_014.png}{250}

Voici quelques applications de base et leurs besoins :

\begin{center}
	\begin{tabular}{|*{4}{c|}}
	\hline
	\dred{\underline{Application}} & \dred{\underline{Perte de données}} & \dred{\underline{Débit}} & 
	\dred{\underline{Timing à respecter}} \\	 
	\hline
	Transfert de fichiers & Pas de pertes & Elastique & Aucun \\
 	\hline
	Mails & Pas de pertes & Elastique & Aucun \\
	\hline
	Documents Web & Pas de pertes & Elastique & Aucun \\
	\hline
	Audio/Vidéo Temps-Réel & Perte tolérée & $5$kbps-$1$Mbps/$10$kbps-$5$Mbps & Quelques $100$aines de msec\\
	\hline
	Audio/Vidéo stockées & Perte tolérée & $5$kbps-$1$Mbps/$10$kbps-$5$Mbps & Quelques secondes \\
	\hline
	Jeu interactif & Perte tolérée & Quelques kbps & Quelques $100$aines de msec\\
	\hline
	Messagerie instantanée & Pas de pertes & Elastique & Oui et non\\
	\hline
	\end{tabular}
\end{center}

Pour gérer ces différents besoins, la couche transport nous offre 2 protocoles différents : \dred{TCP} et 
\dred{UDP}. \\
$ $ \\
$ $ \\

\underline{\textbf{TCP}}
\begin{itemize}
\item orienté connexion, des réglages entre le processus client et le processus serveur sont requis,
\item transport de confiance entre les processus client et serveur,
\item contrôle de flux, l'expéditeur ne peut pas submerger le récepteur,
\item contrôle de la congestion, ralentissement de l'expéditeur lorsque le réseau est surchargé,
\item aucune garantie de timing, débit minimum ni de sécurité.
\end{itemize}

\underline{\textbf{UDP}}
\begin{itemize}
\item transport non fiable entre les 2 processus,
\item ne permet pas de configurer la connexion, ne garantit aucune fiabilité, de contrôle de flux, de contrôle
de congestion, aucun timing ni de sécurité.
\end{itemize}

\begin{center}
	\begin{tabular}{|*{3}{c|}}
		\hline
		\textbf{\underline{Application}} & \textbf{\underline{Protocole couche application}} & 	
		\textbf{\underline{Protocole transport}} \\
		\hline
		e-mail & SMTP [RFC 2821] & TCP \\
		\hline
		administration d'accès à distance & Telnet [RFC 854] & TCP \\
		\hline
		Web & HTTP [RFC 2616] & TCP \\
		\hline
		Transfert de fichier & FTP [RFC 959] & TCP \\
		\hline
		Multimédia en streaming & HTTP (eq. Youtube) \& RTP [RFC 1889] & TCP or UDP \\
		\hline
		Téléphonie Internet & SIP,RTP, proprietary(Skype par ex) & Typiquement UDP  \\		
		\hline
	\end{tabular}
\end{center}

\newpage

\subsection{Web \& HTTP}

Chaque objet constituant une page Web (texte, photo, applet JAVA, ...) est identifié par une \textbf{URL} 
(Uniform Resource Locator) ou une \textbf{URI} (Uniform Resource Identifier). Pour afficher une page Web, le 
protocole utilisé est \textbf{HTTP}(HyperText Transfer Protocol) qui se base sur le modèle client/serveur. Le 
client démarre un browser Internet qui demande, reçoit et affiche les pages Web et le serveur répond aux
requêtes reçues. Le protocole HTTP utilise le protocole TCP pour le transport des informations, le client 
initialise une connexion TCP vers le serveur sur le port 80, le serveur accepte celle-ci, ensuite les 2 
processus s'envoient des messages tant que la connexion TCP n'est pas coupée. On précise également que HTTP est
«stateless» c'est-à-dire qu'il ne maintient aucune information à propos des requêtes précédentes d'un client. On 
introduit à présent le \textbf{RTT}\textit{(Round Trip Time)} : temps mis par un paquet pour aller du client 
au serveur et en revenir, nous allons nous en servir pour comparer les 2 types de connexion HTTP :
\begin{itemize}
\point{Non-persistent HTTP}{Au plus un objet est envoyé par la connexion TCP. C'est-à-dire que pour chaque objet
à télécharger le client initialise une nouvelle connexion TCP, il faudra donc 1 RTT pour initialiser la 
connexion, ainsi qu'un RTT afin d'envoyer et recevoir le fichier. On y ajoute également le temps de transfert du
fichier que l'on nomme T. Au final le temps de réponse (donc le temps pour demander et recevoir un objet de la 
page) sera de 2RTT+T. On se rend donc compte qu'il y a un overhead par objet téléchargé cependant souvent les
browsers initialisent parallèlement plusieurs connexions TCP pour récupérer chaque objet de la page.}
\point{Persistent HTTP}{Plusieurs objets peuvent être envoyés sur une seule connexion TCP. Plusieurs objets sont
envoyés sur la même connexion (donc s'il y a 10 images à télécharger, les 10 peuvent être envoyés via la même
connexion). Dans ce cas ci, le serveur laisse la connexion ouverte après avoir répondu au client et ce dernier
envoie une requête au serveur dès qu'il rencontre un objet à charger. On aura donc besoin d'un seul RTT pour 
chaque objet !} \\
\end{itemize}

\noindent Le schéma qui suit représente les comportements des 2 types de \textbf{HTTP}, on peut ainsi remarquer 
de manière graphique que le \textbf{persistent HTTP} necéssite moins de temps pour charger une page.

\imgR{CN_015.png}{300}

\subsubsection{Messages HTTP}

\textbf{\underline{Messages «request»}} \\

\noindent Les messages sont formatés en \textbf{ASCII} (lisible par l'homme). Voici un exemple type de 
requête : 
\begin{lstlisting}
1  GET /somedir/page.html HTTP/1.1
2  Host: www.someschool.edu
3  User-agent: Mozilla/4.0
4  Connection: close
5  Accept-language: fr
6  
7  
\end{lstlisting}
\textit{(Remarque : Les lignes ne sont numérotées que pour les besoins de l'exemple)}

\begin{enumerate}
\item Ligne de la requête, il existe plusieurs commandes : \textbf{GET}, \textbf{POST}, \textbf{HEAD} ainsi 
que \textbf{PUT} et \textbf{DELETE} depuis \textbf{HTTP/1.1}. 
\begin{itemize}
\point{GET}{permet d'obtenir une page Web,}
\point{POST}{permet de donner des données à la page Web (lors de l'emploi de formulaire),}
\point{HEAD}{permet de demander au serveur de laisser tomber l'objet qui ne répond pas,}
\point{PUT}{permet d'uploader le fichier spécifié dans le corps du message à l'URL spécifiée,}
\point{DELETE}{permet de supprimer le fichier spécifié dans l'URL.}
\end{itemize}
\item Demandeur de la page.
\item Browser Internet utilisé (uniquement pour des statistiques).
\item Indique au serveur de fermer la connexion après avoir répondu.
\item Langage(s) préféré(s).
\item Retour à la ligne pour signifier la fin du header.
\item Retour à la ligne pour indiquer la fin du message. (CR LF en BNF)
\end{enumerate}

\newpage
De manière générale : \\
\imgR{CN_016.png}{350}

\textbf{\underline{Messages «response»}} \\

\noindent Voici un exemple type : 
\begin{lstlisting}
1  HTTP/1.1 200 OK
2  Connection: close
3  Date: Thu, 06 Aug 1998 12:00:15 GMT
4  Server: Apache/1.3.0 (Unix)
5  Last-Modified: Mon, 22 Jun 1998
6  Content-Length: 6821
7  Content-Type: text/html
8
9  data ...
\end{lstlisting}

\noindent La première ligne correspond au statut, les suivantes sont assez claires, la 4ème détaille le serveur 
utilisé pour les statistiques. Concernant le statut, il y a plusieurs codes, dont notamment :
\begin{itemize}
\point{200 OK}{la requête a réussi, le réponse contient l'objet demandé.}
\point{301 Moved Permanently}{l'objet désiré a été déplacé, la nouvelle location de l'objet est spécifié dans
le message.}
\point{400 Bad Request}{le message de requête n'a pas été compris par le serveur.}
\point{404 Not Found}{l'objet recherché n'a pas été trouvé sur le serveur.}
\point{500 HTTP Version Not Supported}{assez équivoque...} \\
\end{itemize}

\noindent De manière générale :

\imgR{CN_017.png}{300}

\subsubsection{Les cookies}

Les cookies sont des fichiers stockés sur l'ordinateur d'un hôte et qui sont gérés par le browser Internet. Ils
peuvent contenir des autorisations, des cartes de shopping, des recommandations, l'état de la session d'un 
utilisateur (Web e-mail). Ils permettent au protocole \textbf{HTTP} de conserver un état de manière indirecte 
(vu que le protocole ne le permet pas à la base). Voici un exemple de l'utilisation de cookies :

\imgR{CN_018.png}{300}

\subsubsection{Les caches Web (serveurs proxy)}

Dans les réseaux munis d'un serveur proxy, toute requête \textbf{HTTP} est envoyée au serveur proxy et ce 
dernier va adopter un comportement différent en fonction de la disponibilité de la page dans sa mémoire interne. 
Soit il possède la page demandée en stock, dans ce cas il l'envoie directement sinon il la demande au serveur 
originel (le serveur du site), la stocke et l'envoie au client. C'est assez utile car ça diminue le 
temps de réponse nécéssaire pour récupérer une page et ainsi que le nombre de transferts sur les liens d'accès 
des institutions. Il faut aussi prendre en compte que la copie stockée par le serveur proxy n'est peut-être pas 
à jour, c'est ainsi que l'on introduit le \textbf{GET conditionnel}. Cette requête est envoyée par le serveur 
proxy au serveur originel signifiant d'envoyer une page si elle a été modifiée depuis la dernière copie faite, si 
ce n'est pas le cas le serveur originel répond avec un statut \textbf{«304 Not Modified»} sinon il répond avec 
un statut \textbf{«200 OK»} et joint la nouvelle version de la page.

\imgRT{CN_019.png}{150}

\subsection{FTP (File Transfer Protocol)}

\textbf{FTP} est le protocole utilisé pour les transferts de fichiers entre hôtes à distance. Il est également 
basé sur le modèle client/serveur (le client étant celui qui initialise le transfert) et utilise le port 21 avec 
des connexions \textbf{TCP}. Le client initialise une connexion \textbf{TCP} avec le serveur appellée 
\textit{connexion de contrôle}, avec cette connexion le client peut parcourir l'arborescence proposée par le 
serveur. Quand le serveur reçoit une demande de transfert de fichier, il initialise une seconde connexion 
\textbf{TCP} avec le client appellée \textit{connexion de données} via laquelle il va envoyer le fichier et dès
que celui-ci est envoyé et reçu par le client, il ferme la connexion (il en réouvrira une autre si un autre 
fichier est désiré). Précisons également que \textbf{FTP} maintient un état, en effet, il maintient le 
répertoire courant ainsi qu'une identification antérieure.

\subsection{E-Mails (SMTP, POP3, IMAP)}

Il y a 3 composants dans une communication mail : les agents utilisateurs, les serveurs mail et le protocole 
pourl'envoi de mail : SMTP (Simple Mail Transfer Protocol). Les \textbf{agents} sont les programmes comme 
Outlook et Thunderbird (ou autres) qui permettent de composer, d'éditer et de lire des mails ; les mails étant
stockés sur le serveur mail. Ces derniers contiennent une boite aux lettres contenant les messages destinés aux 
utilisateurs et une file de messages attendant d'être envoyés. Ils utilisent le protocole \textbf{SMTP} pour 
communiquer entre eux, sur le modèle client/serveur. Les messages échangés sont toujours placés sur le serveur
mail, pour les lire, le client doit faire appel à un agent.\\

\noindent\textbf{\underline{Le protocole SMTP}}
\begin{itemize}
\item utilise \textbf{TCP} sur le port 25 pour le transfert,
\item transfert direct, du serveur expéditeur au serveur récepteur,
\item 3 phases de transfert : salutations, transfert du/des messages, fermeture.
\item interaction en commande/réponse : la commande est formatée en ASCII 7bits et la réponse est un code de
statut et une phrase \textit{(comme HTTP)}.
\item utilise des connexions persistentes,
\item utilise CR LF . CR LF pour déterminer la fin d'un message.
\end{itemize}
\textit{Contrairement à HTTP, un message peut contenir plusieurs objets.}

\noindent L'accès au serveur mail se fait également via un protocole, il y en a plusieurs : 
\textbf{POP}\textit{(Post Office Protocol)} (autorisation et téléchargement), \textbf{IMAP}\textit{(Internet 
Mail Access Protocol)} (plus de caractéristiques, plus complexes, manipulation de messages stockés sur le 
serveur), \textbf{HTTP} (Gmail,Hotmail,...). \textbf{POP3} ne tient aucun état des sessions des utilisateurs 
contrairement à \textbf{IMAP}. En effet, il permet aux utilisateurs d'organiser les messages en dossiers, il 
tient donc l'état des dossiers ainsi que le mapping entre les ID's et les dossiers.

\subsection{DNS (Domain Name System)}

Comme vu précédemment, pour identifier un élément connecté à Internet, on utilise son adresse IP. Or, lorsque 
l'on utilise Internet on entre pas des adresses IP mais des noms d'hôte (adresses Internet) du genre 
\textit{www.google.be}. Le \textbf{DNS} sera le dispositif faisant le lien entre un nom d'hôte et l'adresse IP
correspondante. Le \textbf{DNS} est une base de données distribuée organisée en hiérarchie de beaucoup de noms
de serveur.

\imgR{CN_020.png}{300}

\noindent Imaginons qu'un client cherche à joindre \textit{www.amazon.com}, le client demande à un «root DNS 
server» de trouver l'adresse IP correspondante, celui-ci ne la possédant pas, il répond au client qu'il ne sait 
pas, le client s'adresse alors au serveur \textbf{DNS} «.com» avec le même résultat et finit par s'adresser au 
serveur \textbf{DNS} «amazon.com» qui lui donne l'adresse IP à joindre. \textit{(C'est une première approche)}

\textbf{DNS} fournit plusieurs services dont la traduction d'un nom d'hôte en adresse IP, l'aliasing d'hôte, 
l'aliasing de serveur de mail ainsi que la répartition de charges (serveurs Web répliqués permettant d'associer
un ensemble d'adresses IP à un seul nom d'hôte). \textbf{DNS} n'est pas centralisé pour éviter que le serveur
unique soit submergé, de plus le serveur serait assez distant pour pas mal d'utilisateurs. On peut également y 
voir une difficulté pour la maintenance, l'autonomie ainsi que l'évolutivité.

Il y a différents types de serveurs \textbf{DNS} :
\begin{itemize}
\point{Local Name Servers}{Ils n'appartiennent pas vraiment à la hiérarchie, chaque ISP résidentiel, compagnie,
université en possède un (On les appelle également les «default name servers»). Lorsqu'un hôte effectue une 
requête \textbf{DNS} elle est automatiquement envoyée au Local Name Server (celui-ci agissant comme un serveur
proxy). Celui-ci obtient soit la réponse de manière itérative soit de manière récursive. Dans le modèle 
itératif, chaque serveur qu'il interroge lui répondant qu'il ne connait pas la réponse lui indique également un
autre serveur à interroger. Dans le modèle récursif, il interroge un \textbf{root DNS Server}, et si celui-ci ne 
connait pas la réponse, il la demande à un serveur \textbf{TLD}, si ce dernier ne connait pas la réponse, il la
demande à un \textbf{authoritative DNS server} ; ensuite le message de réponse contenant le mapping entre l'hôte
et l'adresse IP remonte les différents serveurs jusqu'au \textbf{Local Name Server}.}
\point{Root name servers}{Ils sont contactés par les «local name server» qui ne savent pas résoudre un hôte, si 
le «Root name server» ne parvient pas à résoudre l'hôte, il contacte un «authoritative name server». Une fois 
qu'il a eu la réponse du serveur ou s'il pouvait résoudre lui même l'hôte, il envoit le mapping au «local name 
server» ayant fait la demande.}
\point{Authoritative DNS servers}{Ce sont les serveurs \textbf{DNS} associés à certaines organisations.}
\point{Top-level domain servers (TLD)}{Ce sont les serveurs responsables des hôtes «.com», «.net», «.edu», ... 
ainsi que tous les serveurs des pays comme «.be», «.nl», ...}
\end{itemize}

\subsubsection{DNS Records}

Les serveurs \textbf{DNS}, une fois qu'ils ont appris un mapping entre un hôte et une adresse IP, stocke ce 
mapping en mémoire cache. Ces entrées dans la mémoire cache disparaissent après un certain temps, le 
\textbf{TTL}\textit{(Time To Live)}. Les entrées de la base de données \textbf{DNS} sont appelées des 
\textbf{Resource Records}\textit{(RR)}, elles contiennent un nom, une valeur, un type et un ttl. Selon le type,
l'entrée a un sens différent : \\
\begin{itemize}
\point{Type = A}{Le champ \blu{nom} contient un nom d'hôte et \blu{valeur} contient l'adresse IP correspondante.}
\point{Type = NS}{Le champ \blu{nom} contient un nom de domaine et \blu{valeur} contient le nom d'hôte de 
l'authoritative server pour ce domaine.}
\point{Type = CNAME}{Le champ \blu{nom} contient un alias et \blu{valeur} le nom canonique de l'alias.}
\point{Type = MX}{Le champ \blu{valeur} contient le nom du serveur mail associé avec le contenu du champ
\blu{nom}.}
\end{itemize}

\textbf{DNS} est également un protocole spécial utilisé pour les messages entre serveurs \textbf{DNS}, il 
utilise \textbf{TCP} ou \textbf{UDP} sur le port 53. Il présente la particularité que les messages requêtes ont
le même format que les messages réponses, voici un aperçu sommaire de la structure d'un message \textbf{DNS} :

\imgRT{CN_021.png}{150}

\subsection{Applications P2P}

Nous allons traiters 3 sujets : la distribution de fichiers, la recherche d'informations et Skype.

\subsubsection{Distribution de fichiers}

\imgR{CN_022.png}{300}

Si on se base sur un modèle serveur/client, le temps $D_{CS}$ pour que le serveur envoie à chacun des $N$ 
utilisateurs un fichier de taille $F$ sera dans le meilleur des cas égal au maximum entre le temps pour que le 
serveur envoie ses $N$ copies et que l'utilisateur ayant le moins de bande passante télécharge sa copie
entièrement : 
\begin{center}
$D_{CS} \geq \max{\left(\dfrac{NF}{u_s},\dfrac{F}{d_{min}}\right)}$
\end{center}

Basons nous à présent sur un système P2P, cette fois le serveur ne doit envoyer qu'une seule copie, dans le 
meilleur des cas les $NF$ bits transférés prennent toute la largeur des bandes passantes. Le temps $D_{P2P}$ 
pour que chacun des $N$ utilisateurs aie téléchargé sa copie du fichier est donc ici égal à :
\begin{center}
$D_{P2P} \geq \max{\left(\dfrac{F}{u_s},\dfrac{F}{d_{min}},\dfrac{NF}{u_s+\sideset{}{_{i=1}^N}\sum{u_i}}\right)}$
\end{center}

\textbf{\underline{BitTorrent}} \\

C'est un système P2P de distribution de fichiers, il est constitué de 2 parties, le \textbf{tracker} qui est en 
fait un serveur recenssant toutes les personnes participant au \textbf{torrent} qui est la seconde partie. Un 
\textbf{torrent} est un groupe de gens partageant des morceaux d'un même fichier. Typiquement un fichier est
divisé en morceaux de $256$Kb, quand un pair rejoint un torrent (en se connectant au tracker), il ne possède 
aucun morceaux, il va les accumuler au fur et à mesure. Au fur et à mesure qu'il les accumule, il en envoit 
certains à d'autres pairs ; une fois son téléchargement terminé, il peut choisir de quitter ou de rester 
connecté afin de permettre à d'autres pairs de profiter des morceaux qu'il possède pour télécharger le fichier. 

En ce qui concerne le téléchargement de morceaux, périodiquement, l'hôte va demander à ses voisins (dans le 
torrent) la liste des morceaux qu'il contient et ensuite envoyer des requêtes d'obtention pour ses morceaux 
manquants. Ensuite, l'hôte envoie à ses 4 voisins qui envoient au plus haut taux tous ses morceaux (réévaluation
du TOP4 toutes les 10 secondes) et une fois toutes les 30 secondes elle envoie au hasard à un 5eme pair, celui-
ci pouvant peut-être entrer dans le TOP4.

\subsubsection{Recherche d'informations - DHT}

\textbf{DHT}\textit{(Distributed Hash Table)} est une base de données distribuée contenant des couples 
(clé,valeur). Un pair peut demander une requete portant sur la clé, et la base de données lui renvoie la valeur; 
il peut également insérer un nouveau couple. Chaque clé est un entier pris dans $[0,2^n-1]$ et chaque pair a un 
numéro identifiant, lors de l'assignation des couples (clé,valeur), on assigne la clé $X$ à l'utilisateur qui 
est son successeur direct. Par exemple si $n=4$ et que l'on a les pairs identifiés par $1,3,4,5,8,10,12,14$, 
alors la clé $13$ sera attribuée à $14$ et la clé $15$ à $1$. 

\imgR{CN_023.png}{400}
\imgR{CN_024.png}{400}

\noindent On peut également implémenter une \textbf{DHT} avec des raccourcis, un pair connaissant également le
chemin vers un autre pair autre que ses voisins directs. On peut ainsi diminuer le nombre de messages à $O(\log 
n)$.

Si un pair se déconnecte, pour que la \textbf{DHT} reste à jour, il faut que chaque pair connaisse les 2 pairs
qui le suivent. Chaque pair envoie périodiquement des messages pour vérifier que ses 2 successeurs sont tous
les 2 toujours connectés. Si un pair vient à se déconnecter, son prédécesseur s'en rend compte et avertit son 
successeur restant qu'il est maintenant son successeur direct et qu'il a besoin de son successeur direct pour 
mettre à jour son 2ème successeur. Si un pair vient à se connecter, il signale à ses 2 successeurs qu'il est là,
et l'information se propage circulairement.

\subsubsection{Skype}

\imageRT{CN_025.png}{150} $\ \ \ \ \ \ \ \ \ $  \imageRT{CN_026.png}{200}

Le système \textbf{Skype} utilise un recouvrement hierarchique avec des supern\oe uds ainsi qu'un index mappant 
les utilisateurs avec leur adresse IP (probablement une \textbf{DHT}). Cette configuration permet d'éviter tout
problème de communication lorsque 2 pairs se connectant derrière des \textbf{NAT}'s essaient de communiquer. En
effet, le \textbf{NAT} empêche un pair externe d'initialiser un appel avec le pair interne, les 2 étant derrière
un \textbf{NAT} l'appel ne peut être initialisé ! La solution est donc simple, on utilise un autre utilisateur
comme relai comme sur la figure de droite.

\subsection{Programmation Socket avec TCP}

Voir Projet 1.

\subsection{Programmation Socket avec UDP}

Voir Projet 2. \\

\hbox{\raisebox{0.4em}{\vrule depth 0.4pt height 0.4pt width 10cm}}

\section{La couche transport (Transport Layer)}

Il s'agit de la couche s'occupant de la communication entre 2 processus distants, nous avons déjà vu les 2 
protocoles que l'on va étudier plus en profondeur : \textbf{TCP} et \textbf{UDP}. Le but des protocoles de 
transport est de permettre la communication entre 2 processus tournants sur 2 machines distinctes connectées à
Internet. Ce protocole est executé dans les extrêmités du réseau, l'expéditeur coupe les données à envoyer en
morceaux et les passe à la couche réseau, le récepteur effectue l'opération inverse. \textit{(La couche réseau 
s'occupe de la communication entre 2 hôtes, pas entre 2 processus !)}

\subsection{Multiplexing/Demultiplexing}

Le \textbf{multiplexing} est l'action consistant à réunir des données venant de sockets différents et de les 
envelopper avec un header. Le \textbf{démultiplexing} quant à lui sera l'action de délivrer les bons segments 
aux bons sockets. Pour ce faire, l'hôte reçoit des datagrammes IP contenant chacun les adresses source et 
destination ainsi qu'un segment de la couche transport contenant lui les ports source et destination. L'hôte
utilise alors les adresses \& ports source et destination pour diriger le segment au bon socket.
\begin{itemize}
\item \textbf{UDP} : Dans le cas d'\textbf{UDP}, les sockets sont identifiés par un couple \textit{(adresse 
destination, port destination)} ce qui a pour effet d'envoyer les paquets venant de n'importe quelle adresse 
source au même socket.
\item \textbf{TCP} : Dans le cas de \textbf{TCP}, les sockets sont identifiés par un $4$-upple \textit{(adresse 
source, port source, adresse destination, port destination)} permettant ainsi de diriger les paquets reçus sur 
le même port vers différents sockets selon l'adresse et le port source. (Utile par exemple dans les serveurs Web 
multithreadés)
\end{itemize}

\subsection{UDP (User Datagram Protocol)}

Il s'agit du protocole utilisant un service «best effort» (littéralement «on fait ce qu'on peut»), dès lors les
segments \textbf{UDP} peuvent être perdus ou délivrés dans un ordre différent que lors de l'envoi. Il n'existe 
aucune connexion entre les 2 hôtes concernés par la communication, il n'y a pas de \textit{«handshaking»} entre 
les 2 pairs et chaque segment est géré indépendamment des autres. Tout ceci n'apparaît pas très flatteur pour 
\textbf{UDP} et on pourrait dès lors se demander en quoi est-il utile ? La réponse est en 4 parties : 
\begin{itemize}
\item il n'y a pas d'établissement de connexion (et donc pas de délai supplémentaire),
\item le protocole est simple, il n'y a pas d'état de connexion chez l'expéditeur ou le receveur,
\item les headers des segments est petit $\rightarrow$ moins d'overhead,
\item aucun controle de congestion, \textbf{UDP} peut torpiller aussi vite que désiré.
\end{itemize}
\textbf{UDP} est ainsi souvent utilisé dans les applications de streaming multimédia (tolérant aux pertes et 
sensible à la vitesse de transfert) mais également dans le cadre de \textbf{DNS} et \textbf{SNMP}. Il est 
possible d'utiliser \textbf{UDP} pour faire du transfert fiable mais il faut alors ajouter cette fiabilité dans
la couche application sous la forme d'une application permettant le recouvrement d'erreur.

\subsubsection{Détection d'erreur : checksum}

Lors de l'envoi d'un paquet, l'expéditeur considère les données comme une séquence d'entiers codés sur 16 bits en 
réprésentation en complément à 1 et en fait la somme ; il place ensuite le résultat dans le paquet et l'envoie.
Le récepteur recalcule la somme et la compare avec la valeur contenue dans le paquet, si celle-ci est différente 
c'est qu'\textbf{au moins} un bit a été modifié lors du trajet.

La représentation à 1 d'un entier $x = b_{N-1}...b_0$ : 
\begin{center}
$x = \left\{ 
\begin{array}{l @{\ \ \ \ si\ } r}
\sumin{1}{N-1}{2^ib_i} & b_{N-1} = 0 \\
\left(\sumin{1}{N-1}{2^ib_i}\right) - \left(2^N-1\right) & b_{N-1} = 1 \\
\end{array}\right.$
\end{center}

Quant à l'addition, il faut les additionner comme s'ils étaient positifs, et si il y a un report, il suffit de 
rajouter 1 à la somme obtenue. Exemple : \imageRT{CN_027.png}{50}. La vérification de la checksum à l'arrivée du
paquet est faite en prenant le complément de la valeur trouvée dans le champ checksum et en y additionnant tous
les entiers du paquet, dès lors si le résultat est composé que de 1 alors la checksum est identique sinon ce
n'est pas le cas.

\subsection{Principes de transfert fiable de données}

Il s'agit d'un des 10 sujets les plus importants en matière de réseau. Tout le problème est d'envoyer via un 
canal non fiable des données de manière fiable... il faut développer un protocole qui permet cela, c'est ce
que nous allons faire de manière théorique.

\subsubsection{RDT (Reliable Data Transfer) - Protocole théorique}

\textbf{\underline{RDT 1.0}} : le canal utilisé est parfaitement fiable $\rightarrow$ pas d'erreurs de bits ni 
de pertes de paquets. Il n'y a aucune relation entre les 2 hôtes modifiant l'état d'un de ces deux-ci, 
l'expéditeur envoie des paquets via le canal et le récepteur les reçoit.

\imgR{CN_028.png}{300}

\textbf{\underline{RDT 2.0}} : le canal peut modifier des bits dans les paquets $\rightarrow$ checksum pour 
détecter les erreurs. Ensuite, pour recouvrir les erreurs, on introduit le système de 
\textbf{ACK}\textit{(Acknowledgements, message où le récepteur dit qu'il a bien reçu le paquet et que celui-ci
est ok)} et \textbf{NAK}\textit{(Negative acknowledgements, message où le récepteur dit qu'il y a une erreur 
dans le paquet)} ; l'expéditeur renvoyant le paquet défectueux à la réception d'un \textbf{NAK}.

\imgR{CN_029.png}{300}
Il y a cependant un problème, en effet, si un message \textbf{ACK} ou \textbf{NAK} est endommagé, l'expéditeur 
ne sait pas ce qui s'est passé du coté du récepteur et il ne peut renvoyer le paquet car il y a risque de 
duplication si c'était un \textbf{ACK}. Pour régler ce problème, l'expéditeur va tout de même renvoyer le paquet 
mais dans chaque paquet il ajoute un numéro de séquence afin que le receveur puisse savoir qu'il a déjà reçu le 
paquet et le dropper. (version 2.1)

\imgR{CN_030.png}{300}
\imgR{CN_031.png}{400}

\textit{(Politique du stop and wait, l'expéditeur envoie un paquet et attend la réponse du récepteur)} \\

Une autre façon de faire, afin de supprimer les \textbf{NAK}, est d'inclure le numéro de séquence du dernier 
paquet reçu dans l'\textbf{ACK} envoyé. Si l'expéditeur reçoit un \textbf{ACK} dupliqué c'est que le dernier 
paquet qu'il a envoyé n'a pas été reçu (= réception d'un \textbf{NAK}) et il le renvoit donc. Exemple : 

\imgR{CN_032.png}{300}

\newpage

\underline{\textbf{RDT 3.0}} : Le canal peut à présent également perdre des paquets, les paquets pouvant être
des données ou des \textbf{ACK}. Afin de pallier à ça, l'expéditeur va attendre pendant un moment «raisonnable»
que le récepteur lui envoie un \textbf{ACK} pour le paquet qu'il a envoyé. S'il ne le reçoit pas dans la fenêtre 
de temps définie, il renvoie le paquet. Si le paquet est juste en retard mais pas perdu, l'\textbf{ACK} sera 
reçu quand même trop tard et le paquet sera renvoyé mais ça ne pose pas de problèmes car les numéros de 
séquences permettent déjà de régler ce problème.

\imgR{CN_033.png}{400}

Regardons comment agit \textbf{RDT 3.0} dans différentes situations : \\
\underline{Aucune perte} :  \imgR{CN_138.png}{300}
\newpage
\underline{Perte de paquet} :  \imgR{CN_139.png}{300}
\underline{Perte d'\textbf{ACK}} :  \imgR{CN_140.png}{300}

\newpage

\underline{Timeout prématuré} :  \imgR{CN_141.png}{300}

Calculons maintenant les performances de \textbf{RDT 3.0}, prenons comme exemple un lien de $1$Gbps, $15$ ms de 
délai de propagation et des paquets de 8000 bits.
\begin{center}
$d_{trans} = \dfrac{L}{R} = \dfrac{8000 bits/paquets}{10^9bits/sec} = 8\mu s$
\end{center}
On calcule également la fraction du temps que l'expéditeur est occupé à envoyer des paquets : 
\begin{center}
$U_{sender} = \dfrac{d_{trans}}{RTT+d_{trans}} = \dfrac{8.10^{-6}s}{2.(1,5.10^-2)s + 8.10^{-6}s} = 0,00027$
\end{center}

On envoit donc un paquet de $1Kb$ toutes les 30 ms, ce qui fait une vitesse de $33Kbps$ sur un lien de $1Gbps$ ! 
Cela est dû au protocole utilisé, en effet, la politique du «stop\&wait» n'est pas très optimale ; on va donc 
étudier des «pipelined protocol» avec lesquels l'expéditeur accepte d'envoyer plusieurs paquets avant d'avoir 
reçu un \textbf{ACK} pour le premier paquet envoyé.

Effet du pipelining : imaginons que l'on envoie 3 paquets avant d'attendre les \textbf{ACK}'s, on aura donc : 
$U_{sender} = \dfrac{3.d_{trans}}{RTT+d_{trans}} \rightarrow$ multiplié par 3 ! (En effet, pendant le 
\textbf{RTT} du premier paquet, on envoie 2 autres paquets au lieu d'attendre à ne rien faire !)

Il existe 2 formes génériques de «pipelined protocol» :
\begin{itemize}
\point{Go-Back-$N$}{L'expéditeur peut avoir jusque $N$ paquets «non-\textbf{ACK}és» sur le lien, le récepteur 
quant à lui envoie des \textbf{ACK} cumulatifs, c'est-à-dire que s'il y a un «trou» il n'en envoit pas. 
L'expéditeur tient un timer pour le plus ancien paquet qui n'a pas encore été «\textbf{ACK}é» et si ce timer
expire, l'expéditeur renvoie tous les paquets qui n'ont pas été \textbf{ACK}és.}
\point{Selective Repeat}{L'expéditeur peut avoir jusque $N$ paquets «non-\textbf{ACK}és» sur le lien, le 
récepteur \textbf{ACK} les paquets individuellement. L'expéditeur tient un timer pour chaque paquet 
non-\textbf{ACK}é, si un timer tombe à 0, il renvoie uniquement le paquet concerné.}
\end{itemize}

\subsubsection{Go-Back-$N$}

L'expéditeur possède des numéros de séquences codés sur $k$ bits et il maintient une fenêtre de taille $N$ dans
laquelle il tient les paquets envoyés et non-\textbf{ACK}és ainsi que les paquets envoyables. Dès que 
l'\textbf{ACK} du premier paquet de la fenêtre est reçu, il déplace la fenêtre d'un rang, permettant ainsi à un
nouveau paquet d'être envoyé.

\imgR{CN_038.png}{400}
\begin{center}\underline{Expéditeur}\end{center}
\imgR{CN_039.png}{400}
\begin{center}\underline{Récepteur}\end{center}
\imgR{CN_040.png}{400}

\newpage

Lorsque que le récepteur reçoit un paquet qui n'est pas dans l'ordre, il le drop (pas de buffer !) et il renvoie
un \textbf{ACK} pour le paquet de plus haut numéro de séquence dans l'ordre. Voici un exemple avec $N=4$ : 

\imgRT{CN_041.png}{300}

\subsubsection{Selective Repeat}

Dans ce cas-ci, le récepteur stocke dans un buffer tous les paquets qu'il reçoit, même s'ils sont pas dans 
l'ordre et l'expéditeur ne renvoit que les paquets dont il n'a pas reçu les \textbf{ACK}'s.

\imgR{CN_042.png}{400}
\newpage
L'expéditeur fonctionne donc de la façon suivante : il regarde si le prochain numéro de séquence disponible est
dans sa fenêtre d'émission, si c'est le cas il envoie le paquet ; sinon il attend un \textbf{ACK}. Lorsqu'un 
timer associé à un paquet envoyé tombe à 0 avant que l'\textbf{ACK} soit arrivé, il renvoie le paquet concerné. 
Lorsqu'il reçoit un \textbf{ACK} il marque le paquet correspondant comme \textbf{ACK}é et si c'est le premier 
de sa fenêtre d'émission (celui d'indice \textit{sendbase}), il avance sa fenêtre d'une unité \\
\textit{(sendbase $\leftarrow$ sendbase$+1$)}. Le récepteur bufferise tous les paquets qu'il reçoit et qui ne 
sont pas dans l'ordre et délivre à la couche supérieure le paquet reçu lorsque c'est le premier de sa fenêtre de
réception (et si les suivants sont déjà là, il les délivre aussi). Ensuite, il avance sa fenêtre de réception
jusqu'au prochain paquet «pas dans l'ordre». (Si le paquet reçu n'est pas dans la fenêtre de réception, il est 
ignoré) Voici un exemple avec $N=4$ : 

\imgR{CN_043.png}{400}
\newpage
\underline{\textbf{Dilemme}}

Il y a un dilemme, les 2 situations qui suivent sont différentes, et pourtant le récepteur ne voit aucune 
différence alors que dans la 1ère situation le paquet de séquence 0 est une donnée dupliquée et non une nouvelle, 
cependant, elle est considérée comme telle (les numéros de séquence sont $0,1,2,3$ et la taille de la fenêtre est 
de $3$).

\imgR{CN_187.png}{400}
\imgR{CN_188.png}{400}

Cette situation est en fait liée aux paramètres que l'on a fixés (les numéros de séquence et la taille de la 
fenêtre). Les tailles des fenêtres doivent être égales ($N$) et elles doivent être $\leq$ à la moitié du maximum 
des numéros de séquences disponibles. Considérons que les numéros de séquences sont numérotés de $0$ à 
$x$, on a donc la relation : $N\leq (x$ div $2)$. Si on ne respecte pas cela, lors de la perte de tous les 
\textbf{ACK}, 1 voire tous les paquets qui seront réenvoyés sera/seront considéré(s) comme une/des nouvelle(s) 
donnée(s) alors que ce n'est pas le cas !

\subsection{TCP (Transmission Control Protocol)}

\underline{\textbf{Caractéristiques}} \\
\begin{itemize}
\item \textbf{point-to-point} : \textit{un expéditeur, un récepteur},
\item \textbf{flux de bytes fiable conservant l'ordre},
\item \textbf{pipelined} : \textit{le contrôle TCP de la congestion et de flux fixe la taille de la fenêtre},
\item \textbf{buffers d'envoi et de réception},
\item \textbf{full duplex data} : \textit{flux de donnée bidirectionnel sur la même connexion \\
(MSS : Maximum Segment Size)},
\item \textbf{orienté connexion} : \textit{«handshaking», c'est-à-dire que les 2 pairs s'échangent des messages
de contrôles et établissent leur état respectif avant l'échange de données},
\item \textbf{flux controlé} : \textit{l'expéditeur ne va pas innonder le récepteur}. \\
\end{itemize}

\underline{\textbf{Structures des messages}}

\imgR{CN_045.png}{300}

Lors de l'envoi de messages \textbf{TCP}, ceux-ci ne dépassent pas \textit{une certaine taille}, cette taille
c'est le \textbf{MSS}. Le \textbf{MSS} est donc la taille maximale d'un segment, il est contraint par le 
\textbf{MTU} \textit{(Maximal Transmission Unit)}, la taille du plus grand payload de la couche lien 
\textit{(spécifique à chaque connexion)}. Typiquement, 
\begin{center}$MSS = MTU-sizeof(TCP/IP\ header) = MTU-40$.\end{center}

Le header \textbf{TCP} est assez volumineux, dès lors, lors de l'envoi de petits paquets, l'overhead (données 
nécessaires ajoutées aux données de départ) est énorme. Par exemple, dans le cas de \textbf{telnet} on envoie
1 caractère à la fois, c'est-à-dire \textbf{1byte}, le header faisant \textbf{40 bytes}, on se retrouve avec un
overhead de \dred{$4000\%$}. La solution (simplifiée) est l'algorithme de Nagle, qui vise à attendre que 
\textbf{MSS} caractères soient dans le buffer d'envoi avant d'englober le tout dans un seul paquet \textbf{TCP} 
(sauf si les données sont «poussées» ou si il n'y a aucun élément non-\textbf{ACK}é).

Les messages \textbf{TCP} contiennent un numéro de séquence qui est un identifiant pour le message (il s'agit du
numéro du premier bit envoyé depuis le début de la connexion), ils contiennent également un nombre 
(\textbf{ACK}) qui spécifie l'identifiant que le message réponse doit posséder.
Quand aux messages qui arrivent dans le mauvais ordre, \textbf{TCP} ne spécifie pas quoi en faire, c'est à 
l'implémenteur de le spécifier.

\imgRT{CN_046.png}{200}

\subsubsection{RTT \& Timeout}

La valeur à fixer pour le \textbf{timeout} est très importante, en effet, une valeur trop courte provoquerait 
des retransmissions non-nécéssaires et une valeur trop grande aurait pour effet de rendre le protocole lent à 
réagir aux pertes de paquets. L'idée est de le fixer à une valeur plus grande que le \textbf{RTT}, le seul
problème est que le \textbf{RTT} varie ! On va donc chercher à évaluer le \textbf{RTT} de la manière la plus 
précise possible. On procède la manière suivante :
\begin{itemize}
\item On calcule \textbf{RTTEchantillon}, le temps mis pour la transmission d'un segment jusqu'à la réception de 
l'\textbf{ACK} correspondant,
\item on calcule le \textbf{RTTEstime} comme : $RTTEstime = (1-\alpha).RTTEstime+\alpha.RTTEchantillon$
\textit{(typiquement $\alpha = 0,125$)},
\item on calcule la déviation entre RTTEchantillon et RTTEstime : $DevRTT = (1-\beta).DevRTT+\beta. |
RTTEchantillon-RTTEstime|$ (typiquement $\beta = 0,25$),
\item finalement on fixe la valeur du timeout à \dred{$RTTEstime+4.DevRTT$}.
\end{itemize}

\subsubsection{Transfert fiable}

\textbf{TCP} a créé un service fiable au dessus de la couche \textbf{IP} dont les services ne le sont pas. Il 
utilise des segments \textbf{pipelined} (plusieurs segments envoyés avant de recevoir un quelconque 
\textbf{ACK}), le principe d'\textbf{ACK}'s cumulatifs et un seul timer pour la retransmission. Dès lors les
retransmissions sont déclenchées par des évènements de timeout ou des \textbf{ACK}'s dupliqués. Voici un 
algorithme simple et simplifié détaillant l'agissement du protocole \textbf{TCP} du coté envoi de données : 

\imgRT{CN_047.png}{225}

Et à présent les différents cas de retransmissions de paquets pour \textbf{TCP} : \\

\imageRT{CN_048.png}{175}
\imageRT{CN_049.png}{175}
\imageRT{CN_050.png}{175}

\textit{Remarque : après chaque retransmission due à un timeout, \textbf{TCP} double la valeur de celui-ci.}

Du coté du récepteur, voici les comportements adoptés :
\begin{itemize}
\item \textit{Arrivée d'un paquet dans l'ordre avec le numéro de séquence attendu et les \textbf{ACK} pour tous 
les paquets précédents ont déjà été envoyé.} \\$\rightarrow$ \textbf{Le récepteur attend $500$ms pour le segment 
suivant, s'il ne le reçoit pas il envoit l'\textbf{ACK} du segment reçu.}
\item \textit{Arrivée d'un paquet dans l'ordre avec le numéro de séquence attendu mais l'\textbf{ACK} pour 
un des paquets précédents n'a pas été envoyé.} \\$\rightarrow$ \textbf{Envoi direct d'un \textbf{ACK} cumulatif 
permettant d'ACKer les 2 segments en même temps.}
\item \textit{Arrivée d'un paquet en dehors de l'ordre avec un numéro de séquence plus grand que celui attendu 
(trou détecté).} \\$\rightarrow$ \textbf{Envoi direct d'un \textbf{ACK} dupliqué indiquant le numéro de séquence 
du paquet attendu.}
\item \textit{Arrivée d'un paquet remplissant partiellement ou completement un trou.} \\$\rightarrow$ 
\textbf{Envoi direct d'un ACK à condition que le segment commence au début du trou.}
\end{itemize}

\noindent Lorsqu'un paquet est perdu, le timeout est parfois très long, alors \textbf{TCP} utilise la politique 
du \textbf{Fast Retransmit}. Cette politique dit que si un segment est perdu, vu que l'expéditeur envoie 
beaucoup de segments les uns à la suite des autres, celui-ci va recevoir beaucoup d'\textbf{ACK} dupliqués et
donc on se dit que si l'expéditeur reçoit 3 \textbf{ACK}'s pour le même segment, il renvoit directement le 
segment concerné. \\ La preuve en image :

\imgRT{CN_051.png}{200}
\newpage
\subsubsection{Contrôle de flux TCP}

Les récepteurs \textbf{TCP} possèdent des buffers et il se peut que l'application soit lente à lire les données,
dès lors l'expéditeur ne pourra pas envoyer trop de données à cet hôte trop vite afin de ne pas le submerger. 
\textbf{TCP} a un service qui permet d'éviter ça, le «Speed-Matching» qui va faire correspondre la vitesse 
d'envoi avec la vitesse d'acquisition de l'application.

\imgR{CN_052.png}{250}

Lors de l'envoi d'\textbf{ACK}, le récepteur envoie également la taille de sa \textbf{Rcv Window}, ainsi 
l'expéditeur limite le nombre de données non-\textbf{ACK}ées à la taille de la \textbf{Rcv Window}.

\subsubsection{Gestion de la connexion TCP}

Comme vu plus haut, avant tout transfert de données, les 2 hôtes désirant communiquer passent par une phase que
l'on appelle «handshaking». Cette phase est constituée de 3 étapes :
\begin{multicols}{2}
\begin{itemize}
\item \underline{Etape 1} : Le client envoie un segment \textbf{TCP SYN} spécifiant le numéro de séquence 
initial. (le segment ne contient aucune donnée) 
\item \underline{Etape 2} : Le serveur reçoit le \textbf{SYN} et répond avec un \textbf{SYNACK} contenant le 
numéro de séquence initial du serveur. (le serveur alloue également un buffer pour la connexion)
\item \underline{Etape 3} : Le client reçoit le \underline{SYNACK} et répond avec un \textbf{ACK} qui peut 
contenir des données.
\item \underline{Problème} : Les «Bad guy» floodent le serveur avec des \textbf{SYN} mais ne renvoient jamais 
l'\textbf{ACK} final, bloquant ainsi le serveur en attente sur cet \textbf{ACK} et empêchant les autres 
personnes de s'y connecter. (Ils peuvent également envoyer le \textbf{SYN} avec une adresse source inexistante)
\end{itemize}
\imgRT{CN_053.png}{200}
\end{multicols}
\newpage
La solution réside dans les \textbf{TCP SYN Cookies}, illustré par l'image ci-dessous : 

\imgRT{CN_054.png}{200}

La fermeture d'une connexion se fait également en plusieurs étapes : 
\begin{multicols}{2}
\begin{itemize}
\item \underline{Etape 1} : le client envoie un segment \textbf{TCP FIN} au serveur. \\
\item \underline{Etape 2} : le serveur répond par un \textbf{ACK}, ferme les connexions et envoie un 
\textbf{FIN}. \\
\item \underline{Etape 3} : le client reçoit l'\textbf{ACK} et répond au \textbf{FIN}. Il entre également dans
une phase d'attente pendant laquelle il répond d'office par un \textbf{ACK} à chaque \textbf{FIN} reçu. \\
\item \underline{Etape 4} : le serveur reçoit l'\textbf{ACK}, la connexion est fermée.
\end{itemize}
\imgRT{CN_055.png}{200}
\end{multicols}

\subsection{Principes de contrôle de congestion}

La congestion, de manière non formelle, c'est lorsque trop de sources envoient trop de données trop vite pour 
que le réseau n'arrive à le gérer. (Attention c'est différent du contrôle du flux) Ce phénomène se manifeste par
des pertes de paquets et des longs délais d'attente.

\subsubsection{Les causes}

\noindent\underline{Scénario 1}
\begin{multicols}{2}
\noindent 2 expéditeurs, 2 récepteurs, un routeur avec \\
des buffers infinis et aucune retransmission. \\
On obtient des grands délais quand le système est congesté. ($\frac{C}{2}$ est le débit maximum atteignable)
\imgR{CN_056.png}{260}
\end{multicols}

\imgR{CN_057.png}{200}

\noindent\underline{Scénario 2}
\begin{multicols}{2}
Un routeur, buffers finis et retransmission d'un paquet perdu.
On a toujours $\lambda_{in}=\lambda_{out}$ (goodput), on a une retransmission parfaite seulement lorsque 
$\lambda_{in}'>\lambda_{out}$. La retransmission des paquets retardés (pas perdus) va rendre $\lambda_{in}'$ 
plus grand que le cas parfait pour un même $\lambda_{out}$. \\
Les conséquences de la congestion sont plus de travail pour un goodput donné ainsi que des retransmissions 
non-nécéssaires : le lien garde des copies multiples du paquet.
\imgR{CN_058.png}{260}
\end{multicols}

\imgR{CN_059.png}{300}

\noindent\underline{Scénario 3}

\begin{multicols}{2}
\imgR{CN_060.png}{240}
\imgR{CN_061.png}{200}
\end{multicols}

4 expéditeurs, chemins à multihop, utilisation de timeout et de retransmission.
Une autre conséquence de la congestion, quand un paquet est droppé, toutes les ressources utilisées pour le 
paquet ont été gaspillées.

Il y a 2 grandes approches pour controler la congestion : 
\begin{itemize}
\point{End-End Congestion Control}{Pas de feedback (retour d'information) explicite du réseau, la congestion
est détectée par les pertes, délais d'un hôte connecté au réseau ; approche utilisée par \textbf{TCP}}.
\point{Network-assisted congestion control}{Les routeurs fournissent des informations aux hôtes connectés au
réseau : soit un simple bit signalant de la congestion ou pas (SNA,DECbit,TCP/IP ECN,ATM) ou explicitement le
taux à laquelle l'expéditeur doit envoyer ses paquets.}
\end{itemize}

\subsubsection{Le contrôle de congestion ATM ABR}

\textbf{ABR} \textit{(Available bit rate)} : c'est un service élastique, si le chemin de l'expéditeur est 
sous-chargé l'expéditeur doit utiliser toute la bande passant disponible sinon si le chemin est congestionné, il 
ralentit jusqu'au taux minimal garanti. \\

Cellules \textbf{RM} \textit{Resource Management} : envoyées par l'expéditeur entrecoupées par des cellules de
données. Les bits dans les cellules \textbf{RM} sont modifiés par les Switch(les routeurs) : le \textbf{NI bit} 
est placé pour signifier à l'expéditeur de ne pas augmenter son débit \textit{(«congestion douce»)} et le
\textbf{CI bit} pour signifier qu'il y a congestion. Ces cellules sont renvoyées intactes à l'expéditeur par le
récepteur.

Dans le cas d'\textbf{ATM ABR}, les cellules \textbf{RM} contiennent un champ contenant 2 bytes 
\textbf{ER}\textit{(Explicit Rate)} qu'un switch congestionné peut modifier pour une plus petite valeur, 
l'expéditeur enverra ses paquets à la vitesse maximale supportée par le réseau. \textbf{Les cellules de données} 
contiennent un bit \textbf{EFCI} qui est mis à 1 par un switch qui est congestionné, si le bit \textbf{EFCI} de 
la cellule de données précédent une cellule \textbf{RM} est mis à 1, l'expéditeur met le bit \textbf{CI} dans 
la cellule \textbf{RM} à 1.

\imgR{CN_062.png}{250}

\subsection{Contrôle de congestion TCP}

\underline{Limitation de la transmission de l'expéditeur} \\
Ca repose sur une fenêtre de congestion \textit{(cwnd)}, différente de la fenêtre de réception \textit{(rwnd)}
utilisée pour le contrôle de flux. La \textbf{cwnd} est dynamique, elle est fonction de la perception de la 
congestion du réseau. On a : $LastByteSend - LastByteAcked \leq \min{(cwnd,rwnd)}$ ainsi que le taux d'émission
de l'expéditeur qui est environ $\boxed{Rate = \dfrac{cwnd}{RTT}}$. L'expéditeur réduira ce taux après chaque 
évènement de perte de paquet \textit{(donc la réception de 3 \textbf{ACK}'s dupliqués ou un timeout)}, c'est
sa manière de détecter qu'il y a de la congestion sur le chemin qu'il utilise.

\subsubsection{Additive Increase, Multiplicative Decrease}

L'idée est d'augmenter le taux d'émission (la taille de la fenêtre) en sondant la bande passante disponible tant 
qu'aucune perte n'est encourue. \textbf{Additive Increase} dit qu'il faut augmenter ce taux par 1 \textbf{MSS} 
tous les \textbf{RTT} jusqu'à ce qu'une perte survienne. A ce moment là, \textbf{Multiplicative Decrease} dit 
qu'il faut diviser le taux d'émission par 2.

\imgR{CN_063.png}{400}
\newpage
\subsubsection{TCP Slow Start}

Quand la connexion commence, \textbf{cwnd} = 1 \textbf{MSS} mais la bande passante disponible peut être beaucoup
plus grande que $\dfrac{\mathbf{MSS}}{\mathbf{RTT}}$, alors qu'il est désirable d'arriver rapidement au maximum. 
On va donc faire croître le taux d'émission de manière exponentielle jusqu'à la première perte.

\imageR{CN_064.png}{200} \imageRT{CN_065.png}{200} \\

On va même plus loin, on va changer l'augmentation exponentielle en augmentation linéaire dès que la valeur du
taux d'émission aura atteint la moitié de la valeur qu'il avait avant le timeout (valeur appellée 
\textit{\textbf{sstresh}}). Graphiquement : 

\imgR{CN_066.png}{200}

Encore mieux, on va déduire la raison de la perte. Si elle est due à la réception de 3 \textbf{ACK} dupliqués 
alors la fenêtre d'émission est coupée en 2 puis elle croît linéairement (recouvrement rapide), par contre, si 
elle est due à un timeout, elle est réinitialisée à 1 \textbf{MSS} puis croit exponentiellement jusqu'à 
\textbf{sstresh} puis linéairement. Le philosophie derrière ça c'est que si on reçoit 3 \textbf{ACK}'s la 
congestion est moins grave vu que l'on reçoit quand même des segments, or si c'est à cause d'un timeout, c'est 
que le réseau n'arrive pas à faire parvenir les messages. \\

\noindent\textbf{\underline{Résumé du contrôle de congestion} : } \\

\noindent Quand \textbf{cwnd} est \textit{en dessous} de \textbf{sstresh}, l'expéditeur est dans la phase 
\textbf{slow-start}, la fenêtre d'émission grandit de manière exponentielle. \\
Quand \textbf{cwnd} est \textit{au dessus} de \textbf{sstresh}, l'expéditeur est dans la phase 
\textbf{congestion-avoidance}, le fenêtre d'émission grandit de manière linéaire. \\
Quand on reçoit 3 \textbf{ACK}'s dupliqués, \textbf{sstresh} est modifié en $\dfrac{\mathbf{cwnd}}{2}$ puis 
\textbf{cwnd} est mis à \textbf{sstresh}. \\
Quand un timeout expire, \textbf{sstresh} est modifié en $\dfrac{\mathbf{cwnd}}{2}$ puis \textbf{cwnd} est mis à 
1 \textbf{MSS}. \\

\begin{center}
	\begin{tabular}{|p{110px}|p{120px}|p{170px}|}
	\hline
	\dblu{Etat} & \dblu{Evenement} & \dblu{Action de l'expéditeur TCP} \\
	\hline
	Slow Start (SS) & \textbf{ACK} d'une donnée non \textbf{ACK}ée reçu & 
	\textbf{cwnd} $\leftarrow$ \textbf{cwnd}$+$\textbf{MSS}$*$(\textbf{MSS}$/$\textbf{cwnd}) et si (\textbf{cwnd} 
	$>$ \textbf{sstresh}) $\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ $
	alors state $\leftarrow$ \textbf{CA}\\
	\hline
	Congestion Avoidance (CA) & \textbf{ACK} d'une donnée non \textbf{ACK}ée reçu & \textbf{cwnd} $\leftarrow$ 
	\textbf{cwnd} $+$ \textbf{MSS}\\
	\hline
	Slow Start ou Congestion Avoidance & Triple \textbf{ACK} dupliqué & \textbf{sstresh} $\leftarrow$ 
	\textbf{cwnd}/2 $\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ $ \textbf{cwnd} $\leftarrow$ \textbf{sstresh} $\ \ 
	\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ $ state $\leftarrow$ \textbf{CA} \\
	\hline
	Slow Start ou Congestion Avoidance & Timeout & \textbf{sstresh} $\leftarrow$ \textbf{cwnd}/2 $\ \ \ \ \ \ \ \ 
	\ \ \ \ \ \ \ \ \ \ $ \textbf{cwnd}$\leftarrow 1$\textbf{MSS} $\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ 
	\ \ \ \ \ \ \ $ state $\leftarrow$ \textbf{SS} 
	\\
	\hline
	Slow Start ou Congestion Avoidance & \textbf{ACK} dupliqué & Incrementer le compteur d'\textbf{ACK} dupliqué 
	pour le segment qui vient d'être \textbf{ACK}é. \\
	\hline
	\end{tabular}
\end{center}

\subsubsection{Débit de TCP}

On ignore le \textbf{Slow Start}, on pose \textbf{W} la taille de la fenêtre quand une perte se produit. Lorsque
la fenêtre est de \textbf{W}, le débit est de $\dfrac{\mathbf{W}}{\mathbf{RTT}}$ et juste après la perte, la 
fenêtre est divisée en 2 et donc le debit est de $\dfrac{\mathbf{W}}{\mathbf{2RTT}}$. On obtient donc un débit 
moyen de : \\
\begin{center}
$\dfrac{ \dfrac{\mathbf{W}}{\mathbf{RTT}} + \dfrac{\mathbf{W}}{\mathbf{2RTT}}} {2} = \boxed{ \dfrac{3W}{4RTT} = 
\dfrac{\bar{W}}{RTT}}$
\end{center}

\subsubsection*{Le futur de TCP}

Par exemple, prenons des segments de $1500$bytes, un \textbf{RTT} de $100$ms et on veut un débit de $10$Gbps.
On veut donc être capable d'envoyer $\frac{10^{10}}{1500*8} = 83.333,333$ paquets par secondes. La fenêtre 
moyenne requise est donc de $\bar{W} = 83.333,33 (=$ débit$*$\textbf{RTT}$)$ (c'est-à-dire le nombre de segments 
envoyés sans qu'il y ait forcément un \textbf{ACK} reçu). Le débit en terme de taux de perte s'exprime ainsi : 
\begin{center}
$Debit \approx \dfrac{1,22.\mathbf{MSS}}{\mathbf{RTT}.\sqrt{L}} \Rightarrow L = 2.10^{-10}$
\end{center}
ce qui signifie qu'il faut des liens avec un taux de perte extrêmement petit ... !\\
De nouvelles versions de \textbf{TCP} pour la haute vitesse sont sous investigation ...

\subsubsection{L'équité de TCP}

\imgR{CN_067.png}{200}

\begin{multicols}{2}
\imgR{CN_068.png}{200}
Le but de l'équité est que s'il y a $K$ sessions partageant le même lien (goulot d'étranglement) de bande 
passante $R$, alors chacun devrait avoir une bande passante de $\frac{R}{K}$. \textbf{TCP} est un protocole
respectant l'équité, en effet, partout où \textbf{Additive Increase} augmente le débit, \textbf{Multiplicative 
Decrease} le réduit proportionnellement.
\end{multicols}

\underline{Remarque} : \textit{Rien n'empêche l'application d'ouvrir des connexions parallèles avec son 
«partenaire», les browsers Internet le font. Imaginons un lien de bande passante $R$ supportant $9$ connexions,
si une nouvelle application demande une connexion \textbf{TCP}, elle obtiendra une bande passante de $\frac{R}
{10}$ mais si elle demande $11$ connexions, elle obtiendra $\frac{11.R}{20} \approx \frac{R}{2}$ !} \\

\hbox{\raisebox{0.4em}{\vrule depth 0.4pt height 0.4pt width 10cm}}

\section{La couche réseau (Network Layer)}

Il s'agit de la couche servant à :
\begin{itemize}
\item transporter un segment d'un hôte à un autre,
\item d'encapsuler les segments dans un datagramme d'un coté, les décapsuler de l'autre et délivrer le segment à 
la couche application,
\end{itemize}
Dans chaque hôte et dans chaque routeur il y a un protocole réseau, chaque routeur examinant les headers de 
tous les datagrammes passant par lui. Chaque protocole réseau a 2 fonctions : \dred{forwarding} et 
\dred{routing}. Le \textbf{forwarding} consiste à prendre le paquet sur le lien d'entrée et le placer sur le bon
lien de sortie pour l'envoyer au destinataire. Le \textbf{routing} consiste à calculer la route la plus courte 
pour relier 2 hôtes.

Avant que le flux de données ne commence, les 2 hôtes et les routeurs utilisés par ces 2 hôtes pour communiquer
établissent une connexion virtuelle. \textit{(Attention, la couche réseau connecte 2 hôtes et les routeurs 
concernés, la couche transport connectait 2 processus ; c'est \textbf{différent})} \\

\textbf{\underline{Services}} \\

Pour un datagramme seul, on pourrait, par exemple, garantir que le datagramme arrivera avec ou non un timing 
prédéfini (par exemple garantir que le datagramme arrive en deça de 5 secondes). 
Pour un flux de données, on pourrait garantir que les datagrammes arrivent dans l'ordre, garantir une bande 
passante minimale au flux, des restrictions sur les changements dans les espaces inter-paquets ou encore 
garantir de la sécurité/confidentialité.

\subsection{Circuit virtuel et réseau de datagrammes}

Un réseau de datagrammes fournit un service de couche réseau sans connexion tandis qu'un circuit virtuel fournit 
un service de couche réseau avec connexion. On voit directement une analogie avec la couche transport modulo le 
fait que le service se fait d'hôte à hôte et qu'il n'y a pas de choix, soit le réseau fournit un service soit 
l'autre et l'implémentation est faite dans le c\oe ur du réseau. 

\subsubsection{Circuit virtuel}

Le chemin allant de la source au destinataire se comporte comme un réseau téléphonique en terme de performance 
et d'actions effectuées le long du chemin. Il y a une configuration du chemin à prendre («call») avant que le 
flux de données ne démarre et chaque paquet envoyé contient un identifiant de circuit et non l'adresse de l'hôte 
de destination. Chaque routeur sur le chemin maintient un «état» pour chaque connexion passant, les liens, 
les ressources des routeurs (bande passante et buffers) doivent être allouées au circuit virtuel $\rightarrow$ 
service prévisible.

Au point de vue implémentation, un circuit virtuel (\textbf{VC}) consiste en un chemin d'une source vers une 
destination, en nombres \textbf{VC} (un nombre pour chaque lien sur le chemin) et en entrées dans les 
\textit{tables de forwarding}\textbf{(FIB)} des routeurs sur le chemin. Les nombres \textbf{VC} de chaque lien 
peuvent être changés, les nouveaux nombres VC venant des \textbf{FIB}.

\imgR{CN_069.png}{200}

Une partie de la \textbf{FIB} du routeur en haut à gauche : 

\begin{center}
	\begin{tabular}{|*{4}{c|}}
	\hline
	Interface entrante & Nombre \textbf{VC} entrant & Interface sortante & Nombre \textbf{VC} sortant \\
	\hline
	1 & 12 & 3 & 22 \\
	\hline
	2 & 63 & 1 & 18 \\
	\hline
	3 & 7 & 2 & 17 \\
	\hline
	1 & 97 & 3 & 87 \\
	\hline
	\end{tabular}
\end{center}

Les protocoles de signalisation sont utilisés pour configurer et maintenir les \textbf{VC} bien qu'ils ne soient
pas utilisés dans l'Internet d'aujourd'hui.

\imgR{CN_070.png}{300}

\subsubsection{Réseau de datagrammes}

Pas de configuration des \textit{calls}, les routeurs ne retiennent aucun état à propos de la connexion et les 
paquets sont \textit{forwardés} grâce à l'adresse hôte de destination, les paquets transitant entre 2 hôtes
peuvent prendre des chemins différents.

\imgR{CN_071.png}{300}

Dans les \textit{tables de forwarding}, chaque adresse est mappée avec une interface de sortie, en fait elle 
mappe un intervalle d'adresse avec une interface. Elle utilise la méthode du \textbf{Longest prefix matching} 
pour ce faire : \\

\imgR{CN_072.png}{300}

\textbf{\underline{Choix}} : pour \dred{Internet}, le type choisi est le \textit{réseau de datagrammes}, tandis 
que dans le cas d'\dred{ATM} (évolué pour la téléphonie) c'est les \textit{circuits virtuels} qui sont 
choisis !

\subsection{Intérieur d'un routeur}

\textbf{\underline{Vue générale}}
\imgR{CN_073.png}{300}

\subsubsection{Fonction des ports d'entrée}

\begin{multicols}{2}
Lors de l'arrivée du datagramme dans la dernière case, le routeur cherche le port de sortie où placer le 
datagramme en utilisant la \textbf{FIB} se trouvant dans la mémoire du port d'entrée. Le but est de trouver ce 
port de sortie à la vitesse de la ligne, sinon les paquets arriveront plus vite que le temps mis pour 
\textit{forwarder} les paquets à l'intérieur de la \textit{«Switching Fabric»}.
\imgR{CN_074.png}{235}
\end{multicols}

Il y a 3 types de \textit{«Switching Fabric»} pour les routeurs : 

\imgR{CN_075.png}{300}

\textbf{\underline{Switching avec mémoire}} \\

Il s'agit de la première génération de routeurs, c'est-à-dire des ordinateurs traditionnels avec le 
\textit{«switching»} sous contrôle du \textbf{CPU}. La vitesse est donc limitée par la bande passante de la 
mémoire du système (2 bus par datagramme), les paquets étant copiés dans cette mémoire. \\
\newpage
\textbf{\underline{Switching avec bus}} \\

Les messages d'entrées passent de la mémoire du port d'entrée à la mémoire du port de sortie par un bus partagé.
La vitesse de \textit{«switching»} est donc limitée par la bande passante du bus. Avec un bus de 32Gbps (CISCO 
5600), on obtient une vitesse suffisante pour les routeurs d'accès et d'entreprise.\\

\textbf{\underline{Switching avec \textit{«crossbar»}}} \\

Cette façon de faire surmonte les problèmes de limitations de bande passante de bus. Utilisés dans les réseaux
«Banyan», il s'agit d'interconnexions destinées à relier les processeurs dans les multiprocesseurs. Il s'agit 
d'un design avancé, il découpe les datagrammes en cellules de taille fixe qui sont envoyées à travers la 
\textit{«switching fabric»}. Le CICSO 12000 utilise cette architecture et possède une vitesse de 
\textit{«switching»} de 60Gbps.

\subsubsection{Les ports de sortie}

Les 2 caractéristiques principales des ports de sortie c'est le buffering et la discipline d'ordonnancement. Le
premier est nécessaire lorsque les datagrammes arrivent plus vite que la vitesse d'émission sur le lien, le 
second est la discipline à adopter pour choisir quel datagramme prendre dans le buffer pour l'envoyer.

Pour le buffer, il est caractérisé par sa taille, auparavant on plaçait sa taille à $RTT.C$ où $C$ est la 
capacité du lien (donc si le lien est de 10Gbps et le RTT de 250ms, le buffer avait une taille de 2,5 Gbits). 
Maintenant, si il y a $N$ flux le buffer doit avoir une taille de \dred{$\frac{RTT.C}{\sqrt{N}}$}.

En ce qui concerne l'ordonnancement, on a 2 méthodes (\textbf{FCFS}\textit{(First Come First Served)} et 
\textbf{WFQ}\textit{(Weighted Fair Queued)}) pour ordonnancer les paquets que l'on prend dans le buffer ainsi 
que 2 méthodes (\textbf{Tail Drop} et \textbf{RED}\textit{(Random Early Detection)}) pour dropper des paquets 
quand c'est nécessaire.

On vient de voir que l'on pouvait avoir un phénomène de queue aux ports de sorties, mais il peut également en
avoir un aux ports d'entrées, si les ports d'entrées sont plus rapides que la \textit{«switching fabric»}. On
assiste alors à un \textbf{HOL}\textit{(Head-of-the-line)}-blocking, c'est-à-dire que le premier paquet de la 
file empêche les autres d'avancer. On a donc des délais à cause de la queue et des pertes possibles.

\subsection{IP (Internet Protocol)}

\subsubsection{Fonctions}

Fixer les conventions d'adressage, le formatage des paquets et les conventions de gestion des paquets.

\subsubsection{Format des datagrammes}

\imgR{CN_076.png}{300}

\textbf{\underline{Fragmentation et Réassemblage des datagrammes IP}} \\

Les liens Internet ont un \textbf{MTU}, et il existe différents types et donc différents \textbf{MTU}. Dès lors
les trop gros datagrammes \textbf{IP} sont fragmentés en plusieurs datagrammes et ils seront réassemblés 
uniquement à la fin du trajet ; tout ceci étant assuré par le header \textbf{IP} qui permet d'identifer et de 
conserver l'ordre des paquets.

\subsubsection{Adressage IPv4}

Une adresse \textbf{IP} est une série de 32 bits permettant d'identifier une interface d'un hôte ou d'un routeur.
(Il a $2^{32} = 4.294.967.296$ de possibilités) Une interface est une connexion entre un hôte/routeur et un lien 
physique. Les routeurs ont typiquement plusieurs interfaces tandis que les hôtes n'en ont généralement qu'une ; 
chaque adresse \textbf{IP} est associée avec chaque interface. \\

\textbf{\underline{Les sous-réseaux}} \\

Une adresse \textbf{IP} est constituée de la partie \textit{«sous-réseau»} qui sont les bits de plus grand ordre 
et la partie \textit{«hôte»}, les bits de plus petit ordre. On définit alors un \dred{sous-réseau} 
\textit{(subnet)} comme un ensemble d'interface de routeur/hôte ayant la même partie \textit{«sous-
réseau»} et qui savent donc se joindre l'un l'autre physiquement sans faire intervenir de routeurs. On identifie
les \textbf{sous-réseaux} via des masques, par exemple pour un \textbf{sous-réseau} regroupant les adresses 
\textbf{IP} \textit{223.1.1.1}, \textit{223.1.1.2}, \textit{223.1.1.3}, \textit{223.1.1.4}; le masque sera 
\red{\textit{223.1.1.0/24}}. (24 correspondant au nombre de bits communs entre toutes les adresses \textbf{IP} 
appartenant au \textbf{sous-réseau})

\imgR{CN_077.png}{150}

\textbf{\underline{Comment un hôte acquiert une adresse IP ?}} \\

Soit elle est codée en dur dans un fichier de configuration, soit via \textbf{DHCP}\textit{Dynamic Host 
Configuration Protocol} qui obtient dynamiquement une adresse à partir d'un serveur. Le but de \textbf{DHCP} est 
d'autoriser l'hôte à obtenir dynamiquement son adresse \textbf{IP} à partir du serveur du réseau lorsqu'il s'y
connecte. L'hôte peut également renouveler son «bail» sur l'adresse en cours d'utilisation. Il permet également
la réutilisation des adresses (elles ne sont conservées que lorsqu'un hôte connecté l'utilise).

\textbf{DHCP} fonctionne de la manière suivante : l'hôte \red{broadcast} \textit{(= envoie à toutes les adresses 
du sous-réseau)} un message \textit{«DHCP Discover»} auquel seul le serveur \textbf{DHCP} répond par un message 
\textit{«DHCP offer»}. L'hôte envoie un message \textit{«DHCP Request»} dans laquelle il demande à obtenir
l'adresse \textbf{IP} que le serveur lui a spécifié dans son précédent message ; le serveur \textbf{DHCP} lui 
répond par un message \textit{«DHCP Ack»} lui signifiant que l'adresse \textbf{IP} lui est bien allouée.
\newpage
\underline{Voici un exemple illustrant le fonctionnement de \textbf{DHCP}} : 

\imageR{CN_078.png}{200} \imageR{CN_079.png}{250}

\noindent\textit{Remarque : pour obtenir la partie sous-reseau d'une adresse \textbf{IP}, le réseau prend la 
portion de l'espace adresse de son fournisseur \textbf{ISP} qui lui est allouée.}

L'allocation hiérarchique permet une «publication» efficace des informations de routage. Le maillon le plus haut
de cette hiérarchie, c'est-à-dire l'organisation qui décide pour tout l'Internet quelle organisation possède 
quel intervalle d'adresse \textbf{IP}, qui maintient les \textbf{DNS}, qui assigne des noms de domaines et qui 
résoud les différends ; c'est l'\textbf{ICANN}\textit{(Internet Corporation for Assigned Names and Numbers)}. \\
Ci-dessous un exemple illustrant le fonctionnement de l'adressage hiérarchique.

\imageR{CN_080.png}{200} \imageR{CN_081.png}{250}

\subsubsection{NAT (Network Address Translation)}

\imageR{CN_082.png}{300}

Dans le réseau local, toute adresse \textbf{IP} est une adresse de $10.0.0/24$ ; mais dès que les datagrammes ont 
passé le \textbf{NAT} (donc vers l'extérieur du réseau local), le \textbf{NAT} modifie l'adresse source en 
$138.76.29.7$. Cet équipement est très utile car il permet d'utiliser une seule adresse \textbf{IP} pour 
plusieurs équipements désireux de se connecter à Internet (on ne doit donc pas demander un interval d'adresse 
\textbf{IP} à notre \textbf{ISP}). Il permet également de modifier les adresses \textbf{IP} du réseau local sans
avoir à notifier le monde extérieur, de même on peut changer d'\textbf{ISP} sans modifier les adresses 
\textbf{IP} du réseau local. Finalement, le point le plus important c'est que de l'extérieur, les équipements
connectés au réseau local derrière un \textbf{NAT} ne sont pas directement adressables par le monde extérieur 
ce qui est un gain de sécurité non négligeable.\\

\textbf{\underline{Implémentation}}\\

Un routeur \textbf{NAT} doit :
\begin{itemize}
\point{Datagrammes sortant}{remplacer l'adresse \textbf{IP} et le port source du datagramme par l'adresse 
\textbf{IP} et un nouveau port,}
\point{Table de traduction}{se souvenir du mapping entre chaque (adresse \textbf{IP} local,port local) et 
(adresse \textbf{IP} du \textbf{NAT}, nouveau port),}
\point{Datagrammes entrant}{faire le travail inverse que pour les datagrammes sortant.}
\end{itemize}

Les ports assignés par le \textbf{NAT} sont codés sur $16$bits, ce qui signifie que l'on peut connecter pas moins
de $2^{16} (= 65536)$ équipements en même temps à travers un \textbf{NAT} ce qui le crédite énormément. 
Cependant, \textbf{NAT} est controversé car les routeurs ne doivent normalement seulement traiter que jusque la 
couche 3 et il viole l'argument de «bout à bout» ce qui peut avoir pour effet que le \textbf{NAT} soit considéré 
comme un compte en lui même par les concepteurs d'applications P2P par exemple. Aussi, la pénurie d'adresse 
\textbf{IP} ne devrait pas être réglée de cette façon mais plutot avec \red{IPv6}. \\

\textbf{\underline{Problème de \textbf{NAT}}} \\

Imaginons qu'un serveur est connecté au réseau derrière un \textbf{NAT} sur l'adresse \textbf{IP} $10.0.0.1$, un 
client externe au réseau local souhaite s'y connecter. Ce dernier ne peut utiliser cette adresse \textbf{IP} pour
le contacter car elle est locale et lui est donc inconnue, la seule manière qu'il a de contacter le serveur c'est
de s'adresser au \textbf{NAT} mais le \textbf{NAT} ne peut savoir à qui il doit adresser le message, le message
est donc droppé. Une \textit{\textbf{première solution}} consiste à configurer de manière statique le 
\textbf{NAT} pour qu'il forward tous les messages reçu sur un port donné au serveur d'adresse \textbf{IP} 
$10.0.0.1$ sur le port désiré (disons $5000$). Une \textit{\textbf{seconde solution}} est d'utiliser le protocole
\textbf{UPnP}\textit{(Universal Plug and Play)} \textbf{IGD}\textit{(Internet Gateway Device)}, celui-ci permet 
aux hôtes connectés localement au \textbf{NAT} de connaître l'adresse publique du \textbf{NAT} ainsi que 
d'ajouter/supprimer des mappings entre les ports du \textbf{NAT} et la machine à joindre. Il existe une 
\textbf{\textit{dernière solution}}, et nous l'avons déjà vue, c'est celle du relai, utilisée par Skype. Cela 
consiste à établir la connexion entre les 2 hôtes désireux de communiquer via un hôte relai.

\subsubsection{ICMP (Internet Control Message Protocol)}

Il s'agit d'un protocole de communication utilisé par les hôtes et les routeurs, il permet de reporter des 
erreurs et des ping/pongs. Concernant les erreurs signalées, on retrouve hôte, port, réseau inaccessible ou 
mauvais protocole. Les messages \textbf{ICMP} sont transportés par des datagrammes \textbf{IP}, ils sont 
structurés comme suit : type code 8bytes. Les 8bytes étant les 8 premiers bytes du paquet qui pose problème.

C'est avec ce type de message que sont effectués les \textbf{traceroutes}, le principe du traceroute est 
d'envoyer une série de segments \textbf{UDP} contenant un \textbf{TTL} croissant, ainsi quand le $n^{eme}$ 
routeur reçoit le message avec un \textbf{TTL} décrémenté à 0, il drop le message et envoie un message d'erreur
à l'expéditeur. Dès lors l'expéditeur connait le routeur qui lui a envoyé ce message et il peut calculer le 
\textbf{RTT} afin d'avoir une idée du temps mis par le paquet pour arriver. \textbf{Traceroute} fait ça 3 fois et 
fait la moyenne des 3 temps ainsi trouvés. Ces envois s'arrêtent lorsqu'éventuellement l'hôte destinataire a été 
trouvé ou que l'on reçoit une erreur spécifiant que l'hôte est inaccessible.

\subsubsection{IPv6}

La motivation de départ était que toutes les \textbf{IPv4} commençaient à être allouées (d'où une pénurie) alors
au lieu de coder les \textbf{IP} sur 32 bits, \textbf{IPv6} propose de les coder sur 128 bits (grossomodo 
$3,4*10^{38}$). Les headers ont également été modifiés afin de faciliter la rapidité de traitement ainsi que la
qualité de service, ils ont une taille fixée de 40 bits. Il faut préciser aussi que les datagrammes \textbf{IPv6}
ne permettent pas la fragmentation.

\imgR{CN_083.png}{200}

\noindent \textbf{Pri}\textit{(ority)} : identifie la priorité des datagrammes dans le flux. \\
\textbf{Flow Label} : identifie les datagrammes appartenant au même flux.\\
\textbf{Next h}\textit{(ea)}\textbf{d}\textit{(e)}\textbf{r} : identifie le protocole de la couche supérieure
utilisé. \\
Il y a d'autres changements par rapport à \textbf{IPv4} comme la checksum qui a été enlevée, les options qui sont
toujours utilisées mais en dehors du header (elles sont pointées par le champ next header) et l'utilisation de
\textbf{ICMPv6} dans lequel de nouveaux messages ont été ajouté (comme «Paquet trop gros») ainsi que des 
fonctions de gestion de groupe multicast. Le seul souci dans tout ça c'est pour les routeurs, en effet, les 
routeurs \textbf{IPv4} ne sauront pas traiter les datagrammes \textbf{IPv6}. La solution est d'utiliser une 
double pile ainsi que le \textbf{tunneling} qui consiste à envelopper un datagramme \textbf{IPv6} dans un 
datagramme \textbf{IPv4} lorsque celui-ci doit passer par un ou des routeur(s) \textbf{IPv4}.

\imgR{CN_084.png}{300}

\subsection{Algorithmes de routage}

Pour symboliser les réseaux, on va faire une abstraction grace aux graphes, pour rappel un graphe \textbf{G} est 
défini par \textbf{F} son ensemble de \textit{flèches} et \textbf{S} son ensemble de \textit{sommets}. Ici, 
\textbf{S} sera l'ensemble des routeurs du réseau et \textbf{F} les liens entre les routeurs. De plus les graphes 
que l'on utilisera auront des coûts sur les flèches et ne seront pas orientés ; le coût sur les flèches étant 
soit toujours \textbf{1} soit inversément proportionnel à la bande passante ou à la congestion. \\

\noindent On peut classer les algorithmes de routage selon 2 critères,
\begin{itemize}
\item \textbf{Information globale ou décentralisée} : soit tous les routeurs ont connaissance de toute la 
topologie et des coûts des liens soit ils ne connaissent que leurs voisins qui leur sont physiquement connectés. 
Dans le premier cas on parle d'\red{algorithmes «link-state»} et dans le second cas les algorithmes sont des 
algorithmes itératifs de calcul et d'échange d'informations avec les voisins ; on parle d'\red{algorithmes 
«vecteur-distance»}.
\newpage
\item \textbf{Statique ou Dynamique} : Les algorithmes statiques sont indiqués dans le cas où les routes changent
lentement dans le temps, les dynamiques pour le cas opposé. Dans ces derniers, il y a donc une mise à jour 
régulière et une réaction en réponse au changement de coût d'un lien.
\end{itemize}

\subsubsection{Un algorithme «link-state» : Algorithme de Dijkstra}

Comme dit ci-dessus, chaque routeur connait les coûts des liens, ceci étant accompli par un broadcast d'état du 
lien $\rightarrow$ tous les n\oe uds ont les mêmes informations. Chaque n\oe ud calcule ses chemins de coût 
minimal pour joindre tous les n\oe uds ce qui lui donne sa table de forwarding. L'algorithme est itératif, après 
$k$ itérations, le n\oe ud connait le chemin de coût minimal pour joindre $k$ n\oe uds. Nous allons utiliser les 
notations suivantes : 
\begin{itemize}
\bfp{c(x,y)}{le coût pour aller du n\oe ud $x$ au n\oe ud $y$, il est $\geq 0$ par définition et vaut $\infty$ si 
$x$ et $y$ ne sont pas voisins,}
\bfp{D(v)}{le coût courrant (donc celui qu'il pense minimal) pour aller du n\oe ud considéré au n\oe ud $v$,}
\bfp{p(v)}{le n\oe ud précédant $v$ sur le chemin menant du n\oe ud considéré à $v$,}
\bfp{M}{l'ensemble de n\oe uds dont le chemin de coût minimal est définitivement connu.} \\
\end{itemize}

\noindent\textbf{\underline{Algorithme}} ($u$ est le n\oe ud source, c'est-à-dire le n\oe ud considéré)
\begin{lstlisting}
Initialisation :
  M = {u}
  Pour chaque noeud v faire
     si v est adjacent a u
        alors D(v) = c(u,v)
     sinon D(v) = infini
  
  Tant que tous les noeuds ne sont pas dans M faire
     trouver w ne se trouvant pas dans M tel que D(w) est minimal
     ajouter w a M
     mettre a jour D(v) pour tout v adjacent a w et qui ne se trouve pas dans M :
       D(v) = min (D(v), D(w)+c(w,v))
\end{lstlisting}

Sa complexité telle quelle, pour $n$ n\oe uds, est en $O(n^2)$ (à chaque itération on va voir tous les n\oe uds 
n'étant pas dans $M$ et il y a $\frac{n(n+1)}{2}$ comparaisons) cependant il existe des meilleures 
implémentations conduisant à une complexité en $O(n\log n)$. \\

\underline{Exemple} : \\
\imageR{CN_085.png}{300} \imageR{CN_086.png}{150} \\

La table de forwarding de $u$ est donc : 

\begin{center}
	\begin{tabular}{|*{2}{c|}}
	\hline
	\dred{Destination} & \dred{Lien} \\
	\hline
	v & (u,v)\\
	\hline
	x & (u,x)\\
	\hline
	y & (u,x)\\
	\hline
	w & (u,x)\\
	\hline
	z & (u,x)\\
	\hline
	\end{tabular}
\end{center}
\newpage
\textbf{\underline{Remarque}} : il peut y avoir des oscillations dans ce système, voici un exemple avec le coût 
des liens calculés sur base du trafic passant par ces liens : 

\imgR{CN_087.png}{300}

\subsubsection{Un algorithme «distance-vecteur» : Bellman-Ford}

Tout l'algorithme est basé sur une équation, l'équation de \textbf{Bellman-Ford} : 

\begin{center}$\boxed{d_x(y) = \min_v\left(c(x,v)+d_v(y)\right)}$\end{center}

\textit{(où $d_x(y)$ est le coût minimal pour aller de $x$ à $y$ et le minimum est pris sur chaque $v$ voisin de 
$x$)}

\textbf{\underline{Remarques}} : 
\begin{itemize}
\item Il s'agit de l'algorithme utilisé pour les routeurs virtuels que nous avons du implémenter dans le projet 
2,
\item $D_x(y)$ est une estimation du coût minimal pour aller de $x$ à $y$,
\item le n\oe ud $x$ connaît les coûts pour joindre tous ses voisins,
\item le n\oe ud $x$ maintient un vecteur distance : $D_x = [D_x(y):y\in N]$,
\item le n\oe ud $x$ maintient également les vecteurs-distances de chacun de ses voisins $D_v = [D_v(y):y\in N]$ 
pour chaque voisin $v$. \\
\end{itemize}

\noindent\textbf{\underline{Principe de l'algorithme}} \\

De temps en temps, chaque n\oe ud envoie ses propres vecteurs-distances(V-D) à ses voisins (de manière
asynchrone). 
Ensuite, quand un n\oe ud reçoit des nouveaux V-D venant d'un voisin, il met à jour ses propres V-D avec 
l'équation de \textbf{Bellman-Ford} : $D_x(y) \leftarrow \min_v\left(c(x,v)+D_v(y)\right)$ pour chaque n\oe ud $y 
\in N$. Dans des conditions légères et naturelles, l'estimation $D_x(y)$ converge vers le coût minimal courrant 
$d_x(y)$.

\begin{multicols}{2}
\noindent Chaque itération de l'algorithme est causée par un changement du coût d'un lien local ou un message
venant des voisins. Lorsqu'un de ses V-D a été modifié, le n\oe ud prévient ses voisins qui préviennent leurs 
voisins respectifs si nécéssaires, et ainsi de suite.
\imgRT{CN_088.png}{125}
\end{multicols}
\newpage
\noindent\textbf{\underline{Changement du coût d'un lien}}\\

\begin{enumerate}
\item \textbf{Le coût diminue}
\imgR{CN_089.png}{100}
Au temps $t_0$, \textbf{y} detecte que le coût du lien a changé, il modifie ses V-D et informe ses voisins.\\
Au temps $t_1$, \textbf{z} reçoit les V-D de $y$ et modifie sa table. Il calcule un nouveau coût pour $x$ et 
informe donc ses voisins. \\
Au temps $t_2$, \textbf{y} reçoit les V-D de $z$ et modifie sa table. Rien ne change, il n'envoie donc rien.
\item \textbf{Le coût augmente}
\imgR{CN_090.png}{100}
Les bonnes nouvelles voyagent vite mais pas les mauvaises nouvelles, on compte $44$ itérations avant que 
l'algorithme ne se stabilise. La solution à ça est d'utiliser le \dred{Poisoned Reverse}, cette technique consiste
à faire «mentir» les routeurs. En effet, si le routeur $Z$ passe par $Y$ pour aller vers $X$ alors il dira à $Y$
que sa distance pour joindre $X$ est de $\infty$ (même s'il existe un lien direct de $Z$ à $X$) et donc $Y$ ne 
passera par $X$ pour aller jusque $Z$).
\end{enumerate}

\subsubsection{Comparaison des 2 algorithmes}

On va les comparer sur 3 points : 
\begin{enumerate}
\bfp{Complexité des messages}
	{\begin{itemize}
	\point{Link-State}{Avec $n$ n\oe uds, $E$ liens, \red{$O(nE)$} messages envoyés.}
	\point{Vecteur-Distance}{Echange de messages entre voisins seulement (temps de convergence variants).}
	\end{itemize}}
\bfp{Vitesse de convergence}
	{\begin{itemize}
	\point{Link-State}{Algorithme en $O(n^2)$ qui recquiert $O(nE)$ messages pouvant subir des oscillations.}
	\point{Vecteur-Distance}{Le temps de convergence varie, peut donner lieu à des boucles de routage ainsi qu'au 
	problème «count-to-infinity»}
	\end{itemize}}
\bfp{Robustesse (qu'arrive-t-il si le routeur fonctionne mal ?)}
	{\begin{itemize}
	\point{Link-State}{Le n\oe ud peut avertir les autres du coût d'un \textbf{lien} érroné, chaque n\oe ud
	calcule uniquement sa propre table.}
	\point{Vecteur-Distance}{Le n\oe ud peut avertir les autres du coût d'un \textbf{chemin} érroné, la table de
	chaque n\oe ud est utilisée par les autres $\rightarrow$ une erreur se propage dans le réseau.}
	\end{itemize}}
\end{enumerate}

\subsubsection{Routage hiérarchique}

On étudie des routeurs idéalisés, on considère qu'ils sont tous identiques et que le réseau est plat ... or ce 
n'est pas le cas en pratique ! Dans le réseau réel il y a plus de $200.000.000$ destinations, on ne peut donc
pas stocker toutes ces destinations dans les tables de routage sinon l'échange des tables de routage va innonder
les liens. De plus comme Internet est un réseau de réseaux, chaque administrateur de réseau va vouloir gérer
personnellement son propre réseau. La solution est de rassembler les routeurs en région \textit{«Autonomous 
System»}\textbf{AS}, les routeurs appartenant aux mêmes \textbf{AS} font tourner le même protocole de routage.
Deux routeurs d'\textbf{AS} différents peuvent faire tourner des protocoles différents. (On appelle routeur 
passerelle un routeur qui a un lien direct vers un routeur d'un autre \textbf{AS})
Les tables de forwarding des routeurs passerelles sont configurées par 2 algorithmes de routage, le protocole
de routage interne ainsi que le protocole de routage inter-AS, le premier configure les entrées pour les 
destinations internes et les 2 protocoles agissent de concert pour configurer les entrées pour les destinations 
externes.\\
\imgR{CN_091.png}{300}
Imaginons à présent qu'un routeur dans un \textbf{AS}, disons AS1, reçoive un paquet pour un autre \textbf{AS},
disons AS2, il doit acheminer ce paquet vers un routeur de passerelle entre AS1 et AS2 (mais lequel ?). AS1
doit apprendre quelles destinations sont atteignables par AS2 et AS3 et propager cette information à tous les 
routeurs d'AS1. Supposons que AS1 apprenne que le \textbf{sous-réseau} \red{X} est atteignable via AS3 (grâce au 
protocole inter-AS) (le routeur passerelle est donc 1c) mais pas via AS2. Le protocole inter-AS propage 
l'information d'accès à tous les routeurs internes et le routeur 1d détermine par les infos de routage intra-AS 
que son interface \dred{I} donne sur le lien de plus faible coût vers 1c et il ajoute donc dans sa table une 
entrée (x,\dred{I}).
Supposons maintenant que AS1 apprenne que \red{X} est atteignable via AS2 et AS3, lorsque 1d reçoit cette
information, il doit savoir par quelle passerelle il doit envoyer le paquet ; c'est également le job du protocole
inter-AS. (\textit{«hot potato routing»} routage dit «de la patate chaude» on passe le paquet à la passerelle la 
plus proche comme on passerait une patate chaude) \\

Les protocoles intra-AS sont également appellés \textbf{IGP}\textit{(Interior Gateway Protocol)}, les plus connus 
sont \textbf{RIP}\textit{(Routing Information Protocol)}, \textbf{OSPF}\textit{(Open Shortest Path First)} et 
\textbf{IGRP}\textit{(Interior Gateway Routing Protocol)} (propriétaire CISCO).

\subsection{Routage dans Internet}

\subsubsection{RIP (Routing Information Protocol)}

Ce protocole est basé sur un algorithme «vecteur-distance», il a été inclus à la distribution BSD-Unix en 1982. Il
utilise le nombre de hops comme distance métrique avec un maximum de 15. Les V-D sont échangés parmi les voisins 
toutes les 30 secondes via des «Response Message» (appellée également «advertisement»), chacun d'eux contient
une liste contenant jusque 25 sous-réseaux destination au sein de l'\textbf{AS}. Ci-dessous un exemple :

\imgR{CN_092.png}{300} 

\imgR{CN_093.png}{300}

\textbf{\underline{Défaillance de lien et recouvrement}} \\

Si aucun «advertisement» n'est reçu après 180 secondes, le lien/le voisin est considéré comme mort, toutes les
routes passant par le voisin sont invalidées et de nouveaux «advertisements» sont envoyés aux voisins. Les 
voisins à leur tour envoient de nouveaux «advertisements» (si la table a changé), ainsi la défaillance du lien se
propage vite à tout le net. (Poisoned Reverse est utilisé pour empêcher d'avoir des boucles de ping/pong mais la
distance infinie est de 16 hops) Les tables de routage \textbf{RIP} sont gérées par la couche application (un 
processus appelé route-d (daemon)) et les «advertisements» sont envoyés dans des paquets \textbf{UDP}, répétés
périodiquement.

\subsubsection{OSPF (Open Shortest Path First)}

\textit{(Le \red{open} signifie que c'est disponible publiquement)} Il utilise un algorithme de type 
\textbf{link-state}. Les \textit{«advertisements»} d'\textbf{OSPF} (directement dans des datagrammes
IP) contiennent une entrée par routeur voisin, ces messages sont disséminés dans l'\textbf{AS} entier (via le 
\textit{flooding}).\\

\noindent\textbf{\underline{Services avancés d'OSPF (non présents dans RIP)}}\\
\begin{itemize}
\bfp{Sécurité}{tous les messages sont authentifiés pour éviter une intrusion.}
\bfp{Plusieurs chemins de même coût autorisés}{un seul en RIP.}
\bfp{Pour chaque lien, plusieurs coûts différents pour différents type de service (\textbf{TOS)}}{par exemple 
pour un lien satellite le coût du lien est bas pour le service «best effort» et haut pour le temps réel.}
\bfp{Support uni et multicast intégré}{multicast \textbf{OSPF}(\textbf{MOSPF}) utilise la même topologie de base
de données qu'\textbf{OSPF}.}
\bfp{Hierarchie}{on utilise l'\textbf{OSPF} hiérarchique dans les grands domaines.}\\
\end{itemize}
\newpage
\noindent\textbf{\underline{OSPF hiérarchique}} \\

\imgR{CN_094.png}{250}

Il s'agit d'une hiérarchie à 2 niveaux : aires locales et épine dorsale \textit{(«backbone»)}. On a des 
«advertisements» que dans les aires locales et chaque n\oe ud d'une aire connaît la topologie totale de l'aire 
mais ne connaisse que le chemin le plus court pour joindre les réseaux dans les autres aires. Dans cette 
hiérarchie, on distingue 3 types de routeurs : 
\begin{itemize}
\bfp{routeur de \textit{«zone frontalière»} (area border)}{résume les distances vers sa propre aire et les 
communique aux autres routeurs de \textit{«zone frontalière»}.}
\bfp{routeur d'\textit{épine dorsale} (backbone)}{fait tourner un routage \textbf{OSPF} limité à l'épine 
dorsale.}
\bfp{routeur \textit{limite} (boundary)}{assure la connexion avec les autres \textbf{AS}'s.}
\end{itemize}

\subsubsection{BGP (Border Gateway Protocol)}

\noindent Il s'agit du standard \textit{de facto} en terme de protocol de routage inter-AS. \textbf{BGP} fournit 
à chaque \textbf{AS} un moyen :
\begin{itemize}
\item d'obtenir les informations d'accessibilité des sous-réseaux des \textbf{AS}'s voisins,
\item de propager l'information d'accessibilité à tous les routeurs internes à l'\textbf{AS},
\item de déterminer des «bonnes» routes vers les sous-réseaux basées sur les informations d'accessibilité et la 
politique.
\end{itemize}
Il permet également aux sous-réseaux d'annoncer leur existence au reste de l'Internet. \\

\textbf{\underline{Les bases de BGP}} \\

\imgR{CN_095.png}{250}

Une paire de routeurs échangent des informations de routage via des connexions \textbf{TCP} semi-permanentes : 
les sessions \textbf{BGP} (elles n'ont pas besoin de correspondre aux liens physiques). Quand AS2 envoie un 
préfixe à AS1, AS2 promet à AS1 qu'il va forwarder les datagrammes vers ce préfixe (AS2 peut regrouper plusieurs
préfixes dans un seul message). En utilisant la session \textbf{eBGP} \textit{(external \textbf{BGP})} entre 3a 
et 1c, AS3 envoie un préfixe d'information d'accessibilité à AS1. 1c peut ensuite utiliser \textbf{iBGP} 
\textit{(internal \textbf{BGP})} pour propager le nouveau préfixe à tous les routeurs d'AS1. 1b peut ensuite 
réenvoyer la nouvelle information d'accessibilité à AS2 sur la session \textbf{BGP} entre 1b et 2a (Quand un 
routeur reçoit un nouveau préfixe, il crée une entrée pour le préfixe dans sa table de forwarding). \\
\newpage
\textbf{\underline{Attributs des chemins et routes BGP}} \\

Les préfixes envoyés incluent des attributs \textbf{BGP}, le préfixe et les attributs forment une
\textit{«route»}. Il y a 2 attributs importants : \red{AS-PATH} et \red{NEXT-HOP}. Le premier contient les 
\textbf{AS}'s par lesquels le préfixe est passé et le second spécifie le routeur interne à l'\textbf{AS} à 
utiliser pour joindre l'\textbf{AS} suivant (il peut y avoir plusieurs liens du \textbf{AS} courrant au prochain 
\textbf{AS}). Quand un routeur passerelle reçoit un «advertisement» de route, il utilise une politique 
d'importation afin d'accepter ou décliner. \\
Dans le cas où un routeur recevrait des informations à propos de 2 routes possibles, il doit être capable de 
choisir la meilleure des 2. Pour ce faire il possède plusieurs critères : 
\begin{itemize}
\item valeur de l'attribut de préférence locale : décision politique,
\item le plus court \red{AS-PATH},
\item le routeur \red{NEXT-HOP} le plus proche : routage de la patate chaude,
\item des critères additionnels ... \\
\end{itemize}

\textbf{\underline{Messages BGP}} \\

Les messages \textbf{BGP} sont échangés en utilisant des connexions \textbf{TCP}, il y en a des différents : 
\begin{itemize}
\point{OPEN}{ouvre une connexion \textbf{TCP} avec le pair et authentifie l'expéditeur,}
\point{UPDATE}{avertit de nouveaux chemins (ou enlève des vieux)}
\point{KEEPALIVE}{garde la connexion ouverte en l'abscence d'UPDATE, c'est également la réponse \textbf{ACK} à 
une requête OPEN}
\point{NOTIFICATION}{rapporte une erreur dans le précédent message, utilisé également pour fermer la connexion.}
\end{itemize}

\textbf{\underline{Politique de routage BGP}} \\

\imgR{CN_096.png}{300}

$A$,$B$ et $C$ sont des réseaux fournisseurs et $W$,$X$ et $Y$ sont des clients de ces réseaux. On dit que $X$ 
est \red{\textit{dual-homed}} \textit{(«qu'il a 2 maisons»)} car il est attaché à 2 réseaux. Dans un cas de ce 
genre, $X$ ne veut pas servir à acheminer des messages de $B$ à $C$ alors il ne va pas notifier à $B$ qu'il 
possède une route vers $C$. Imaginons que $A$ avertisse $B$ d'un chemin $AW$, $B$ va alors avertir $X$ d'un 
chemin $BAW$ mais en aucun cas il ne va avertir $C$. En effet, $B$ ne reçoit aucune recette/revenu pour router 
les paquets venant de $C$ vers $W$ (vu que ni $C$ ni $W$ ne sont des clients de $B$), il va forcer $C$ à passer 
par $A$ pour joindre $W$ car il ne veut router que les paquets venant ou à destination de ses clients. \\

\textbf{\underline{Pourquoi le routage inter-AS et le routage intra-AS est-il différent ?}} \\

3 raisons principales : 
\begin{itemize}
\bfp{Politique}{dans le cadre du routage \textbf{inter-AS} l'administrateur d'un \textbf{AS} désire controler 
comment son trafic est routé et qui route via son réseau alors que dans le cadre du routage \textbf{intra-AS} il
y a un seul administrateur et donc aucune décision politique n'est nécéssaire,}
\bfp{Echelle}{le routage hiérarchique épargne en taille pour les tables et réduit le trafic dû aux mises à jour,}
\bfp{Performance}{dans le cadre du routage \textbf{intra-AS} on peut se concentrer sur les performances, tandis
que dans le routage \textbf{inter-AS} la politique domine la performance.}
\end{itemize}

\subsection{Routage broad/multicasté}

\subsubsection{Le routage broadcast}

Le \red{routage broadcast} consiste à délivrer des paquets d'une source à tous les autres n\oe uds. On voit sur
le schéma ci-dessous que la dupplication des sources est inéfficace, en effet, comment la source détermine-t-elle
les adresses des récepteurs ?

\imgR{CN_097.png}{300}

Il y a plusieurs solutions,
\begin{itemize}
\bfp{Flooding}{quand un n\oe ud reçoit un message broadcast, il envoie une copie à chacun de ses voisins. Cela 
pose des problèmes de cycles et de «broadcast storm», en effet le récepteur peut recevoir plusieurs fois le même 
paquet et le broadcaster plusieurs fois,}
\bfp{Controlled Flooding}{le n\oe ud broadcast le paquet que s'il ne l'a pas déjà fait auparavant. Il faut donc 
que le n\oe ud tienne une trace des paquets déjà broadcasté ou alors on utilise la politique du 
\textbf{RPF}\textit{(Reverse Path Forwarding)} qui sera développée plus bas.}
\bfp{Spanning Tree}{Aucun n\oe ud ne reçoit de paquets dupliqués} \\
\end{itemize}

\noindent\textbf{\underline{Spanning Tree}} \\

\noindent Il faut d'abord construire un \textbf{ST} \textit{(«Spanning Tree»)} (arbre permettant de joindre tous 
les n\oe uds d'un graphe), les n\oe uds ne forwarderont les paquets que sur cet arbre. Pour le créer, on choisit 
un n\oe ud central, ensuite tous les autres n\oe uds envoient de manière unicast un message vers ce n\oe ud 
central. Ce message est forwardé tant qu'il n'arrive pas à un n\oe ud appartenant déjà à l'arbre. Une fois que 
tous les messages ont été envoyés et réceptionnés, le \textbf{ST} est créé. Voici une illustration de la 
constuction d'un \textbf{ST}.

\imgR{CN_098.png}{300}

\subsubsection{Le routage multicast}

Le multicast est identique au broadcast à la différence près que tous les routeurs qui sont concernés par les 
messages multicast doivent appartenir à un groupe, le groupe multicast. On va donc devoir trouver un (ou des) 
arbre(s) pour connecter les routeurs utilisés par des membres du groupe multicast. On peut construire 2 
arbres différents, un basé sur la source \textit{«source-based»} (et qui sera donc différent selon la source) ou 
un arbre partagé \textit{(«shared-tree»)} qui lui sera unique (voir l'image ci-dessous pour voir la différence 
entre les 2).

\imgR{CN_099.png}{250}

\noindent\textbf{\underline{Approches}} \\

\noindent\begin{itemize}
\point{arbre basé sur la source}{un arbre par source, on va utiliser les notions d'arbre des plus courts chemins 
et le \textit{Reverse Path Forwarding},}
\point{arbre partagé}{un arbre pour tout le groupe, on va utiliser les notions de \textbf{ST} minimal 
\textit{(Steiner)} et des arbres \textit{«center-based»}.}
\end{itemize}

La notion d'arbre des plus courts chemins est simple, on construit un arbre reprenant les chemins les plus 
courts permettant de joindre la source aux routeurs où un membre du groupe multicast est connecté (via 
l'algorithme de \textbf{Dijkstra}). \\
Le \textit{Reverse Path Forwarding} se base sur le connaissance du routeur de son chemin le plus court vers 
l'émetteur du paquet multicast, chaque routeur a alors un principe de forwarding simple : 
\begin{lstlisting}
Si (un datagramme multicast est recu par le lien
    correspondant au chemin le plus cours vers la source)
   alors flooder le datagramme sur toutes les autres interfaces
Sinon ignorer le datagramme
\end{lstlisting}

\imgR{CN_100.png}{100}

\noindent\textit{(Les flèches rouges représentent les paquets qui sont forwardés, les grises ceux qui ne le sont 
pas)} \\

\noindent Comme on le voit sur cet exemple, il y a des routeurs qui ne font pas partie du groupe multicast qui 
reçoivent le message, on améliore alors le procédé en ajoutant des messages spéciaux \textit{«prune»}. Ces 
messages sont envoyés vers le «haut du stream» par les routeurs dont tous les voisins «plus bas dans le stream» 
ne font pas partie du groupe multicast, signifiant ainsi qu'il n'est pas utile de lui envoyer le paquet à 
multicaster. Voici le même exemple mais avec \textit{pruning} cette fois : 

\imgR{CN_101.png}{100}

\noindent L'arbre de \textbf{Steiner} est l'arbre de coût minimum reliant tous les routeurs avec des membres du 
groupes multicast qui y sont connectés. C'est un problème NP-complet mais il existe des excellents heuristiques 
pourtant il n'est pas utilisé en pratique à cause de la complexité de son calcul, de la nécéssité de connaitre 
des informations sur tout le réseau et parce qu'il est monolithique, c'est-à-dire qu'il faut le recalculer dès 
qu'un routeur se déconnecte ou joint le réseau. \\

\noindent\textbf{\underline{Arbre Center-based}} \\

\noindent C'est un arbre de distribution unique où un routeur est désigné comme \red{centre} de l'arbre. Pour 
joindre,
\begin{itemize}
\item un routeur de bord envoie un message unicast de raccord destiné au \red{centre},
\item le message de raccord est «traité» par les routeurs intermédiaires et forwardé jusqu'au \red{centre},
\item soit le message touche une branche connecté au \red{centre} soit il arrive au \red{centre} et le chemin 
qu'il a pris devient une nouvelle branche connectée au \red{centre}.
\end{itemize}

\underline{Exemple} : imaginons que R6 est le centre, les numéros indiquent l'ordre des opérations.

\imgR{CN_102.png}{100}

\noindent\textbf{\underline{DVMRP} (Distance Vector Multicast Routing Protocol)} \\

Il utilise le \textit{flood and prune} (et donc \textbf{RPF}) avec des arbres basés sur la source. L'arbre 
\textbf{RPF} est basé sur les tables de routages de \textbf{DVMRP} construite en communiquant avec les routeurs 
\textbf{DVMRP}. Il n'y a aucune hypothèse à propos de l'unicast sous-jacent, le datagramme initial est floodé via
\textbf{RPF} et les routeurs ne désirant pas recevoir le paquet envoie des \textit{prune}'s. \\
Caractéristiques de \textbf{DVMRP} : 
\begin{itemize}
\item \red{«soft state»}, les routeurs \textbf{DVMRP} oublient périodiquement (1min) les branches qui sont 
"prunées" et donc le trafic multicast arrive dans les branches ne le désirant pas, soit elles re-prunent vers le
haut du stream soit elle continue à recevoir le trafic,
\item les routeurs peuvent rapidement se regreffer à l'arbre en suivant le raccord à la feuille de \textbf{IGMP},
\item bric à brac, c'est communément implémenté dans les routeurs commerciaux, le routage «Mbone» est fait en 
utilisant \textbf{DVMRP}.
\end{itemize}

\imgR{CN_103.png}{200}

Pour connecter directement les membres du groupe multicast, on va utiliser la technique du \textbf{tunneling} 
(semblable à celle utilisée pour \textbf{IPv4} et \textbf{IPv6}). Ca consiste à encapsuler un message multicast
dans un message normal qui n'est plus adressé de manière multicast mais au routeur du groupe que l'on doit 
joindre, ce dernier décapsule alors le message. \\

\noindent\textbf{\underline{PIM} (Protocol Independent Multicast)} \\

\noindent Ce protocole ne dépend pas du protocole unicast sous-jacent, il fonctionne avec tout. Il considère 2 
scénarios de distribution multicast : 
\begin{itemize}
\point{Dense}{Les membres d'un groupe sont proches les uns des autres, la bande passante est plus abondante.}
\point{Sparse}{Les membres d'un groupe sont assez dispersés, la bande passante est moins abondante.}\\
\end{itemize}

\underline{Conséquences} : 
\begin{itemize}
\bfp{Dense}{les routeurs sont considérés comme membres du groupe tant qu'ils n'envoient pas de \textit{prune},
le système utilisé est une construction orientée donnée basée sur un arbre mcast (exemple général : 
\textbf{RPF}) et la bande passante et le traitement des routeurs non participant au groupe multicast sont 
«débauchés».}
\bfp{Sparse}{les routeurs ne sont pas considérés comme membre du groupe tant qu'ils ne le joignent pas 
explicitement, le système utilisé est une construction orientée donnée basée sur un arbre mcast (exemple général 
: \textbf{center-based}) et la bande passante et le traitement des routeurs non participant au groupe multicast 
sont «conservateurs».} \\
\end{itemize} 

\textbf{\underline{Mode Dense} : } \\

C'est du \textit{\red{flood-and-prune}} \textbf{RPF} similaire à \textbf{DVMRP} mais :
\begin{itemize}
\item le protocole de routage unicast sous-jacent fournit des informations à \textbf{RPF} pour les datagrammes 
entrants,
\item le stream descendant est moins compliqué (et moins efficace) que celui de \textbf{DVMRP}, cela réduisant la 
dépendance à l'algorithme de routage unicast sous-jacent,
\item \textbf{PIM} possède un mécanisme de protocole pour les routeurs lui permettant de détecter si c'est un 
\textit{«routeur-feuille»}. \\
\end{itemize}
\newpage
\textbf{\underline{Mode Sparse} : } \\

\imgR{CN_104.png}{100}

Les routeurs doivent envoyer un message de raccord à un point de rendez vous (\textbf{RP}), les routeurs 
intermédiaires mettent à jour l'état et forward le message. Après avoir joint le groupe avec \textbf{RP}, le 
routeur peut basculer sur l'arbre «source-specific» permettant ainsi d'augmenter les performances dû au fait 
qu'il y a moins de concentration et que les chemins sont plus courts. Les expéditeurs de données envoient leurs
données de manières unicast au point de rendez-vous qui s'occupe de tout distribuer en descendant son arbre 
(\textbf{RP} peut envoyer des messages \textit{STOP} s'il n'y a aucun récepteur). \\

\hbox{\raisebox{0.4em}{\vrule depth 0.4pt height 0.4pt width 10cm}}

\section{La couche lien (Link Layer)}

\subsection{Introduction et services}

\noindent\textbf{\underline{Topologie}} \\

Les hôtes et les routeurs sont des \dred{\neus}, tous les canaux qui connectent 2 \neuSPs adjacents le long 
d'un chemin de communication sont des \dred{liens}, les paquets sont ici appelés des \dred{frames} qui encapsule
les datagrammes. La couche lien est responsable du transfert de datagrammes d'un \neuSP à un \neuSP adjacent au 
delà du \dred{lien}. \\

\noindent\textbf{\underline{Contexte}} \\

Le datagramme est transféré par différent protocole de lien au delà de différents liens (par exemple, 
\textbf{Ethernet} sur le premier lien, \textbf{frame relay} sur les liens intermédiaire et \textbf{802.11} sur le
dernier lien) et évidemment chaque protocole fournit des services différents.

\subsubsection{Services}

\begin{itemize}
\item \textbf{Construction, accès au lien} : on encapsule le datagramme dans une \textbf{frame} en y ajoutant un 
\textit{header} et un \textit{trailer} et demande l'accès au canal si il y a un moyen de partage. Pour 
identifier les sources et les destinations des frames, on utilise les \red{adresses \textbf{MAC}} (qui sont 
différentes des adresse \textbf{IP}).
\item \textbf{Livraison de confiance entre des \neuSPs} (voir chapitre 3) : rarement utilisé sur les liens à un 
taux d'erreur de bit faible (fibre optique, pair torsadée), par contre les liens sans fil ont un taux d'erreur 
élevé.
\item \textbf{Contrôle de flux} : espacement entre les envois et les réceptions d'un \neu ,
\item \textbf{Detection d'erreur} : les erreurs sont causées par du bruit ou une atténuation du signal, le 
récepteur détecte une erreur et soit demande le renvoi de le frame, soit il la drope,
\item \textbf{Correction d'erreur} : le récepteur détecte et corrige les erreurs sans demander de retransmission,
\item \textbf{Half-duplex et full-duplex} : avec l'half-duplex, les \neuSPs aux bouts d'un lien peuvent 
transmettre mais pas en même temps.
\end{itemize}

La couche lien est implémentée dans chaque routeur et chaque hôte, plus précisément dans un adaptateur 
(\textbf{NIC}\textit{(Network Interface Card)}) comme les cartes Ethernet, PCMCI ou 802.11 qui est placé dans le 
bus système des hôtes. Cet adaptateur est un mélange d'hardware, software et firmware.

\subsection{Détection et correction d'erreur}

Chaque frame possède les 2 champs suivants :
\begin{itemize}
\bfp{EDC}{Error Detection and Correction bits (redondance)}
\bfp{D}{Données protégées par une vérification d'erreur, peut inclure des champs du header.}
\end{itemize}
La détection d'erreur n'est pas fiable à $100\%$, il se peut que le protocole ne détecte pas une erreur, mais 
c'est rare. Un champ \textbf{EDC} plus grand accroit la probabilité de la détection et la correction.

La détection et la correction d'erreur sont 2 choses différentes, elles permettent toutes 2 de détecter les 
erreurs avec une bonne probabilité mais pour la première la seule solution est de redemander l'envoi de la frame 
à l'expéditeur alors que dans la seconde le récepteur est capable de résolver les erreurs sans aucune 
restransmission.

\subsubsection{Single-bit Parity Checking}

Le principe de la vérification de la parité est d'ajouter un bit au $N$ bits de données représentant la parité du
nombre de $1$ dans les données. Si le bit est à $1$ c'est que le nombre est impair, si il est à $0$ c'est qu'il 
est pair ; une manière simple de le calculer est : $\left(\sumin{1}{n-1}{b_i}\right) \mod 2$ le résultat est la
parité du nombre de bits à $1$ dans les données. Cette façon de faire permet de détecter toutes les erreurs de 
bits à condition que le nombre d'erreur affectant le paquet de données soit impair, sinon elles ne le sont pas 
(normal car si 2, 4, 6 ou plus de bits sont modifiés, la parité ne change pas !). Quelques calculs de probabilité
nous permettent de dire : 
\begin{center}
$p\{erreur\ detectee\} = p[X\ impair] = \sumin{1,i\ impair}{N}{\binom{N}{k}p^k(1-p)^{(N-k)}}$
$p\{erreur\ non-detectee\} = p[X\ pair] = \sumin{1,i\ pair}{N}{\binom{N}{k}p^k(1-p)^{(N-k)}}$
\end{center}

Par exemple, si $N=16+1$(bit de parité), on a $p[X\ impair] = 0.1453391$ (mais si on avait aucun bit de parité la 
probabilité serait de $0$) et $p[X\ pair] = 0.01171769$ (sans bit de parité : $0.1485422$).

\subsubsection{FEC (Forward Error Correction)}

\noindent\textbf{\underline{Bit de parité à 2 dimensions}} \\

\noindent Connu également sous le nom de \textbf{VRC/LRC}\textit{(Vertical/Longitudinal Redundacy Check)}, il 
consiste à calculer un bit de parité comme ci-dessus pour chaque bloc de $N$ bits mais également un bit de parité 
en «colonne» pour chaque ensemble de $M*N$ bits. Il peut ainsi détecter et corriger les erreurs d'un seul bit 
$\rightarrow$ moins de retransmissions sont nécéssaires.

\imgRT{CN_105.png}{200}

\textit{Typiquement, les blocs font 7/8 bits (1 caractères).}\\
\newpage
\noindent Les vérifications de parité sont principalement utilisées dans les transmissions de données asynchrones 
(comment dans des bus en série), elles impliquent un \red{overhead} significatif de \red{$12,5\%$} (1 bit 
supplémentaire par groupe de 8 bits). L'efficacité de cette pratique diminue au fur et à mesure que la taille
des blocs augmente, de plus, elle n'est pas adéquate pour les transmissions de données synchronisées à haute 
vitesse, nous aurons besoins d'autres mécanismes comme la \textit{checksum} et le \textbf{CRC}\textit{(Cyclic 
Redundancy Check)}. \\

\subsubsection{CRC (Cyclic Redundancy Check)}

Le but de ce principe est de maximiser la probabilité de trouver les erreurs tout en minimisant l'overhead de 
redondance nécessaire. L'idée est devoir chaque paquet de $N$ données comme un polynôme de degré $N-1$, $M(x)$
\textit{(Exemple : $10011010 \rightarrow x^7+x^4+x^3+x$)}. \\

\stitre{Principe}

\noindent L'expéditeur et le récepteur s'échangent des polynômes, ils se mettent d'accord sur un polynôme $C(x)$
qui fait partie du protocole (comme le polynôme 32 bits d'Ethernet) ensuite $P(x)$ est calculé à partir de $M(x)$ 
ce premier étant divisible par $C(x)$.

\imgR{CN_106.png}{200}

\stitre{Arithmétique polynomiale modulo 2}

\noindent \begin{itemize}
\item Coefficients $\in\{0,1\}$,
\item $x+y=x-y=x\ XOR\ y$,
\item $B(x)$ est divible par $C(x)$ si et seulement si le degré de $B(x)$ est plus grand que le degré de $C(x)$, 
\item $B(x)/C(x) = B(x)-C(x)$ et celui-ci est obtenu en effectuant des $XOR$ bit à bit.
\end{itemize}

Voici un rappel de la table de vérité de $XOR$ ainsi qu'un exemple : \\

\begin{center}\imageRT{CN_107.png}{100} \imageR{CN_108.png}{200}\end{center}

\noindent Il faut maintenant calculer $P(x)$, la façon de faire est décrite dans le cours, elle ne sera pas 
développée ici. \\

\stitre{Le polynôme $C(x)$}

\noindent L'idée pour trouver $C(x)$ est de trouver un polynôme tel qu'il est très peu probable qu'il soit 
divisible uniformément en présence d'erreur. On prouve ça de la manière suivante, on considère $P'(x)$ le 
polynôme $P(x)$ entaché d'une erreur représentée par $E(x)$ (on a donc $P'(x)=P(x)+E(x)$) et on sait que l'erreur
ne sera pas découverte si le polynôme $P'(x)$ est uniformément divisble par $C(x)$, il faut donc que : \\
$reste\left(\dfrac{P'(x)}{C(x)}\right) \neq 0 \\
\Leftrightarrow reste\left(\dfrac{P(x)+E(x)}{C(x)}\right) \neq 0 \\
\Leftrightarrow reste\left(\dfrac{P(x)}{C(x)}\right) + reste\left(\dfrac{E(x)}{C(x)}\right) \neq 0 \\
\Leftrightarrow 0 + reste\left(\dfrac{E(x)}{C(x)}\right) \neq 0 \\
\Leftrightarrow reste\left(\dfrac{E(x)}{C(x)}\right) \neq 0 \\$

\noindent\textit{En d'autres termes, il faut que $E(x)$ ne soit pas divisible par $C(x)$.} \\

\stitre{Les polynômes $C(x)$ typiques}

\begin{center}
	\begin{tabular}{|*{2}{c|}}
	\hline
	\dred{\underline{Norme}} & \dred{\underline{Polynôme}} \\
	\hline
	CRC-8 & $x^8+x^2+x^1+1$ \\
	\hline
	CRC-10 & $x^{10}+x^9+x^5+x^4+x^1+1$ \\
	\hline
	CRC-12 & $x^{12}+x^3+x^2+1$ \\
	\hline
	CRC-16 & $x^{16}+x^{15}+x^2+1$ \\
	\hline
	CRC-CCITT & $x^{16}+x^{12}+x^5+1$ \\
	\hline
	CRC-32 & $x^{32}+x^{26}+x^{23}+x^{22}+x^{16}+x^{12}+x^{11}+x^{10}+x^8+x^7+x^5+x^4+x^2+x^1+1$ \\
	\hline
	\end{tabular}
\end{center}

\stitre{Propriétés de CRC}

\begin{itemize}
\item Toutes les erreurs d'un seul bit peuvent être détectées \textit{(si $x^k$ et $x^0$ ont des coefficients 
non-nuls)}.
\item Toutes les erreurs de 2 bits peuvent être détectées \textit{(si $C(x)$ a un facteur avec au moins $3$ 
termes)}.
\item Les erreurs de n'importe quel nombre impair de bits peuvent être détectées \textit{(si $C(x)$ contient le 
facteur $(x+1$)}.
\item Toute erreur d'éclatement avec une longueur $< k$ peut être détectée.
\end{itemize}

\subsection{Liens à accès multiples et protocoles}

Il y a 2 types de liens : 
\begin{itemize}
\item\textbf{point-to-point} : \textit{comme le \textbf{PPP} pour les accès «dial-up», \textbf{HDLC} ou encore 
lien point-to-point entre un switch \textbf{Ethernet} et l'hôte,}
\item\textbf{broadcast} (lien ou média partagé) : \textit{comme le vieux \textbf{Ethernet}, le stream montant 
\textbf{HFC} ou la \textbf{LAN} sans-fil 802.11.}
\end{itemize}

\subsubsection{Les protocoles d'accès multiples}

On utilise un seul canal de diffusion qui est partagé et s'il y a 2 transmissions ou plus par \neu , ça donne 
lieu à des interférences et il y a collision si un \neuSP reçoit 2 (ou plus) signaux en même temps. Le protocole 
d'accès multiples consiste en un algorithme distribué qui détermine comment les \neuSPs partagent le canal (c'est-
à-dire quand un \neuSP peut transmettre) en tenant compte du fait que les communications concernant le partage du 
canal doivent utiliser ce même canal. \\
Prenons un canal de diffusion à $R$ bps, lorsqu'un \neuSP veut émettre, il peut émettre à un taux égal à $R$. 
Lorsque $M$ \neuSPs veulent émettre chacun peu émettre à un taux de $\frac{R}{M}$. Tout ça étant simple et 
totalement décentralisé c'est-à-dire qu'il n'y a pas de \neuSP spécial pour coordonner les transmissions et il n'y 
a aucune synchronisation des horloges ni des slots. \\

\stitre{Classification des protocols d'accès multiples}

\noindent Il existe 3 grandes classes :
\begin{itemize}
\bfp{partitionnement du canal}{divise le canal en plus petite pièces (slots de temps, fréquences, de code) et 
alloue une pièce à un \neuSP pour un usage exclusif}
\bfp{accès aléatoire}{le canal n'est pas divisé, les collisions sont donc possibles mais permet de «récupérer» 
après une collision}
\bfp{à tour de rôle}{un \neuSP prend son tour, un \neuSP ayant plus de données à envoyer peut prendre un tour 
plus long.} \\
\end{itemize}
\newpage
\stitre{Protocoles de partitionement du canal}

\begin{itemize}
\point{TDMA}{ (time division multiple access) chaque \neuSP a accès au canal par tour et chaque station a une
longueur fixée pour les slots qu'elle envoie à chaque tour (la longueur étant égale au temps de transmission du 
paquet). Les slots non-utilisés le restent, comme les slots 2,5 et 6 sur l'exemple ci-dessous : }
\imgR{CN_109.png}{200}
\point{FDMA}{ (frequency division multiple access) le spectre du canal est divisé en bande de fréquence, chaque 
station se voit allouer une bande de fréquence et les bandes non-utilisées le restent : }
\imgR{CN_110.png}{250}
\end{itemize}

\stitre{Protocoles d'accès aléatoire}

Quand un \neuSP a un paquet à envoyer, il le transmet au taux maximum du canal $R$ (aucune coordination à priori 
entre les \neus ) mais lorsque 2 \neuSPs envoient en même temps un paquet, il y a \red{collision}. Dès lors, ces 
protocoles spécifient comment détecter et récupérer après une collision (via, par exemple, des retransmissions 
retardées). Dans ces protocoles on trouve : \textbf{slotted ALOHA}, \textbf{ALOHA}, \textbf{CSMA}, 
\textbf{CSMA/CD}, \textbf{CSMA/CA}, ... \\

\stitre{Slotted ALOHA}

\underline{Hypothèses} : 
\begin{itemize}
\item toutes les frames sont de $L$ bits,
\item le temps est divisé en slots de tailles égales($R$) (le temps pour transmettre une frame est de $L/R$),
\item les \neuSPs commencent à transmettre seulement au début des slots,
\item les \neuSPs sont synchronisés,
\item si 2 ou plus de 2 \neuSPs transmettent dans le même slot, tous les \neuSPs détectent une collision.
\end{itemize}

\noindent Quand un \neuSP obtient une nouvelle frame, il la transmet dans le prochain slot ensuite,
\begin{itemize}
\item soit il n'y a pas collision, le \neuSP peut alors envoyer une nouvelle frame dans le slot suivant,
\item soit il y a collision, le \neuSP retransmet alors la frame dans tous les slots suivants jusqu'à ce qu'il y
parvienne sans collision.
\end{itemize}
\imgR{CN_111.png}{300}
\underline{Les avantages et inconvénients d'\textbf{ALOHA}} :
\begin{itemize}
\bfp{avantages}{un seul \neuSP actif peut transmettre continuellement au taux maximal permis par le canal, 
l'algorithme est simple et hautement décentralisé (seuls les slots dans les \neuSPs doivent être synchronisés)}
\bfp{inconvénients}{il y a des collisions et du gaspillage de slots, des slots inutilisés, les \neuSPs doivent 
être capables de détecter une collision plus rapidement que de transmettre un paquet et les horloges doivent être 
synchronisées}
\end{itemize}

\noindent On définit l'efficacité comme la fraction à long terme de slots "à succès", c'est-à-dire la probabilité
pour qu'un slot soit utilisé pour une transmission utile. Pour ce faire, l'idée est de trouver le $p^*$ qui 
maximise $Np(1-p)^{N-1}$ et après quelques calculs on trouve $p^* = \frac{1}{e} = 0.37$ ce qui signifie que le 
canal est utilisé pour des transmissions utiles \red{$37\%$} du temps. \\

\stitre{Unslotted ALOHA}

\noindent Plus simple et pas de synchronisation, lorsqu'une frame arrive on la transmet immédiatement ce qui 
augmente le risque de collisions, en effet, la frame envoyée en $t_0$ peut entre en collision avec toutes les 
frames émises dans l'intervalle $[t_0-1,t_0+1]$. Dans ce cas-ci l'efficacité est de \red{$18\%$}.\\

\stitreD{CSMA}{Carrier Sense Multiple Access}

\noindent Le principe est d'écouter avant d'envoyer, si le canal n'est pas utilisé on transmet, sinon on retarde 
la transmission. Il se peut cependant que des collisions arrivent, les délais de transmissions peuvent impliquer 
que 2 \neuSPs ne se rendent pas compte qu'ils vont émettre en même temps et dans ce cas là tout le temps de 
transmission d'un paquet est gaspillé. On note donc le rôle de la distance et des temps de transmission dans la
probabilité qu'une collision survienne.

\imgRT{CN_112.png}{200}

Une variante plus élaborée existe, il s'agit de \textbf{CSMA/CD} \textit{(CSMA/Collision 
Detection)}. Les collisions sont détectées dans un temps assez court et les transmissions qui engendrent une 
collision sont arrêtées dès que l'on détecte une collision (on réduit ainsi le temps de gaspillage du canal). En
ce qui concerne la déctection de collision, c'est chose aisée dans les transmissions cablées (on calcule la force 
des signaux et on compare la force du signal envoyé et la force du signal reçu) mais c'est plus difficile dans 
les \textbf{LAN}'s sans fil (la force du signal reçu étant affecté par la force de transmission locale).

\imgR{CN_113.png}{200}
\newpage
\stitre{Les protocoles «à tour de rôle»}

\noindent On a vu que les protocoles de partitionnement du canal divisaient le canal de manière équitable et 
efficace à haute charge mais lorsque peu de \neuSPs utilisent le canal, il y a des slots non-utilisés ($\frac{1}
{N}$ de la bande passante est allouée si il n'y a qu'un seul \neuSP actif). Quant aux protocoles d'accès 
aléatoire, c'est l'inverse, ils sont efficaces lorsque le canal est peu chargé, un \neuSP pouvant utiliser toute la
bande passante lorsqu'il est le seul actif mais à haute charge, les risques de collisions augmentent diminuant 
ainsi les perfomances globales. Les \textbf{protocoles «à tour de rôle»} va rechercher le meilleur des 2 
protocoles décrits précédemment.\\
Il y a ce qu'on appelle le \textit{«polling»}, ceci consiste à nommer un \neuSP comme maître, ce maître invite les 
\neuSPs esclaves à transmettre les uns après les autres. \textit{(Cette façon de faire est souvent utilisé avec 
des périphériques "bêtes")} Il faut cependant s'inquièter de 3 détails, il y a un overhead pour le
\textit{polling}, il y a une certaine latence et si le maître a des ratés, tout le système est compromis. On peut 
également utiliser le \textit{«token passing»} qui consiste à se passer séquentiellement un jeton, le \neuSP 
possédant le jeton pouvant transmettre. Les inquiétudes sont les mêmes que pour le \textit{polling} sauf qu'ici 
l'overhead provient du \textit{token} et si ce dernier a des ratés, tout le système est compromis. \\

\stitre{Résumé des protocoles à accès multiples}

\begin{itemize}
\point{partionnement du canal}{division par temps, fréquence ou code.}
\point{accès aléatoire}{
\begin{itemize}
\item \textbf{ALOHA}, \textbf{S-ALOHA}, \textbf{CSMA}, \textbf{CSMA/CD},
\item détection de transmission facile à implémenter dans les technologies cablées, difficile dans les sans-fils,
\item \textbf{CSMA/CD} est utilisé dans \textbf{Ethernet}\textit{(IEEE 802.3)}
\item \textbf{CSMA/CA} est utilisé dans les sans fils \textit{(IEEE 802.11)}
\end{itemize}}
\point{à tour de rôle}{\textit{«polling»} («scrutin») ou \textit{«token passing»}, utilisés dans 
\textbf{Bluetooth}\textit{(IEEE 802.15.1)}, \textbf{FDDI}, \textbf{IBM Token Ring}\textit{(IEEE 802.5)}}
\end{itemize}

\subsection{Adressage couche lien}

\subsubsection{Les adresses MAC et ARP}

Les adresses \textbf{MAC}\textit{(Medium Access Control)} (parfois appellées adresses \textbf{LAN}, 
\textbf{physiques} ou \textbf{Ethernet}) sont codées sur $48$bits (adresse broadcast : FF-FF-FF-FF-FF-FF),
gravées dans la \textbf{ROM} de la \textbf{NIC}\textit{(Network Interface Card)}et parfois modifiables par les 
programmes. Elles ont comme fonction de passer une frame d'une interface à une autre qui lui est connectée 
physiquement (même réseau). L'allocation de celles-ci est gérée par l'\textbf{IEEE} et les fabriquants achètent 
des intervalles d'adresses \textbf{MAC} pour garantir l'unicité. L'adresse \textbf{MAC} est fixe, ça permet une 
portabilité : on peut déplacer la carte \textbf{LAN} d'une \textbf{LAN} à l'autre (Ce n'est pas le cas des 
adresses \textbf{IP} qui dépendent du sous-réseau auquel le matériel est connecté). \\

\stitreD{ARP}{Address Resolution Protocol}

\noindent Ce protocole permet de déterminer l'adresse \textbf{MAC} d'un hôte tout en connaissant son addresse 
\textbf{IP}, chaque hôte possèdant une table \textbf{ARP} stockant des couples adresse \textbf{MAC}/adresse 
\textbf{IP}, chaque couple étant associé à un \textbf{TTL}\textit{(Time To Live)}, temps après quoi le couple est
oublié (20 minutes typiquement). \\
\underline{Scénario exemple} : \\
$A$ veut envoyer un datagramme à $B$ et l'adresse \textbf{MAC} de $B$ n'est pas dans la table \textbf{ARP} de 
$A$, $A$ \textbf{broadcast} un paquet requête \textbf{ARP} contenant l'adresse \textbf{IP} de $B$ (toutes les
machines connectées à la \textbf{LAN} reçoivent le message). $B$ reçoit le paquet et répond à $A$ avec son 
adresse \textbf{MAC} (en unicast) ; à la réception de cette réponse, $A$ sauvegarde le mapping entre l'adresse 
\textbf{IP} et l'adresse \textbf{MAC} de $B$ (ce couple sera supprimé de la table lorsque le \textbf{TTL} sera 
expiré). Tout ce processus se fait sans l'intervention des administrateurs, les \neuSPs créent leurs tables 
\textbf{ARP} seuls, on dit que le processus est \textit{«plug-and-play»}. \\
\newpage
\underline{Autre scénario} :
\imgR{CN_114.png}{350}
$A$ désire envoyer un message à $B$ (on suppose que $A$ connaît l'adresse \textbf{IP} de $B$) via le routeur $R$ 
, ce dernier possède 2 tables \textbf{ARP} une pour chaque \textbf{LAN}. $A$ créé un datagramme avec comme source
$A$ et comme destination $B$, $A$ utilise \textbf{ARP} pour avoir l'adresse \textbf{MAC} de $R$ avec l'adresse 
\textbf{IP} $111.111.111.110$. $A$ crée ensuite une frame couche réseau avec l'adresse \textbf{MAC} de $R$ comme
destination, cette frame contenant le datagramme \textbf{IP} destiné à $B$. La \textbf{NIC} de $A$ envoie la 
frame à la \textbf{NIC} de $R$, $R$ décapsule le datagramme \textbf{IP} de la frame \textbf{Ethernet} et lis 
ensuite le destinataire du datagramme. $R$ utilise alors sa table \textbf{ARP} pour avoir l'adresse \textbf{MAC} 
de $B$ et crée ensuite une frame \textbf{Ethernet} contenant le datagramme \textbf{IP} venant de $A$ et destiné à 
$B$.

\subsection{Ethernet}

Technologie de \textbf{LAN} cablée dominante, elle est peu coûteuse, ce fut la première technologie \textbf{LAN} 
largement utilisée, elle est plus simple et moins cher que les \textit{token \textbf{LAN}'s} et \textbf{ATM} et 
elle supporte des grandes vitesses (de $10$Mbps à $10$Gbps). \textbf{Ethernet} utilise la topologie en étoile, au 
lieu de connecter tous les \neuSPs sur un bus partagés (et ainsi donner lieu à d'éventuelles collisions), on place
un switch actif au milieu et les \neuSPs connectés à ce switch communiquent entre eux via un protocole 
\textbf{Ethernet} (via le switch).
\imgR{CN_115.png}{200}

\stitre{Structure d'une frame Ethernet}

\imgR{CN_116.png}{250}
\begin{itemize}
\bfp{preamble}{il contient 7 bytes (avec comme modèle $10101010$) suivi par un byte (avec comme modèle 
$10101011$), ce \textbf{preamble} est utilisé pour synchroniser la fréquence des horloges du récepteur et de 
l'expéditeur en utilisant un \textbf{PLL} \textit{(Phase-Locked Loop)},}
\bfp{Adresses}{codées sur $6$ bytes, si l'adaptateur reçoit une frame avec une adresse de destination qu'il 
connait (ou une adresse broadcast) il passe les données de la frame au protocole de la couche réseau,}
\bfp{Type}{indique le protocole de la couche supérieure (principalement \textbf{IP} mais peut également être 
\textbf{Novell IPX}, \textbf{AppleTalk}, \textbf{ARP}, ...}
\bfp{CRC}{appellée parfois \textbf{FCS}\textit{(Frame Check Sequence)}, elle est vérifiée à la réception et si 
une erreur est détectée, la frame est droppée.} \\
\end{itemize}

\noindent \textbf{Ethernet} est sans connexion et non fiable (comme \textbf{UDP}), les \textit{streams} de 
datagrammes passés à la couche réseau peuvent présenter des trous, ces trous seront remplis si l'application 
utilise \textbf{TCP}. Il utilise le protocole \textbf{unslotted CSMA/CD} comme protocole d'accès multiple.\\

\stitre{L'algorithme \textit{CSMA/CD} d'Ethernet}

\underline{Fonctionnement}

\begin{lstlisting}
 1  La NIC recoit des datagrammes de la couche reseau
 2      cree une frame
 3  Si la NIC sent que le canal est non-utilise 
 4     alors elle commence la transmission de la frame
 5  Sinon elle attend que le canal soit libre
 6  Si la NIC ne recoit aucune autre transmission pendant la transmission de sa frame
 7     alors elle en a fini avec la frame
 8  Sinon la transmission est avortee et la NIC envoie un signal jam
 9  Apres avoir avorte la transmission, la NIC entre en exponential backoff :
10           Apres la m^ieme collision, 
11              la NIC choisit aleatoirement K dans {0,1,...,2^m - 1}
12              la NIC attend K*512 "bits times" et puis reessaie.
\end{lstlisting}

\noindent Le signal \textbf{jam} sert à s'assurer que les autres \neuSPs qui sont entrain de transmettre ont 
détectés une collision, ils font de $32$ à $48$ bits. Le \textit{«bit time»} est égal à 1 microseconde pour 
l'Ethernet $10$Mbps et pour $K = 1023$, l'attente est d'environ $50$msec. Le but de l'\textit{exponential 
backoff} est d'adapter les tentatives de retransmission à la charge actuelle estimée, plus la charge est grande 
plus le temps d'attente aléatoire sera grand. (Exemple : $10$ collisions $\rightarrow 2^{10} - 1 = 1023 
\rightarrow$ on choisit $K$ dans $\{0,1,...,1023\}$) \\

\noindent L'efficacité $E$ de cet algorithme est calculé comme suit : $E = \dfrac{1}{1+5\left( \frac{t_{prop}}
{t_{trans}} \right)}$ où $t_{prop}$ est le délai maximum de propagation entre 2 \neuSPs dans la \textbf{LAN} et 
$t_{trans}$ est le temps mis pour transmettre une frame de taille maximale. Cette efficacité tend vers $1$ 
lorsque $t_{prop}$ tend vers $0$ et/ou $t_{trans}$ tend vers $\infty$. Cet algorithme est dès lors plus simple, 
bon marché, décentralisé et meilleur que \textbf{ALOHA}. \\

\noindent Il existe beaucoup de différents standards \textbf{Ethernet}, ils ont tous le protocole d'accès 
multiple et la taille des frames en commun mais ont différentes vitesses de transmission (de $2$ Mbps à $10$ 
Gbps) et utilise des liens physiques différents (câble, fibre, ...)

\imgR{CN_117.png}{275}

\subsection{Transmission physique \textit{(Couche physique)}}

Nous allons nous intéresser un peu à la manière dont les bits sont transmis sur un média. Pour transmettre les 
bits, il y a un \textit{encodage} fait par l'expéditeur et un \textit{décodage} effectué par le récepteur ; il y 
a un bon nombres de schémas d'encodages ingénieux mais nous allons nous intéresser uniquement aux plus évidents 
et plus faciles. Lors d'une transmission, les bits (encodés en signaux électriques) sont soumis à différents 
phénomènes comme le bruit, la bande passante limitée ou encore l'atténuation du signal. Voici un schéma simplifié
de la transmission de bits :

\imgR{CN_118.png}{225}

Pour les bandes passantes hautes fréquences, le modulateur imprime des symboles digitaux encodés sur un 
transporteur qui transforme la fréquence, quant aux basses fréquences, le modulateur encode les symboles digitaux 
sous une forme de vague adaptée à la transmission (en générale une onde carrée) ($\rightarrow$ pas de 
transformation de la fréquence).

\subsubsection{NRZ (Non-Return-to-Zero)}

Encodage évident, on imprime un niveau électrique par niveau de bit soit de manière unipolaire ($1=+A,\ 0=0$) 
soit de manière bipolaire ($1=+A,\ 0=-A$) (transmission basse-fréquence $\rightarrow$ pas de fréquence porteuse).

\imgR{CN_119.png}{225}

\noindent Le signal est échantillonné avec l'horloge et on a des seuils pour reconnaître un $1$ ou un $0$. Cette 
méthode nécéssite une synchronisation des 2 horloges, en effet, le cas échéant le récepteur pourrait encore plus 
ou moins de bits qu'il n'a réellement réçu ; comme sur l'exemple ci-dessous.

\imgR{CN_120.png}{225}

\subsubsection{Encodage de Manchester}

\imgR{CN_121.png}{225}

Chaque bit correspond à une transition, le $0$ est représenté par un passage du niveau le plus bas au niveau le 
plus haut, le $1$ est représenté par l'inverse (il est utilisé dans \textbf{10BaseT}). Il permet aux horloges des
2 pairs communiquants de se synchroniser (en utilisant un \textbf{PLL} par exemple) en envoyant une frame 
préambule. L'inconvénient majeur de cet encodage est que la fréquence d'un signal peut être, dans le pire des 
cas, 2 fois celle d'un signal encodé en \textbf{NRZ} (efficacité : $50\%$ \textit{(voir exemple ci-dessous)}.

\imgR{CN_122.png}{300}

\subsubsection{Encodage 4B/5B}

L'objectif de ce codage est d'augmenter l'efficacité du codage ($80\%$), dans ce cas-ci, le \textit{baud rate} 
(le nombre de signaux envoyés par seconde) est égal au \textit{bit rate} (le nombre de bits envoyés par seconde)
multiplié par $\frac{5}{4}$. Le principe est d'encoder chaque groupe de $4$ bits par un code de $5$ bits choisi
spécialement pour éviter les suites de plus de deux $0$ consécutifs (les suites de $1$ sont résolues en utilisant 
\textbf{NRZI} \textit{(\textbf{NRZ} inverted)} (utilisé dans \textbf{100BaseT}).

\imgR{CN_123.png}{300}

\subsection{Les switchs de la couche lien}

\subsubsection{Les Hubs}

Basé sur la topologie en étoile, ce matériel \textit{(bête)} est un «répéteur» dans le sens où il reçoit des bits
sur une de ses interfaces et il les transmet sur toutes les autres interfaces (à la même vitesse). Il y a donc 
des risques évidents de collisions, le \textbf{hub} n'utilisant aucun buffer ni d'algorithme évitant les 
collisions.

\subsubsection{Les Switchs}

Ce matériel est \textit{«plus intelligent»} qu'un \textbf{hub}, il prend un rôle actif. Il est capable de stocker
et de forwarder des frames \textbf{Ethernet}, pour ce faire, il examine l'adresse \textbf{MAC} des frames 
entrantes et il forwarde de manière sélective la frame sur une ou plusieurs interfaces (il utilise 
\textbf{CSMA/CD} pour accéder aux canaux). Il est totalement transparent, \textit{plug-and-play} et il apprend 
par lui même, tout ceci implique que les hôtes ne sont pas au courant qu'il est présent et il n'a pas besoin 
d'être configuré. Les hôtes ont une connexion dédiée et directe avec le \textbf{switch}, \textbf{Ethernet} est 
utilisé comme protocole sur le canal permettant cette connexion. Les seules collisions possibles sont sur ces 
connexions, en effet, le \textbf{Switch} suivant peut très bien permettre l'acheminement d'une frame de $A$ à 
$A'$ et de $B$ à $B'$ de manière simultanée et sans collision.

\imgR{CN_124.png}{100}

Pour connaitre les différents destinations qu'il peut atteindre, le \textbf{switch} garde en mémoire une table 
appellée \textit{Switch Table} qui contient des triplets (adresse MAC, interface, TTL). Il la remplit au fur et à 
mesure qu'il reçoit des frames (il regarde qui est l'expéditeur et ajoute l'adresse MAC de l'expéditeur ainsi que
l'interface d'où vient la frame dans sa table). A la réception d'une frame, le \textbf{switch} forwarde la frame
uniquement si la destination n'est pas sur l'interface d'où vient la frame et si la destination est inconnue, il
envoie la frame sur toutes ses autres interfaces. On peut interconnecter plusieurs \textbf{switch}'s :

\imgR{CN_125.png}{250}

Imaginons que $C$ envoie une frame à $I$ :
\begin{itemize}
\item d'abord $C$ envoie la frame à $S_1$, $S_1$ ne connaissant pas la destination l'envoie sur ses 3 autres 
interfaces,
\item $A$ et $B$ reçoivent une frame qui ne leur est pas destinée, ils la droppent,
\item $S_4$ reçoit une frame avec une destination inconnue, il l'envoie sur ses 2 autres interfaces,
\item $S_2$ reçoit une frame avec une destination inconnue, il l'envoie sur ses 3 autres interfaces,
\item $D$, $E$ et $F$ droppent la frame,
\item $S_3$ reçoit une frame avec une destination inconnue, il l'envoie sur ses 3 autres interfaces,
\item $G$ et $H$ reçoivent une frame qui ne leur est pas destinée, ils la droppent,
\item $I$ reçoit la frame et répond à $C$,
\end{itemize}
\textit{(Remarque : chaque switch qui reçoit la frame venant de $C$ enregistre la destination $C$ dans sa table, 
idem avec la réponse de $I$)}\\
\textit{(Remarque 2 : on peut aussi envisager que $S_3$ connaisse déjà l'adresse de $I$ et ne flood pas la 
frame)}

\subsubsection{VLAN (Virtual LAN)}

En pratique, on utilise des cables redondants afin de résister plus facilement aux pannes car si 2 switchs ne 
sont reliés que par un seul lien, si celui-ci tombe en panne, la communication n'est plus possible.

\imgR{CN_126.png}{250}

Cependant, imaginons que $A$ envoie une frame à $W$, si les 2 liens sont opérationnels, le paquet va tourner 
plusieurs fois ($S_1$ et $S_2$ ne connaissant pas la destination, ils vont l'envoyer sur chacune de leurs 
interfaces, $S_3$ étant connecté aux $2$, il va recevoir $2$ fois la frame). La solution à ce problème est 
d'utiliser les \textbf{VLAN}'s. Une seule \textbf{LAN} physique peut posséder plusieurs \textbf{VLAN}, le 
principe de celles-ci est de forwarder le trafic venant d'une \textbf{VLAN} uniquement sur cette même 
\textbf{VLAN} \textit{(Chaque port du switch est assigné à une ou plusieurs \textbf{VLAN})}.

\imgR{CN_127.png}{250}

\noindent Si une \textbf{LAN} possède 2 \textbf{VLAN}'s et 2 \textbf{switch}'s, il n'est pas obligatoire d'avoir 
un lien par \textbf{VLAN} entre les 2 \textbf{switch}'s, il suffit d'ajouter une donnée à la frame transportée :

\imgR{CN_128.png}{300}

\noindent\underline{\textbf{Remarque}} : les \textbf{switchs} et les \textbf{routeurs} sont tous 2 des matériels 
de stockage et forwarding mais les \textbf{routeurs} font partie de la couche réseau (ils examinent le header 
de la couche «réseau») alors que les \textbf{switchs} font partie de la couche liens. De plus, les 
\textbf{routeurs} maintiennent une table de routage et implémentent des algorithmes de routage tandis que les 
\textbf{switchs} maintiennent une table de switch et implémentent des algorithmes de filtrage et d'apprentissage.

\subsection{PPP (Point-to-Point Protocol)}

Le principe du \textit{«Point-to-Point»} est simple, un expéditeur, un récepteur, un lien ; il n'y a donc aucun 
contrôle d'accès au média et aucune nécéssité de l'adressage explicite \textbf{MAC}. Ce principe est utilisé pour
les liens de type \textit{dial-up} ou encore \textbf{ISDN}. Les protocoles de \textit{«Point-to-Point»} 
\textbf{DLC} \textit{(Data Link Control)} sont \textbf{PPP} et \textbf{HDLC}\textit{(High level Data Link Control 
)}. \\

\stitre{Les exigences de PPP}
\begin{itemize}
\point{Packet framing}{encapsulation des datagrammes de la couche réseau dans des frames de la couche lien, 
transporte des données de n'importe quel protocole de la couche réseau,}
\point{Transparence des bits}{doit transporter tous les modèles de bits dans le champ de données}
\point{Détection d'erreur}{pas de correction}
\point{Connection Liveness}{détecte et signale une défaillance du lien à la couche réseau}
\point{Négociation de l'adresse couche réseau}{chacune des extrémités peut apprendre/configurer l'adresse de 
l'autre}
\end{itemize}
Il n'y a cependant pas de corrections/recouvrements d'erreurs, contrôle de flux, assurance sur l'ordre d'arrivée 
ni la possibilité de supporter des liens «multipoints» (comme avec le polling).\\
\newpage
\stitre{Structure de la frame}

\imgR{CN_129.png}{300}
\begin{itemize}
\bfp{flag}{délimiteur pour le framing}
\bfp{address}{ne fait rien (c'est une option)}
\bfp{control}{ne fait rien (dans le futur, peut-être y aura-t-il des champs de controle multiple)}
\bfp{protocol}{le protocole de la couche supérieur auquel délivrer la frame (\textbf{PPP-LCP},\textbf{IP},...)}
\bfp{info}{données de la couche supérieure qui sont transportées}
\bfp{check}{CRC pour la détection d'erreur}
\end{itemize}

\noindent A cause de la transparence des bits, le champ de données doit être autorisé à contenir une suite de bit 
qui est également un \textit{flag}, dès lors il faut que le récepteur puisse savoir que ce qu'il reçoit n'est pas 
un flag mais une suite de bits faisant parties des données. Pour signifier celà, l'expéditeur envoie avant cette 
suite de bits une autre suite de bits qui \textit{«déspécialise»} le \textit{flag} qui va le suivre. Pour faire 
une analogie, c'est comme le $\backslash$ dans les chaines de caractères dans les langages de programmation (et 
si l'expéditeur doit envoyer la suite de bits correspondant au $\backslash$, il en envoie $2$, le récepteur 
enlevant automatiquement le premier). Ce procédé est appelé \textit{«Byte stuffing»}.

\imgR{CN_130.png}{225}

Avant d'échanger des données de la couche réseau, les pairs de la liaison de données doivent configurer le lien 
\textbf{PPP} \textit{(longueur maximale des frames, authentification, ...} et «prendre connaissance/configurer» 
les informations de la couche réseau \textit{(pour \textbf{IP}, il faut transporter les messages \textbf{IPCP} 
pour configurer/prendre connaissance des adresses \textbf{IP}}.

\subsection{Virtualisation des liens : ATM, MPLS}

La virtualisation est une chose souvent utilisé en informatique, il s'agit d'une abstraction puissante dans le 
ingénierie des systèmes (comme la \textbf{JVM} par exemple). L'approche des couches d'abstractions nous dit de ne 
pas nous inquiéter des détails de la couche inférieure et de communiquer avec elle qu'abstraitement. \\

La virtualisation des liens a pour but d'homogéniser la communication entre les réseaux, afin de permettre à 2 
réseaux de type différents (ayant des conventions différentes) de communiquer (comme un réseau cablé et un réseau 
satellite). Ainsi, au départ il y a 2 couches d'adressage, \textit{«Internetwork»} et \textit{«local network»} et
on ajoute une couche s'occupant de tout rendre homogène au niveau de la couche \textit{«Internetwork»}. 

\subsubsection{ATM (Asynchronous Transfer Mode)}

\textbf{ATM} et \textbf{MPLS} sont des réseaux séparés avec leurs propres droits ; ils ont des modèles de 
services, d'adressage, et de routage différents d'Internet. Ils sont vu par Internet comme des liens logiques 
connectant des routeurs \textbf{IP} (comme les liens dialup font partie d'un autre réseau, le réseau 
téléphonique). \\

\textbf{ATM} était le standard des années $90/2000$ pour la haute vitesse (de $155$ à $622$ Mbps et encore plus), 
il utilise l'architecture \textbf{BISDN}\textit{(Broadband Integrated Service Digital Network)}. Son but est le 
transport intégré end-end de données vocales, vidéos. Il est ainsi caractérisé par les exigences de temps de 
rencontre/qualité de service imposées par la voix et les vidéos et sa commutation de paquets qui utilisent des 
circuits virtuels (les paquets sont de tailles fixées, on les appelle «cellules». C'est également une base 
technique pour la prochaine génération de téléphonie. \\

\stitre{Architecture}

\imgR{CN_131.png}{200}

On distingue la couche physique (idem que précédemment), la couche \textbf{ATM} qui correspond à la couche 
«réseau» d'Internet et qui s'occupe de la commutation des cellules et du routage et finalement la couche 
d'adaptation. Cette dernière n'est présente qu'aux bords d'un réseau \textbf{ATM}, elle s'occupe de segmenter, 
réassembler les données ; elle est à quelques détails près l'analogue de la couche de transport d'Internet. \\

\noindent \textbf{ATM} est considéré comme s'étendant sur les 2 couches (réseau et transport), en effet, de 
manière visuel, il s'agit d'une technologie de réseau transportant des données d'un point à un autre ; d'un autre 
point de vue, les switch \textbf{ATM} sont utilisés pour connecter des routeurs \textbf{IP} \textit{«d'épine 
dorsale»} (voir chapitre sur la couche réseau). \\

\stitreD{AAL}{Couche «ATM Adaptation»}

Elle permet d'adapter la couche supérieure (\textbf{IP} ou application native \textbf{ATM}) à la couche 
\textbf{ATM}, ainsi elle est présente que dans les systèmes en bordure du réseau (pas dans les \textbf{switchs}).
Elle fragmente un segment qu'elle reçoit en plusieurs cellules \textbf{ATM} (un peu comme les segments 
\textbf{TCP} fragmentés dans plusieurs datagrammes \textbf{IP}). Il y a différentes versions des couches 
\textbf{AAL}, dépendant la classe de services d'\textbf{ATM} : \textbf{AAL1} pour le 
\textbf{CBR}\textit{(Constant Bit Rate)} comme pour l'émulation de circuit, \textbf{AAL2} pour le 
\textbf{VBR}\textit{(Variable Bit Rate)} comme pour des vidéos \textbf{MPEG}, \textbf{AAL5} pour les données 
comme les datagrammes \textbf{IP}. \\

\stitre{Couche ATM}

Le service principal qu'elle fournit est de transporter les cellules à travers le réseau \textbf{ATM}, elle 
fournit également d'autres services vraiment différents de ceux de la couche réseau de \textbf{IP} (\textbf{CBR},
\textbf{ABR}, \textbf{VBR}, \textbf{UBR}). \\

\stitre{Circuits virtuels}

\noindent Les cellules sont transportées sur les circuits virtuels de la source à la destination, 
\begin{itemize}
\item organisation et démontage du «call» pour chaque «call» avant que les donnés puissent transiter,
\item chaque paquet contient un identifiant de circuit virtuel (pas l'ID de la destination),
\item chaque switch sur le chemin entre la source et la destination maintient un état pour chaque connexion 
passante,
\item les liens, les ressources des switch's (buffers, bandes passantes) doivent être alloués au circuit virtuel 
pour avoir des performances comme dans un circuit.
\end{itemize}
Il existe des circuits permanents (\textbf{PVC}), il s'agit de connexions longue durée, elles sont typiquement 
permanentes lorsqu'elles permettent de joindre un routeur \textbf{IP} ; il existe également des circuits commutés
(\textbf{SVC}) qui sont mis en place dynamiquement sur la base du \textit{per-call}.
\begin{itemize}
\item \textbf{avantage} de l'approche circuit virtuel d'\textbf{ATM} :
	\begin{itemize}
	\item la performance «qualité de service» garantie pour les connexions mappée en circuit virtuel (bande 
	passante, délai, ..)
	\end{itemize}
\item \textbf{désavantages} :
	\begin{itemize}
	\item support inefficace pour le trafic de datagrammes,
	\item un circuit virtuel permanent entre chaque paire source/destination n'est pas faisable ($N^2$ connexions 
	nécessaires)
	\item les circuits commutés introduisent une latence, un overhead de transformation pour l'organisation des 
	\textit{call} lors de connexions à courte durée. \\
	\end{itemize}
\end{itemize}

\stitre{La cellule ATM}

\imgR{CN_132.png}{275}

Elles possèdent un header de $5$ bytes et un payload de $48$ bytes, celui-ci est petit car le délai pour créer 
les cellules est très petit et il fait $48$ bytes car c'est un compromis entre $32$ et $64$ bytes. Les champs du 
\textit{header} :
\begin{itemize}
\bfp{VCI}{identifiant du canal virtuel (le champ va changer de lien en lien à travers le réseau)}
\bfp{PT}{type de payload (cellule \textit{RM} ou cellule de données)}
\bfp{CLP}{priorité de la cellule (plus c'est élevé plus elle est prioritaire)}
\bfp{HEC}{checksum du header (CRC)} \\
\end{itemize}

\stitre{Couche physique d'ATM}

Il y a 2 morceaux dans la couche physique, 2 \textit{«sous-couches»} la \textbf{TCS}\textit{(Transmission 
Convergence Sublayer)} qui adapte la couche \textbf{ATM} à la sous-couche supérieure, la 
\textbf{PMD}\textit{(Physical Medium Dependent)} qui dépend du média physique utilisé. La génération de la 
checksum CRC du header, la délimitation des cellules, l'envoi de cellules vides lorsque la \textbf{PMD} n'est
pas structurée ; tout ça sont des fonctions de la \textbf{TCS}. La \textbf{PMD} peut avoir plusieurs structures :
\textbf{SONET/SDH} structure de transmission de frames (comme un conteneur transportant des bits), elle utilise 
la synchronisation des bits, la partition temporelle et elle possède différentes vitesses ; \textbf{TI/T3} 
structure de transmission de frames (vieille hiérarchie des téléphones) possèdant des vitesses allant de 
$1.5$Mbps à $45$Mbps. \\

\stitre{IP-Over-ATM}

\begin{multicols}{2}
\textbf{IP} classique, on utilise uniquement les adresses \textbf{IP} et \textbf{MAC} (sur le schéma ci-contre,
il y a $3$ «réseaux»)
\imgR{CN_133.png}{70}
\end{multicols}

\begin{multicols}{2}
\textbf{IP-Over-ATM}, ici on a remplacé un «réseau» par un «réseau \textbf{ATM}», on va devoir jongler entre 
adresses \textbf{ATM} et adresses \textbf{IP}.
\imgR{CN_134.png}{100}
\end{multicols}

\begin{multicols}{2}
\imgR{CN_135.png}{225}
\begin{enumerate}
\bfp{à l'hôte source}{la couche \textbf{IP} effectue le mapping entre l'adresse \textbf{IP} et l'adresse 
\textbf{ATM} de la destination (en utilisant \textbf{ARP}),}
\bfp{dans le réseau ATM}{déplacer les cellules le long d'un circuit virtuel vers la destination,}
\bfp{à l'hote destination}{\textbf{AAL5} réassemble les cellules pour recréer le datagramme original et si la CRC
est ok, le datagramme est passé à la couche \textbf{IP}.} \\
\end{enumerate}
\end{multicols}

\stitreD{MPLS}{MultiProtocol Label Switching}

\imgR{CN_136.png}{225}

Le but initial était d'augmenter la vitesse de forwarding d'\textbf{IP} en utilisant un \textit{label} de 
longueur fixée à la place des adresses \textbf{IP} pour effectuer ce forwarding (mais les datagrammes \textbf{IP} 
conserve quand même les adresses \textbf{IP}). On va adapter les idées de l'approche des circuits virtuels.

\begin{multicols}{2}
\imgR{CN_137.png}{225}
Les routeurs \textbf{MPLS} intelligents que l'on connait aussi sous le nom de \textbf{LSR} \textit{(Label-
Switched Router)} forwardent les paquets sur une interface de sortie en se basant uniquement sur la valeur du 
label (n'inspecte même pas l'adresse \textbf{IP}) (la table de forwarding de \textbf{MPLS} est donc distincte de 
la table de forwarding \textbf{IP}). Un protocole de signalisation est nécéssaire pour organiser le forwarding, 
rendant le forwarding possible sur des chemins que \textbf{IP} seul ne permettrait pas d'emprunter et utilisant 
\textbf{MPLS} pour l'ingénierie du trafic. 2 protocoles de ce type connus : \textbf{RSVP-TE} et \textbf{LDP}. 
Ces routeurs intelligents doivent coexister avec les routeurs \textit{\textbf{IP} only}. \\
\end{multicols}

\hbox{\raisebox{0.4em}{\vrule depth 0.4pt height 0.4pt width 10cm}}

\section{Sans fil et réseaux mobiles}

\subsection{\'{E}léments d'un réseau sans fil}

\imgR{CN_142.png}{300}
\begin{itemize}
\point{Les hôtes (A)}{il s'agit de pc portable, pda, \textbf{IP} phone, ... Ils exécutent des applications et 
peuvent être mobiles ou pas (sans fil ne signifie pas toujours mobilité).}
\point{La station de base (B)}{elle est typiquement connectée à un réseau cablé, elle sert de relai pour/est 
responsable de l'envoi de paquets entre le réseau sans fil et le réseau cablé. (Les antennes de téléphonie 
cellulaire ou encore les points d'accès 802.11)}
\point{Les liens sans fil (C)}{ils sont typiquement utilisés pour connecter les mobiles à la station de base mais 
ils sont aussi utilisés comme lien de «colonne vertébrale». Des protocoles d'accès multiples gèrent l'accès à ces 
liens (comme pour les liens filaires). Le taux de données et la distance de transmission différent entre ces 
liens sans fil.}
\point{Mode infrastructure}{la station de base connecte les mobiles au réseau cablé. Lorsqu'un mobile change
de station de base (lui fournissant un accès au réseau cablé), on parle de «handoff» (transfert).}
\point{Mode ad hoc}{aucune station de base n'est utilisée, les \neuSPs peuvent seulement communiquer à l'intérieur 
de la couverture du lien qui les «relie», ces \neuSPs s'organisent comme un réseau, routant les uns par les autres 
pour atteindre leur destination.}
\end{itemize}

\subsection{Liens sans fil, caractéristiques}

\stitre{Caractéristiques des liens standards}

\imgR{CN_143.png}{300}
\newpage
\stitre{Classification des réseaux sans fils}

\begin{center}
	\begin{tabular}{|c|p{140px}|p{140px}|}
	\hline
	Type$\backslash$nombre de hops & Simple hop & Plusieurs hops \\
	\hline
	Infrastructure & l'hôte se connecte à la station de base qui est connectée à Internet & l'hôte va devoir 
	utiliser plusieurs \neuSPs sans fil comme relai avant d'être connecté à Internet. (\textit{mesh net}) \\
	\hline
	Ad Hoc & pas de station de base, pas de connexion à Internet.$\ \ \ \ \ \ \ \ \ \ \ $ (Bluetooth, ...) & pas 
	de station de base, pas de connexion à Internet. L'hôte va devoir utiliser plusieurs \neuSPs sans fil comme 
	relai pour atteindre un autre \neuSP sans fil (\textit{MANET}, \textit{VANET})\\
	\hline
	\end{tabular}
\end{center}

\stitre{Différences avec les liens filaires}
\begin{itemize}
\bfp{force du signal diminuée}{le signal radio s'atténue au fur et à mesure qu'il se propage à travers la 
matière \textit{(path loss)}}
\bfp{interference venant d'autres sources}{les fréquences de réseau sans fil sont standardisées et partagées par 
d'autres matériels (comme les GSM), ce partage donne lieu à des interférences entre les différents matériels.}
\bfp{propagation dans plusieurs directions}{le signal radio se réfléchit sur les objets au sol arrivant à 
destination avec des temps légèrement différents} \\
\end{itemize}

\stitre{Modèle simple de l'atténuation du signal}

\imgR{CN_144.png}{400}

\noindent\textit{On utilise le \textbf{Decibel} pour comparer 2 signaux (par exemple signal transmis et signal
reçu), on peut l'utiliser comme un ratio de puissance : $K = 10\log_{10}\left[\dfrac{P_r}{P_t}\right]$ ou un 
ratio d'amplitude $K = 20\log_{10}\left[\dfrac{A_r}{A_t}\right]$.} \\

\begin{multicols}{2}
\noindent On définit le \textbf{SNR}\textit{«signal-to-noise ratio»} qui mesure le ratio de bruit qui se trouve 
dans un signal, plus ce ratio est élevé plus il est facile d'extraire le signal du bruit. Si on augmente la 
puissance de l'émission pour une couche physique donnée, ça augmente le \textbf{SNR} et diminue le 
\textbf{BER}\textit{(Bit Error Rate)}. Pour un \textbf{SNR} donné, il faut choisir la couche physique qui 
satisfait les exigences du \textbf{BER} et fournissant le plus haut débit (le \textbf{SNR} peut changer avec la 
mobilité, il faudra donc adapter la couche physique de manière dynamique).
\imgR{CN_145.png}{200}
\end{multicols}

\subsubsection{Problèmes rencontrés}

\begin{multicols}{2}
Le problème du \dred{terminal caché} est assez simple, sur l'exemple, $B$ et $A$ communiquent, $B$ et $C$ 
également par contre $A$ et $C$ ne sont pas au courant que l'autre émet, cela pose probleme car $A$ ne peut pas 
savoir qu'il interfère avec $C$ lors de ses émissions avec $B$ (et $C$ ne peut le savoir non plus).
\imgR{CN_146.png}{125}
\end{multicols}

\begin{multicols}{2}
Le problème de l'\dred{atténuation du signal} est semblable, l'émission de $A$ n'arrive pas jusque $C$ (et vice-
versa) et ils ne savent ni l'un ni l'autre qu'ils peuvent interférer l'un avec l'autre au niveau de $B$
\imgR{CN_147.png}{125}
\end{multicols}

\begin{multicols}{2}
La problème du \dred{terminal exposé} se pose lors de la configuration illustrée sur l'exemple, $B$ et $C$ 
communiquent mais $A$ ne sait pas que $C$ existe et $D$ ne sait pas que $B$ existe. A cause de \textbf{CSMA}, $B$ 
et $C$ n'émettront pas simultanément vers, respectivement, $A$ et $D$.
\imgR{CN_148.png}{125}
\end{multicols}

\subsubsection{CDMA (Code Division Multiple Access)}

Il est utilisé dans plusieurs canaux de diffusion sans fil standard (cellulaire, satellite, ...). Il assigne un 
code unique à chaque utilisateur, tous les utilisateurs partagent la même fréquence mais chacun a sa façon 
d'encoder les données définie par le code (le signal encodé sera égal aux données de départ multiplié par le 
codé). Pour le décodage, le récepteur effectue le produit interne du signal encodé et du code ; cela permet à 
plusieurs expéditeurs d'émettre en même temps avec peu d'interférence. 
\newpage
\underline{Exemple} : 
\imgR{CN_149.png}{400}
\imgR{CN_150.png}{400}

\subsection{LAN's sans fil IEEE 802.11 (Wi-Fi)}

\noindent Toutes les versions de \textit{802.11} utilisent \textbf{CSMA/CA} pour les accès multiples et elles 
possèdent toutes une version \textit{ad hoc} et une version avec une station de base. Voici quelques versions :
\begin{itemize}
\bfp{802.11b}{elle utilise le spectre $2.4$-$2.485$ GHz ainsi que le \textbf{DSSS} \textit{(Direct Sequence 
Spread Spectrum} dans la couche physique (c'est-à-dire que chaque utilisateur utilise le même code pour encoder 
les données) et elle possède une vitesse allant jusque $11$Mbps,}
\bfp{802.11a}{elle utilise le spectre $5.1$-$5.8$ GHz et possède une vitesse allant jusque $54$Mbps,}
\bfp{802.11g}{elle utilise le spectre $2.4$-$2.485$ GHz et possède une vitesse allant jusque $54$Mbps,}
\bfp{802.11h \textit{multiple antennae}}{elle utilise le spectre $2.4$-$5.8$ GHz et possède une vitesse allant 
jusque $200$Mbps.} \\
\end{itemize}

\noindent En mode infrastructure, les hôtes sans-fil communiquent avec la station de base (appelée aussi point 
d'accès (\textbf{AP})). On appellera \textbf{BSS}\textit{(Basic Service Set)} ou encore \textbf{cellule} un 
ensemble contenant des hôtes sans fil et un point d'accès. (en mode ad hoc, uniquement les hôtes) \\

\stitre{Canaux \& associations}

Dans la version \textit{802.11b} le spectre est divisé en $11$ canaux à différentes fréquences, l'administrateur 
du point d'accès doit donc choisir la fréquence(le canal) pour le point d'accès en sachant qu'il peut y avoir des
interférences avec les fréquences choisies par les voisins.  

\imgR{CN_151.png}{300}

Le problème est que plusieurs point d'accès peuvent être accessible sur le même canal, c'est pourquoi chaque 
point d'accès sera identifié par son \textbf{SSID}\textit{(Service Set Identifier)} qui sera défini par 
l'administrateur de ce point d'accès. Les \textbf{NIC} des hôtes devront s'associer avec uniquement un seul 
\textbf{SSID}, c'est-à-dire crée un "lien virtuel" avec le point d'accès. Pour ce faire, l'hôte doit scanner les
canaux et «écouter» des messages balises contenant le \textbf{SSID} d'un point d'accès ainsi que son adresse 
\textbf{MAC} et finalement sélectionner le point d'accès auquel s'associer. L'opération peut demander une 
authentification, et pour l'obtention d'une adresse \textbf{IP} dans le sous-réseau du point d'accès, l'hôte va 
typiquement utiliser \textbf{DHCP}. \\
Pour la partie \textit{«scan»}, l'hôte a le choix entre le \textbf{scan passif} et le \textbf{scan actif} :
\begin{enumerate}
\item \underline{Actif} \\
L'hôte envoie un message sonde de manière broadcast, les \textbf{AP} envoient une réponse à la sonde lors de la 
réception. Ensuite l'hôte envoie une requête d'association au point d'accès sélectionné (celui-ci lui envoyant 
une réponse d'association pour confirmer).
\item \underline{Passif} \\
Des frames balises sont envoyées périodiquement par les points d'accès, l'hôte envoie alors une requête 
d'association au point d'accès sélectionné (celui-ci lui envoyant une réponse d'association pour confirmer).
\end{enumerate}
\imgR{CN_152.png}{350}

L'algorithme de sélection du point d'accès n'est pas spécifié dans le standard \textit{802.11}, il est laissé à 
\textit{«l'implémenteur»}. On peut suivre les pistes suivantes : les \textbf{NIC} fournissent des indications 
quant à la force du signal reçu (\textbf{RSS} \textit{(Received Signal Strength)}), certains \textbf{AP} 
recquièrent une identification, certains \textbf{AP} sont peut-être plus chargés que d'autres ou encore plusieurs 
\textbf{AP}'s peuvent utiliser le même canal (conduisant ainsi à des interférences et donc une bande passante 
réduite).

\subsubsection{CSMA/CA (CSMA Collision Avoidance)}

L'idée est d'éviter les collisions, et donc d'empêcher que 2 (ou plus) \neuSPs émettent en même temps. Dans la 
norme \textit{802.11}, il n'y a aucune détection de collision (à cause des problèmes que l'on a développé plus 
haut) c'est pourquoi on va éviter les collisions en utilisant \textbf{CSMA/CA}. \\

\stitre{Fonctionnement}

\begin{multicols}{2}
\imgR{CN_153.png}{160}
\noindent L'expéditeur, s'il sent que le canal n'est pas utilisé pendant un certain temps \textbf{DIFS} 
\textit{(\textbf{D} InterFrame Space)} alors il transmet la frame entière (pas de détection de collision). Sinon 
s'il sent que le canal est utilisé il lance un temps d'attente aléatoire, ce timer décrémente lorsque le canal 
est libre et est gelé quand il est utilisé. Lorsque le timer atteint $0$, il transmet la frame et s'il ne reçoit 
pas d'\textbf{ACK}, il agrandit l'intervalle utilisé pour générer le temps d'attente aléatoire et relance un 
timer. Le récepteur quant à lui, lorsqu'il reçoit une frame correctement il envoie un \textbf{ACK} après un 
certain temps \textbf{SIFS}\textit{(Short InterFrame Space)} (les \textbf{ACK} sont nécéssaires à cause du 
problème du terminal caché).
\end{multicols}

Pour aller plus loin, on a l'idée d'autoriser les expéditeurs à réserver le canal plutot qu'un accès aléatoire au 
canal pour envoyer des frames de donnés, cela permettant d'éviter la collision des frames de beaucoup de données.
Pour ce faire, l'expéditeur envoie tout d'abord une \textit{petite} requête \textit{«to-send»} (\textbf{RTS}) à 
la station de base en utilisant \textbf{CSMA}. La station de base, en réponse à cette requête broadcast un 
\textit{«clear-to-send»}, ainsi tous les \neuSPs entendent cette réponse et l'émetteur de la requête \textbf{RTS} 
peut dès lors envoyer sa frame et les autres \neuSPs retardent leur transmission. \underline{Voici un exemple} : 

\imgR{CN_154.png}{350}

\subsubsection{Adressage des frames 802.11}

\imgR{CN_155.png}{400}
\newpage
\noindent L'adresse $1$ correspond à l'adresse \textbf{MAC} du destinataire , l'adresse $2$ de l'expéditeur, 
l'adresse $3$ est l'adresse \textbf{MAC} de l'interface routeur à laquelle le point d'accès est connecté et 
l'adresse $4$ est utilisé que dans le mode \textit{ad hoc}. \textit{Duration} contient la durée du temps de 
transmission réservée (\textbf{RTS}/\textbf{CTS}), \textit{seq control} contient le numéro de séquence de la 
frame (pour \textbf{RDT}) et \textit{type} contient le type de frame (\textbf{RTS}, \textbf{CTS}, \textbf{ACK}, 
data). \\

\begin{multicols}{2}
$\ $\\
$\ $\\
\noindent Si un hôte $H_1$ bouge tout en restant dans le même sous-réseau \textbf{IP}, $H_1$ conserve la même 
adresse \textbf{IP}. Le switch (via le self-learning) se «souviendra» quelle interface utiliser pour joindre 
$H_1$ lorsqu'il recevra une frame de $H_1$.
\imgR{CN_156.png}{150}
\end{multicols}

\subsubsection{Capacités avancées de 802.11}

\begin{enumerate}
\point{Rate adaptation \textit{(adaptation du taux)}}{la station de base et les mobiles changent dynamiquement le 
taux de transmission (modulation technique de la couche physique) au fur et à mesure que le mobile bouge et que 
le \textbf{SNR} change : }
	\begin{itemize}
	\item le \textbf{SNR} diminue et le \textbf{BER} augmente au fur et à mesure que le mobile s'éloigne de la 
	station de base,
	\item quand \textbf{BER} devient trop élevé, on bouge vers un taux de transmission plus bas mais avec un 
	\textbf{BER} moins élevé.
	\end{itemize}
\point{Power management \textit{(gestion de l'alimentation)}}{Les \neuSPs ont la possibilité d'avertir le point 
d'accès qu'ils vont \textit{«dormir»} jusqu'au prochain envoi de messages balises.  Le \textbf{AP} sait dès lors 
qu'il ne doit pas forwarder les frames qui sont destinés au \neuSP lui ayant stipulé qu'il «dormait» (Le \neuSP 
se réveille avant le lancement du prochain message balise). Les messages balises contiennent une liste de mobiles 
qui ont des frames à recevoir et qui sont en attente au point d'accès, dès lors si le \neuSP fait partie de ces 
\neuSPs il se réveille, sinon il dort jusqu'au prochain message balise.}\\
\end{enumerate}

\stitre{802.15 : personal area network}

\begin{multicols}{2}
\imgR{CN_157.png}{225}
Il s'agit d'un réseau de type \textit{ad hoc} d'une portée inférieure à $10$ mètres de diamètre qui sert de 
remplacement pour les cables (de souris, clavier, écouteurs, ...). Il est basé sur une approche 
«maître/esclaves», les esclaves demandant la permission d'envoyer des données au maître et ce dernier donnant ces 
permissions. Le \textbf{802.15} a été évolué de la spécification \textit{Bluetooth}, il utilise le spectre 
$2.4$-$2.5$Ghz et peut atteindre une vitesse de $721$ kbps.
\end{multicols}
\newpage
\stitre{802.16 : WiMAX}

\begin{multicols}{2}
Il est très semblable au \textit{802.11} et au réseau cellulaire, il utilise le système d'infrastructure avec une 
station de base. Les transmissions entre la station de base et les hôtes se font avec une antenne 
omnidirectionnelle tandis que les transmissions entre les différentes stations de base se font avec une antenne 
point à point. La différence majeure avec \textit{802.11} c'est qu'il permet une couverture de $9,7$ kilomètres 
(c'est-à-dire couvrir toute une ville plutot qu'un bâtiment pour \textit{802.11}) et il peut atteindre une 
vitesse de $14$Mbps.
\imgR{CN_158.png}{275}
\end{multicols}

Les frames de transmissions sont différentes ici, elles sont composées de $2$ sous-frames, la sous-frame 
\textit{down-link} qui concerne les transmissions de la station de base vers les \neuSPs et la sous-frame 
\textit{up-link} qui concerne les transmissions des \neuSPs vers la station de base. \textbf{WiMAX} fournit des 
mécanismes pour l'ordonnancement de ces frames mais aucun algorithme.
\imgR{CN_159.png}{300}
\begin{center}\textit{(Aspect général d'une frame de \textbf{WiMAX})}\end{center}

\subsection{Acces à l'Internet cellulaire}

\subsubsection{\'{E}léments de l'architecture du réseau céllulaire}

\begin{multicols}{2}
\imgR{CN_160.png}{250}
\begin{enumerate}
\bfp{Les cellules}{elles couvrent des régions géographique et contiennent une \textbf{BTS}\textit{(Base 
Transceiver Station)} analogue du point d'accès dans le \textbf{802.11}. Les mobiles s'attachent au réseau via 
la \textbf{BTS} et communiquent avec celle-ci via l'air en suivant des protocoles pour la couche physique et pour 
la couche lien}
\bfp{Les MSC \textit{(Mobile Switching Center)}}{ils connectent les cellules à l'Internet, s'occupent des 
organiations des \textbf{calls} et de la mobilité.}
\bfp{Les BSC \textit{(Base Station Controller)}}{elles s'occupent de l'allocation des canaux, de la pagination et 
du transfert. (non-représentées sur le schéma ci-contre)}
\end{enumerate}
\end{multicols}

\newpage

\begin{multicols}{2}
\noindent Pour partager le spectre radio des mobiles vers la \textbf{RTS}, 2 techniques sont possibles : 
\begin{enumerate}
\bfp{FDMA et TDMA combinés}{on divise le spectre en canaux de fréquences et chaque canal est divisé en slot de 
temps (technologie \textbf{GSM})}
\bfp{CDMA \textit{(Code Division Multiple Access)}}{technologie \textbf{IS-95 CDMA} et \textbf{CDMA 2000}}
\imgR{CN_161.png}{250}
\end{enumerate}
\end{multicols}

\stitre{Bref sondage des standards dans le domaine cellulaire}

\begin{itemize}
\point{Systèmes 1G}{voix analogiques}
\point{Systèmes 2G}{canaux de voix :
	\begin{itemize}
	\item \textbf{IS-136 TDMA} : \textbf{FDMA/TDMA} combinés (Amérique du nord),
	\item \textbf{GSM} \textit{(Global System for Mobile communications)} : \textbf{FDMA/TDMA} combinés (le plus 
	répandu, représente plus de $80\%$ des mobiles),
	\item \textbf{IS-95 CDMA}
	\end{itemize}}
\point{Systèmes 2,5G}{canaux de voix et de données, il s'agit d'une extension de la 2ème génération pour ceux qui 
ne pouvaient pas attendre la 3ème. Dans ces extensions on compte le \textbf{GPRS}\textit{(General Packet Radio 
System)} qui a été basé sur \textbf{GSM} et dans lequel les données sont envoyées sur plusieurs canaux (s'ils 
sont disponibles). Il atteint une vitesse de \dred{$115$kpbs}. On compte également l'\textbf{EDGE} 
\textit{(Enhanced Data for Global Evolution)} basé également sur \textbf{GSM}, il utilise une modulation 
améliorée lui permettant d'atteindre une vitesse de \dred{$384$kbps}. Et finalement, on trouve également le 
\textbf{CDMA-2000}\textit{(phase 1)} qui lui est basé sur \textbf{IS-95} avec des vitesses allant jusque 
\dred{$144$kbps}.}
\point{Systèmes 3G}{voix/données :
	\begin{itemize}
	\item \textbf{UMTS}\textit{(Universal Mobile Telecommunications Service)}, le service de données est assuré 
	par \textbf{HSDPA/HSUPA}\textit{(High Speed Downlink/Uplink Packet Access)}, il permet une vitesse de 
	\dgre{$3$Mbps}.
	\item \textbf{CDMA-2000} : \textbf{CDMA} dans des slots \textbf{TDMA}, le service de données est assuré par
	\textbf{1xEVDO} \textit{(1xEvolution Data Optimized)}, les vitesses allant jusque \dgre{$14$Mbps}.
	\end{itemize}}
\end{itemize}

\subsection{Principes d'adressage et de routage des utilisateurs mobiles}

Spectre de mobilité à partir de la perspective du réseau : \\

\imgR{CN_162.png}{300} 

\subsubsection{Vocabulaire}

\imgR{CN_163.png}{300} 

\begin{enumerate}
\bfp{réseau domicile}{«maison» permanente du mobile,}
\bfp{adresse permanente}{l'adresse dans le réseau domicile, peut toujours être utilisée pour joindre le mobile,}
\bfp{agent de domicile}{entité qui va réaliser les fonctions de mobilité sur le nom du mobile lorsque le 
mobile est 
à distance,}
\bfp{correspondant}{désire communiquer avec le mobile,}
\bfp{agent extérieur}{entité dans le réseau visité qui va réaliser les fonctions de mobilité sur le nom du 
mobile,}
\bfp{adresse permanente}{reste constante}
\bfp{\textit{care-of-address}}{adresse dans le réseau visité}
\bfp{réseau visité}{réseau dans lequel le mobile réside actuellement}
\end{enumerate}

\stitre{Mobilité : Enregistrement}

\imgR{CN_164.png}{300}
Le mobile contacte l'agent extérieur en entrant dans un réseau visité, ensuite l'agent extérieur contacte
l'agent de domicile pour lui signifier que tel mobile est résident sur son réseau. On a comme résultat final que 
l'agent extérieur connaît l'existence du mobile et l'agent de domicile connaît la localisation du mobile.

\noindent Afin de joindre un ami mobile, on ne va pas laisser le routage s'en occuper (n'étant pas faisable pour 
des millions de mobiles). On va plutot laisser les extrémités s'en charger : 
\begin{enumerate}
\item \textbf{\underline{Routage indirect}}\\
La communication venant du correspondant vers le mobile est routée à travers l'agent de domicile qui forwarde les 
messages vers le mobile à distance, tandis que le mobile répond directement au correspondant. On constate que le 
mobile utilise 2 adresses : l'\textit{adresse permanente} et la \textit{care-of-address}, la première est 
utilisée par le correspondant et la seconde est utilisée par l'agent de domicile pour forwarder les messages 
jusqu'au mobile \textit{(par conséquent, le correspondant n'a aucune idée de la localisation du mobile)} ; les 
fonctions de l'agent extérieur pouvant être assurées par le mobile lui-même. On constate également l'apparition 
d'un triangle de routage dont les cotés représentent les échanges de messages entre entités, ce triangle est 
inefficace lorsque le mobile et le correspondant sont dans le même réseau. \newpage \textit{(En effet le 
correspondant doit passer par l'agent domicile qui peut être très éloigné alors que le correspondant est dans le 
même réseau que lui)} \\
\textbf{\textit{Imaginons maintenant que le mobile change de réseau,}} il s'enregistre auprès du nouvel agent 
extérieur, celui-ci s'enregistre auprès de l'agent de domicile, ce dernier mettant à jour la \textit{care-of-address} du mobile et les paquets continuent à être forwardés jusqu'au mobile (mais avec une nouvelle 
\textit{care-of-address}). La mobilité et le changement de réseau sont donc totalement transparents, les 
connexions en cours peuvent être maintenues.
\item \textbf{\underline{Routage direct}} \\
Le correspondant obtient l'adresse du mobile via l'agent de domicile de ce dernier puis envoie ses messages à 
l'agent extérieur du réseau où se trouve le mobile, agent qui forwarde ensuit les messages au mobile. Le mobile, 
comme pour le routage indirect répond directement au correspondant. Ce type de routage surmonte le problème du 
triangle de routage mais il n'est pas transparent au correspondant car il doit obtenir la \textit{care-of-
address} du mobile (et il y aura un soucis lorsque le mobile changera de réseau visité). 
Il existe une mobilité accomodante avec le routage direct, on utilise l'agent extérieur du premier réseau visité 
comme ancre, et tous les messages destinés au mobile routeront d'abord par l'ancre. Lorsque le mobile change de 
réseau, l'agent extérieur du nouveau réseau visité s'arrange pour recevoir les données de l'agent extérieur du
réseau visité juste avant (\textit{chaînage}).
\end{enumerate}

\subsection{Mobile IP}

\textbf{Mobile IP} possède beaucoup de caractéristiques que nous avons déjà vu tels que les agents de domicile et 
extérieur, l'enregistrement, les \textit{care-of-adress} et l'encapsulation. Il y a 3 composantes dans la norme 
\textit{Mobile IP} : le routage indirect des datagrammes, l'agent de découverte et l'enregistrement avec les 
agents de domicile. \\

\stitre{Le routage indirect}

\imgR{CN_165.png}{300}

\newpage
\stitre{Agent de découverte}

\noindent Les agents envoient des «annonces d'agent» pour publier leur service en broadcastant des messages 
\textbf{ICMP} : \imgR{CN_166.png}{300}

\stitre{Enregistrement}

\noindent Les messages d'enregistrement sont envoyés en \textbf{UDP} via le port \textbf{434}. Ce système 
fonctionne selon le principe de requête/réponse et les messages contiennent : 
\begin{itemize}
\item Type : 1 pour une requête, 3 pour une réponse,
\item Care-of-Address (COA),
\item Home-Agent Address (HA),
\item Permanent Address (PA),
\item Lifetime (in seconds),
\item Numéro d'identification ($64$ bits) utilisé pour mapper les réponses avec les requêtes et protéger contre 
les \textit{«replay attacks»},
\item Options qui spécifie le type d'encapsulation (GRE, Minimal Encapsulation)
\item Code (accepté, refusé, erreur (pour les réponses))
\end{itemize}

Les agents tiennent en mémoire des tables d'état, l'agent de domicile maintient une \textit{Mobility Binding 
Table} contenant des triplets (Permanent Address,Care-of-Address,Lifetime) tandis que l'agent extérieur maintient 
une \textit{Visitor Table} contenant des quadruplets (Permanent Address, Home-Agent Address, MAC Address, 
Lifetime). \textit{(Le champ «Lifetime» fonctionne comme un \textbf{TTL}, lorsqu'il atteint 0, l'entrée 
correspondante est supprimée de la table)}

\noindent\underline{Exemple d'enregistrement}

\imgR{CN_167.png}{400}

\subsection{Traitement de la mobilité dans les réseaux cellulaires}

\imgR{CN_168.png}{200}

On définit le réseau domicile comme le réseau du fournisseur chez qui l'utilisateur est abonné 
\textit{(Mobistar, Proximus, Base,...)}. Dans ce réseau on trouve la \textbf{HLR}\textit{(Home Location 
Register)} qui est une base de données qui contient de manière permanente, les numéros de téléphone, des 
informations de profil (services, préférences, facturation), des informations à propos de la location actuelle. 
Le réseau visité est comme définit plus haut et on y trouve la \textbf{VLR}\textit{(Visitor Location Register)} 
qui est une base de données contenant une entrée pour chaque utilisateur actuellement dans le réseau (même si il
s'agit du réseau domicile de l'utilisateur). Quant au routage, \textbf{GSM} utilise le routage indirect :

\imgR{CN_169.png}{325}

\subsubsection{GSM : transfert avec un MSC commun}

Le but du transfert est de router les appels via la nouvelle station de base sans interruption. Les raisons pour 
déclencher un transfert sont les suivantes : 
\begin{enumerate}
\item un signal plus fort vers/depuis la nouvelle station de base,
\item balance de chargement, ca permet de libérer des canaux dans la station de base que l'utilisateur quitte,
\end{enumerate}
\textit{(\textbf{GSM} ne spécifie pas pourquoi effectuer un transfert mais uniquement comment en faire)}\\
\textit{(Le transfert est initialisé par la station de base que l'utilisateur quitte)} \\

\newpage

\stitre{Déroulement du transfert}

\begin{multicols}{2}
\imgR{CN_170.png}{250}
\begin{enumerate}
\item \textit{old BS} informe le \textbf{MSC} d'un transfert imminent et fournit une liste de $1^+$ nouvelles 
stations de base,
\item le \textbf{MSC} configure le chemin (alloue les ressources) vers la nouvelle station de base,
\item \textit{new BS} alloue un canal radio pour l'utilisation du mobile,
\item \textit{new BS} signale au \textbf{MSC} et à \textit{old BS} qu'elle est prête,
\item \textit{old BS} dit au mobile d'effectuer un transfert vers \textit{new BS},
\item le mobile et \textit{new BS} échangent des messages pour activer le nouveau canal,
\item le mobile signale au \textbf{MSC} via \textit{new BS} que le transfert est terminé, le \textbf{MSC} réroute 
les appels,
\item les ressource du \textbf{MSC} et de \textit{old BS} sont libérées.
\end{enumerate}
\end{multicols}

\subsubsection{GSM : transfert entre MSC}

\begin{multicols}{2}
\imgR{CN_171.png}{220}
Comme pour le routage direct, on introduit la notion d'«ancre», ici, l'\textit{ancre \textbf{MSC}} sera le 
premier \textbf{MSC} visité lors d'un appel, appel qui continue à être routé via l'\textit{ancre \textbf{MSC}}. 
Les nouveaux \textbf{MSC} sont ajoutés à la fin de la chaine de \textbf{MSC} lorsqu'un mobile se déplace vers un 
nouveau \textbf{MSC}. \textbf{IS-41} permet une étape optionnelle de minimisation du chemin pour raccourcir la 
chaîne contenant plusieurs \textbf{MSC}.
\end{multicols}

\subsection{GSM Vs Mobile IP}

\begin{center}
	\begin{tabular}{|p{120px}|p{180px}|c|}
	\hline
	\dred{Élement de GSM} & \dred{Commentaire sur l'élément de GSM} & \dred{Élément de mobile IP} \\
	\hline
	Système domicile & Réseau auquel le numéro permanent du mobile de l'utilisateur appartient & Réseau domicile 
	\\
	\hline
	\textbf{MSC} domicile (\textbf{HLR}) & Point de contact pour obtenir une adresse routable du mobile de 
	l'utilisateur & Agent de domicile \\
	\hline
	Système visité & Réseau différent que le système domicile où le mobile réside actuellement & Réseau visité \\
	\hline
	\textbf{MSC} visité (\textbf{VLR}) & Responsable de la configuration des appels vers les/venant des \neuSPs 
	mobiles se trouvant	dans les cellules associées au \textbf{MSC} & Agent extérieur \\
	\hline
	Numéro de \textit{Roaming} & Adresse routable pour les appels téléphoniques entre le \textbf{MSC} 
	domicile et le \textbf{MSC} visité, visible ni par le mobile ni par le correspondant & \textit{Care-of-
	address}\\
	\hline
	\end{tabular}
\end{center}

\subsection{Mobilité et protocoles de couche supérieure}

Logiquement, l'impact devrait être minimal car le modèle de \textit{best-effort} reste inchangé et \textbf{TCP} 
et \textbf{UDP} peuvent fonctionner (et fonctionnent) sur le sans fil et les mobiles. Cependant, en terme de 
performance on constate des pertes de paquets et des délais dus aux erreurs de bits et aux transferts et 
\textbf{TCP} interpretant les pertes comme de la congestion va diminuer de manière non-nécessaire sa fenêtre de 
congestion. De plus il faut aussi tenir compte de la dépréciation des délais pour le trafic en temps-réel ainsi 
que la bande passante limitée des liens sans fil. \\

\hbox{\raisebox{0.4em}{\vrule depth 0.4pt height 0.4pt width 10cm}}

\section{Le réseau multimédia}

Ce chapitre va nous amener à classer les applications multimédia, identifier les services Internet dont ces 
applications ont besoin et tirer le meilleur du service \textit{best effort}. Pour ce faire, il y a des 
protocoles spécifiques pour le \textit{best effort} ainsi que des mécanismes et des architectures pour fournir 
une qualité de service.

\subsection{Les applications multimédia en réseau}

Elles sont classées en 3 classes : \textit{«\red{streaming stored}»}, \textit{«\red{live streaming}»} et 
\textit{«\red{interactive, real-time}»}. Point de vue caractéristiques fondamentales de ces applications, on 
trouve :
\begin{enumerate}
\item elles sont typiquement sensibles aux délais, le délai d'un point à l'autre et le delai 
\textbf{jitter}\textit{(la variabilité de délais de paquet dans le même stream de paquet)},
\item elles sont tolérantes aux pertes, si elles ne sont pas fréquentes,
\item elles sont l'antithèse des flux de données qui sont plutôt tolérants aux délais et intolérants aux pertes.
\end{enumerate}

\subsubsection{Applications \textit{«streaming stored»}}

Le média est stocké à la source et transmis au client, celui-ci commence sa \textbf{lecture 
différée}\textit{(\red{playout})} avant que toutes les données ne lui soient parvenues. Il y a donc une 
contrainte de temps pour les données encore à envoyer, elles doivent être envoyées à temps pour la lecture 
différée. Ces applications possèdent des fonctionnalités semblables à celles d'un magnétoscope, en effet, le 
client peut faire pause, avancer, reculer, bouger la barre de défilement, ... Elles acceptent un délai initial de 
$10$ secondes ainsi qu'un délai de $1$ à $2$ secondes avant qu'une commande ne soit satisfaite (pause, avancer, 
...).

\imgR{CN_172.png}{400}

\subsubsection{Applications \textit{«streaming live»}}

Il s'agit, par exemple, d'une émission de radio par Internet ou encore d'un évènement sportif en direct. Au 
niveau du streaming, c'est assez semblable avec le \textit{«stored streaming»}, il y a des buffers pour la 
lecture celle-ci pouvant être décalée 10 secondes après la transmission (il reste encore des contraintes de 
temps). Au niveau de l'interactivité, l'avance rapide est impossible mais la pause et le retour en arrière, eux, 
le sont.

\subsubsection{Applications \textit{«real-time, interactive»}}

Il s'agit, par exemple, de la téléphonie \textbf{IP}, de vidéo-conférences ou de «mondes» interactifs distribués. 
Ces applications possèdent des exigences quant au délai d'un point à l'autre, par exemple pour l'audio, un délai 
inférieur à $150$ msec sera considéré comme \textit{«bon»} et un délai situé entre $150$ et $400$ msec sera 
considéré comme «OK», un délai supérieur serait décerné par les utilisateurs et nuirait à l'interactivité de 
l'application \textit{(dans ce délai est compté le temps de parcours mais également la mise en paquet des 
données)}.

\subsubsection{Internet et le multimédia}

Si on utilise le service \textit{best-effort} de \textbf{TCP/UDP/IP}, on aura aucune garantie quant aux délais et 
aux pertes, les applications multimédia Internet de nos jours utilisent des techniques au niveau application pour 
atténuer (aussi bien que possible) les effets de délais et de pertes. Il existe 3 pistes d'évolution possible 
pour Internet afin de mieux supporter le multimédia :
\begin{enumerate}
\bfp{Philosophie des services intégrés}{changements fondamentaux dans Internet tels que les applications puissent
réserver de la bande passante allant d'une extrêmité à une autre, ces changements recquièrent des nouveaux 
programmes complexes à implémenter dans les hôtes et les routeurs.}
\bfp{«Laissez-faire»}{pas de changements majeurs mais plus de bande passante quand nécéssaire 
\textit{(«overprovision»)}, distribution de contenu et multicast au niveau de la couche application.}
\bfp{Philosophie des services différenciés}{moins de changements dans l'infrastructure de l'Internet et encore 
fournir des services de $1^{ere}$ et $2^{nde}$ classe.}
\end{enumerate}

\subsection{«Streaming stored» audio et vidéo}

Afin de tirer le meilleur du service \textit{best effort}, il existe des techniques de steaming au niveau 
application : le buffering du coté du client, utilisation d'\textbf{UDP} plutot que \textbf{TCP}, plusieurs 
encodage du multimédia. Le lecteur de media aura pour rôles de supprimer le \textbf{jitter}, décompresser le 
média, dissimuler les erreurs et d'offrir une interface contenant des contrôles pour l'interactivité avec 
l'utilisateur.

\subsubsection{Internet multimédia}

L'approche la plus simple est de stocker le fichier multimédia dans un fichier et ce fichier est envoyé comme un 
objet \textbf{HTTP} et est reçu entièrement par le client puis passé au lecteur. Cette approche ne permet pas le 
streaming car la lecture n'a lieu qu'une fois le fichier entièrement arrivé. L'approche du streaming consite à ce 
que le browser Internet reçoive un \textit{\red{metafile}}, il lance le lecteur multimédia et lui passe le
\textit{metafile}, le lecteur contacte alors le serveur et le serveur diffuse \textit{(stream)} le fichier 
multimédia au lecteur. Une autre approche est d'utiliser un serveur de streaming, qui va permettre d'utiliser 
d'autres protocoles que \textbf{HTTP} pour la connexion entre le lecteur et le serveur. Sur le schéma ci-dessous,
l'étape $3$ peut être assurée par \textbf{UDP/TCP} par exemple.

\imgR{CN_173.png}{300}

\stitre{Buffering du client}

Le buffering du coté du client permet de compenser le délai \textbf{jitter} en ajoutant du délai de réseau :

\imgR{CN_174.png}{300}

\stitre{TCP ou UDP ?}

\begin{itemize}
\point{UDP}{le serveur envoie au taux approprié pour le client (sans se soucier de la congestion du réseau) ; 
souvent, le taux d'envoi $=$ taux d'encodage $=$ taux constant puis taux de remplissage $=$ taux constant $-$ 
paquets perdus. Le délai de playout (lecture décalée) pour supprimer le délai de \textbf{jitter} est court 
(de l'ordre de 2 à 5 secondes). Le recouvrement d'erreur est effectué seulement si le temps le permet.}
\point{TCP}{le serveur envoie au taux maximal autorisé par \textbf{TCP}, dès lors le taux de remplissage varie en 
fonction du controle de congestion de \textbf{TCP}. En conséquence, le délai de playout est plus grand pour 
«lisser» le taux de livraison de \textbf{TCP} (la famine est peu probable si le débit moyen 
\textbf{TCP} est supérieur à $2$ fois le taux de bit du média). Le couple \textbf{HTTP/TCP} passe plus 
facilement au travers des pare-feu.} \\
\end{itemize}

\noindent Pour gérer les différentes capacités de réception des clients, le serveur stocke et transmet des 
copies multiples des fichiers multimédias, encodés à différents taux.

\newpage

\subsubsection{Contrôle par l'utilisateur du media en streaming : RTSP}

\textbf{HTTP} ne cible pas le contenu multimédia et ne fournit aucune commande telle que pause ou avance rapide. 
C'est pourquoi on va utiliser le protocole \textbf{RTSP}, il s'agit d'un protocole de la couche application et de 
type client-serveur et qui va fournir les commandes à l'utilisateur (pause, ...). Par contre, \textbf{RTSP} ne 
définit pas comment le fichier multimédia est encapsulé pour le streaming par le réseau, il ne restreint pas à 
l'emploi d'un protocole de transport en particulier (\textbf{TCP} et \textbf{UDP} possibles) et il ne spécifie 
pas non plus comment le lecteur média doit mettre en buffer les fichiers multimédia. \\
\textbf{RTSP}, à l'instar de \textbf{FTP}, utilise un canal «out-of-band» (port 554) pour tout ce qui est 
contrôle du média, le média étant lui transféré dans le canal «in-band» (et donc un autre port). \\

\stitre{Exemple d'échange RTSP}

\imgR{CN_175.png}{400}
\imgR{CN_176.png}{400}

\subsection{Tirer le meilleur du service \textit{best effort}}

Parmis les applications intéractives temps-réel, on retrouve les \textbf{PC-2-PC phone} comme \textit{Skype}, les
\textbf{PC-2-phone} comme \textit{Dialpad}, \textit{Net2phone} et \textit{Skype} ou encore les 
\textbf{videoconférences par webcam} comme \textit{Skype} et \textit{Polycom}. Nous allons détailler l'exemple 
des applications \textbf{PC-2-PC phone}.

\subsection{Internet phone}

L'audio venant du haut-parleur alterne entre des moments de conversations et des périodes de silences, on atteint 
un débit de $64$kbps (8Kbytes/sec) pendant les moments de conversations \textit{(talkspurts)} (les paquets ne 
sont générés que pendant ces moments). L'application envoie l'audio par «morceaux», à chaque morceau elle attache 
un header de la couche application et par la suite le morceau et le header sont encapsulés dans un segment 
\textbf{UDP}. L'application en envoie toutes les $20$msec (pendant les moments de conversations), ce qui fait 
qu'un morceau contient 160 bytes de données (vu que l'on transfère à 8Kbytes/sec). \\

\stitre{Les délais et pertes}

Il y a 3 types de délais/pertes dans les conversations téléphoniques par Internet : \\
\begin{enumerate}
\bfp{perte du réseau}{perte de datagrammes \textbf{IP} dûe à la congestion du réseau,}
\bfp{perte de delai}{perte d'un datagramme \textbf{IP} dû à son arrivée trop tardive pour le \textit{playout} de
l'utilisateur. Son retard pouvant être causé par le traitement, la mise en file d'attente dans le réseau ou des 
délais supplémentaires dans les extrémités du réseau (typiquement le retard maximum toléré est de $400$ms).}
\bfp{délai \textit{jitter}}{«temps de trajet»}
\end{enumerate}
\textit{En général, un taux de perte entre $1$ et $10\%$ peut être toléré.} \\

\noindent Le délai de \textit{playout} pour l'utilisateur peut être fixé ou adaptatif, considérons les 2 
approches :
\begin{enumerate}
\point{Délai de playout fixé}
{\\
Le récepteur essaie de lire en différé chaque morceau exactement $q$ msecs après que le morceau ait été généré ;
si le morceau arrive après ce délai, il est droppé. Instinctivement, plus $q$ est grand moins il y aura de 
paquets droppés, et plus $q$ est petit plus l'impression d'interactivité sera bonne. On peut voir sur le 
graphique suivant que si on fixe le playout à $p$ il y a un paquet qui va manquer le playout ($\rightarrow$ 
droppé) tandis que si on le fixe à $p'$, tous les paquets arrivent à temps.
\imgR{CN_177.png}{400}
}
\point{Délai de playout adaptatif}
{\\
Le but de cette approche est de diminue le délai de playout tout en conservant un taux de perte dû aux retards 
peu élevé. L'approche consiste à estimer le délai du réseau et d'ajuster le playout à chaque début de moment de 
conversation (les périodes de silences sont comprimées et allongées). Chaque morceau émis (toujours toutes les 
$20$msec durant un moment de conversation) est préfixé par une marque de temps + un numéro de séquence.
\textit{\underline{Notations} :}
\begin{itemize}
\item $t_i = $ marque de temps du $i^{eme}$ paquet (temps auquel le paquet a été généré),
\item $r_i = $ temps auquel le paquet $i$ a été reçu par le récepteur,
\item $p_i = $ temps auquel le paquet $i$ est joué par le récepteur,
\item $r_i-t_i = $ délai de réseau pour le $i^{eme}$ paquet,
\item $d_i = $ estimation du délai de réseau moyen après que le $i^{eme}$ paquet ait été reçu.
\end{itemize}
Avec ces notations on peut écrire :
\begin{itemize}
\item estimation dynamique du délai de réseau moyen : $d_i = (1-u)d_{i-1}+u(r_i-t_i)$ où $u$ est une constante 
fixée,
\item estimation de la déviation moyenne du délai : $\nu_i = (1-u)\nu_{i-1}+u|r_1-t_i-d_i|$, \\
($d_i$ et $\nu_i$ sont calculés pour chaque paquet reçu mais ils ne sont utilisés qu'au début d'un moment de 
conversation)
\item pour le premier paquet dans un moment de conversation le temps de playout est : $p_i=t_i+d_i+K\nu_i$ où $K$ 
est une constante fixée, les paquets restants sont joués périodiquement. \\
\end{itemize}
\noindent Afin de déterminer quel paquet est le premier d'un moment de conversation, le récepteur procède de la 
manière suivante : si il n'y a pas de perte, il regarde les marques de temps et si la différence entre 2 marques 
successives est plus grande que $20$msec alors le paquet est le premier du moment de conversation, sinon il doit 
également regarder les numéros de séquence. Si la différence est toujours supérieur à $20$msec et qu'il n'y a pas 
de trou dans les numéros de séquence (donc pas de perte) alors le paquet est le premier du moment de 
conversation.
}
\end{enumerate}

\stitre{Recouvrement d'une perte de paquet}

L'objectif est d'anticiper les pertes (aucune retransmission), pour ce faire, il existe 2 schémas : 
\textbf{FEC}\textit{(Forward Error Correction)} et \textbf{Interleaving}.
\begin{enumerate}
\point{FEC}
{\\
Pour chaque groupe de $n$ morceaux, on crée un morceau redondant construit en appliquant des \textbf{XOR} aux $n$ 
morceaux originaux. On envoit donc $n+1$ morceaux, ceci augmentant la bande-passante nécéssaire par $\frac{1}
{n}$. On est dès lors capable de reconstruire les $n$ morceaux originaux si au plus un morceau a été perdu. On 
doit fixer le délai de \textit{playout} à un seuil suffisant pour transférer les $n+1$ morceaux. Concernant $n$, 
il faut arriver à un compromis, car plus $n$ augmente \gre{moins la bande-passante est gaspillée} mais \red{plus 
le délai de \textit{playout} est grand} et \red{plus la probabilié que plus d'un morceau soit perdu augmente}. 
\textbf{FEC} propose également une seconde approche, il s'agit d'envoyer comme information redondante le stream 
audio en faible qualité (par exemple le stream \textbf{PCM} est envoyé à $64$kbps et le stream redondant de 
\textbf{GSM} est envoyé à $13$kbps). Avec cette approche, tant qu'il n'y a pas de perte consécutive (2 paquets de 
suite qui sont perdus) le récepteur est capable de «masquer» la perte :
\imgR{CN_178.png}{400}
\textit{(Pour faire face au problème d'une perte de paquets consécutifs, on peut ajouter au paquet $n$ des 
informations redondantes concernant les 2 paquets qui le précèdaient ($n-2$ et $n-1$))}
}
\newpage
\point{Interleaving}
{\\
Les morceaux sont divisés en unités plus petites (par exemple $4$ unités de $5$msec par morceau) et les paquets 
que l'on va envoyer contiendront des informations provenant de différents morceaux de telle sorte que si il y a 
une perte, il restera une majeure partie de chaque morceau. Cette approche n'introduit pas d'overhead dû à une 
redondance mais elle augmente le délai de playout.
\imgR{CN_179.png}{400}
}
\end{enumerate}

\subsection{Fournir des multiples classes de service et des garanties de qualité de service}

L'idée est de partionner le trafic en classes, le réseau traitant différemment les classes (un comme un service 
\textbf{VIP} et un service ordinaire). Pour différencier ces classes, on fait appel aux bits de 
\textbf{ToS}\textit{(Type of Service)} ; il y a une certaine granularité, il y a des services différents parmis 
les multiples classes par parmi les connexions individuelles. \\

\stitre{Scénario numéro 1 : audio \& \textbf{FTP}}

\imgR{CN_180.png}{300}

Les transferts du \textbf{FTP} peuvent créer de la congestion dans le routeur et causer des pertes audio. On 
cherche donc à donner la priorité à l'audio.

\noindent\dred{\underline{Principe numéro 1} : Il est nécessaire de marquer les paquets pour la distinction 
entre les paquets des différentes classes. Il faut également une nouvelle politique de traitement dans les 
routeurs pour qu'ils traitent les paquets en conséquence.} \\

\stitre{Scénario numéro 2 : application se conduisant mal}

Imaginons une application qui envoie plus vite que le taux déclaré, il faut dès lors forcer les sources 
d'adhérer à l'allocation de bande-passante qui leur a été faite. On utilise donc des principes de marquage et de 
«police» aux extrêmités du réseau.

\noindent\dred{\underline{Principe numéro 2} : Il est nécessaire de fournir une protection/isolation entre les 
classes.}  \\
\newpage
\stitre{Scénario numéro 3 : bande-passante non-utilisée}

L'allocation de bande-passante est fixée et non partageable, dès lors si une application n'utilise pas son 
allocation, on a une utilisation non-efficace de la bande-passante.

\noindent\dred{\underline{Principe numéro 3} : Il est souhaitable que l'utilisation des ressources allouées 
soit efficace lorsque l'on fournit l'isolation.}

\subsubsection{Mécanismes d'ordonnancement}

L'ordonnancement consiste à choisir quel est le prochain paquet à envoyer sur le lien, on développe 4 algorithmes
simples :
\begin{enumerate}
\point{FIFO \textit{(First In First Out)}}
{\\
\imgR{CN_181.png}{400}
Consiste à simplement envoyer dans l'ordre d'arrivée dans la file. Lorsque la queue est remplie, soit on drop le 
paquet qui vient d'arriver, soit un paquet aléatoire, soit un paquet en se basant sur un système de priorité.
}
\point{Ordonnancement avec priorités}
{\\
\imgR{CN_182.png}{400}
Consiste à transmettre d'abord le paquet avec la plus grande priorité $\rightarrow$ différentes classes avec des 
priorités différentes
}
\point{Ordonnancement «Round Robin»}
{\\
\imgR{CN_183.png}{300}
\newpage
Semblable à l'ordonnancement précédent mis à part le fait que dans celui-ci, on scanne de manière cyclique les 
files des différentes classes et on envoie un de chaque (si il y en a dans la file). Ca permet d'éviter une 
certaine famine qui pourrait se présenter dans l'autre cas (en effet si on reçoit que des paquets prioritaires, 
les non-prioritaires vont rester dans leur file indéfiniment).
}
\point{Ordonnancement «Weighted Fair Queuing» (WFQ)}
{\\
\imgR{CN_184.png}{300}
Généralise le «Roung Robin», permettant d'insérer des poids spécifiant le nombre de paquet pour chaque classe à
envoyer par cycle (si il y en a en attente). Par exemple si $w_1=3$, $w_2=2$, $w_3=4$, après un cycle, si aucun 
paquet n'arrive entre temps, il ne restera qu'un seul paquet dans la file de $w_1$ et on aura envoyé les $2$ 
paquets de $w_2$ suivis par les $4$ paquets de $w_3$.
}
\end{enumerate}

\subsubsection{Mécanismes de police}

Le but de ces mécanismes est de limiter le trafic pour qu'il n'excède pas les paramètres déclarés. Les 3 
critères communément utilisés sont : 
\begin{itemize}
\bfp{Taux moyen (à long terme)}{combien de paquets peuvent être envoyés par unité de temps, la question cruciale 
étant de savoir la taille de l'intervalle car $100$ paquets par seconde et $6000$ paquets par minute ont le même 
taux moyen.}
\bfp{Pic de taux \textit{(Peak Rate)}}{exemple général $\rightarrow 6000$ ppm, $1500$ ppm comme pic de taux.}
\bfp{Taille (maximum) d'éclat}{nombre maximum de paquets envoyés consécutivement (sans aucun ralentissement 
n'intervenant)} \\
\end{itemize}
\newpage
\stitre{Token Bucket}

\imgR{CN_185.png}{300}

On limite l'entrée au taux moyen et à la taille d'éclat spécifiés. Un bucket peut détenir plusieurs tokens, 
tokens qui sont générés à un taux de $r$ tokens/sec à moins que le bucket ne soit plein. Sur un intervalle de 
longueur $t$ le nombre de paquets admis est inférieur ou égal à $(r*t+b)$. \\

\noindent\textit{\textbf{Token Bucket} associé à \textbf{WFQ} fournit une borne supérieure garantie sur les 
délais, c'est-à-dire une \dred{qualité de service garantie} !}
\imgR{CN_186.png}{400}

\end{document}